---
title: "Naive Bayes"
author: "Yuxiao Lin"
toc: true
number-sections: true
highlight-style: pygments
format: 
  html: 
    code-fold: true
    html-math-method: katex
  pdf: 
    geometry: 
      - top=30mm
      - left=20mm
##  docx: Never, unless to accommodate a collaborator
---


## Naive Bayes

This section was written by Yuxiao Lin, a junior majoring in Statistical Data
Science at the University of Connecticut.

This section will focuses on Bayes’ theorem, the types of Naive Bayes classifiers, 
and provides an example application.

### Introduction

Naive Bayes is a probabilistic classification algorithm based on Bayes' theorem. 
It assumes that all features are conditionally independent given the class label. 
Using this assumption, the model calculates the probability of each class given the 
observed features, and then selects the class with the highest probability as the 
prediction.

Since it has a simple structure and fast computation, Naive Bayes is widely used 
in tasks such as text classification, sentiment analysis, and spam detection.

### Bayes' Theorem

Bayes’ Theorem is a mathematical formula used to update a posterior probability 
based on a prior probability and new evidence.
Its basic form is:

$$P(Y|X) = \frac{P(Y)P(X|Y)}{P(X)},$$

where,

- $P(Y|X)$: Probability of class $Y$ given features $X$ - posterior probability.
- $P(X|Y)$: Probability of features given class $Y$ - the likelihood.
- $P(Y)$: Probability of class $Y$ - prior probability.
- $P(X)$: Total probability of features $X$ (marginal probability of features) - the evidence.


### Naive Bayes Classifier

The Naive Bayes classifier is a model based on Bayes' theorem and the assumption of 
conditional independence among features. Although it applies Bayes' theorem in its 
decision rule, it is not necessarily a "Bayesian model" in the statistical sense. 
In practice, Naive Bayes is usually trained using:

- a frequentist approach via Maximum Likelihood Estimation (MLE), or  
- a Bayesian approach using prior and posterior distributions.


### Parameter Estimation and Prediction

Naive Bayes does not optimize a loss function like logistic regression.  
It "learns" by counting label–feature co-occurrences, using MLE to directly 
compute class and conditional probabilities — making it fast and simple to fit.

- Prior probability of class $y$:

$$P(Y = y) = \frac{\text{count}(Y = y)}{N}.$$

- Conditional probability of feature $X_i$ given class $y$:

$$P(X_i = x_i \mid Y = y) = \frac{\text{count}(X_i = x_i, Y = y)}{\text{count}(Y = y)}.$$


Once the probabilities are estimated, the model predicts the label of a new 
observation $X = (x_1, x_2, \dots, x_n)$, then select the one with the highest value:

$$
\hat{y} = \arg\max_{y} P(Y = y) \prod_{i=1}^n P(X_i = x_i \mid Y = y).
$$


### Types of Naive Bayes

To handle different types of input data, 5 types of the Naive Bayes classifier
have been developed. Each type is suited to a specific type of data. These types include:

- **Gaussian Naive Bayes**: Used when the dataset contains continuous features. It assumes
that for each class, the values of every feature follow a Gaussian (normal) distribution. 
The model estimates the mean and variance of each feature within each class, and uses 
those to compute the likelihood of a sample belonging to that class.

- **Bernoulli Naive Bayes**: Designed for binary features (0 or 1). Commonly used for text 
classification. It can also be used with engineered binary features like `is_weekend`, `is_borough`.

- **Multinomial Naive Bayes**: A classification algorithm that models feature counts 
using a multinomial distribution, commonly applied in text classification tasks.

- **Categorical Naive Bayes**: Used for classification tasks with categorical features.  
Each feature is assumed to follow a categorical distribution, where the likelihood of 
each category is estimated independently within each class.

- **Complement Naive Bayes**: This classifier was designed to address some of the 
strong assumptions and limitations of the standard Multinomial Naive Bayes model. 
It is particularly well-suited for imbalanced datasets, especially in text classification tasks.



### Advantages and Limitations

Advantages:

- Simple and fast to train and predict, even with large datasets.

- It is not sensitive to missing features because each feature is treated independently.

- Performs well on high-dimensional data, such as in text classification.

               
Limitations:             

- Suffers when features are strongly correlated.

- Assumes conditional independence, which rarely holds in the real world.



### Example for NYC Flood Data

Since the original dataset is in CSV format, we first perform data cleaning to filter records from 
2024, remove irrelevant columns, and extract time-related features.

We assume that a complaint is considered delayed if the response time is equal to or exceeds 72 hours.

```{python}
import pandas as pd

df = pd.read_csv('data/nycflood2024.csv',
                 dtype={"Incident Zip": str}, parse_dates=["Created Date"])
df.columns = df.columns.str.lower().str.replace(' ', '_')

# Drop empty columns
df = df.drop(columns=['location_type', 'vehicle_type', 'taxi_company_borough',
    'bridge_highway_name', 'bridge_highway_segment', 'road_ramp', 'landmark',
    'due_date', 'facility_type', 'bridge_highway_direction', 'taxi_pick_up_location'])

# Make sure the date is in 2024
df_2024 = df[(df["created_date"] >= "2023-12-31") &
             (df["created_date"] <= "2024-12-31")].copy()

# Make sure the 'closed_date' has same range as 'created_date'
match_date = df_2024[df_2024["closed_date"] == df_2024["created_date"]]

# Set those dates to NaT for further cleaning
df_2024.loc[match_date.index, ["created_date", "closed_date"]] = pd.NaT

# Calculate missing values were created
df_2024.loc[match_date.index, ["created_date", "closed_date"]].isna().sum()
```

#### Prepare Features and Target Variable

To extract the hour and weekday from the request creation time for use as model 
features, and create the target variable.


```{python}
# Convert date columns to datetime format for feature extraction
df_2024['created_date'] = pd.to_datetime(df_2024['created_date'])
df_2024['closed_date'] = pd.to_datetime(df_2024['closed_date'])

# Extract just the time component
df_2024["created_date"] = pd.to_datetime(df_2024["created_date"], format="%Y-%m-%d %H:%M:%S")
df_2024["closed_date"] = pd.to_datetime(df_2024["closed_date"], format="%Y-%m-%d %H:%M:%S")

# Split 'created_date' to hour and weekday
df_2024["hour"] = df_2024["created_date"].dt.hour
df_2024["weekday"] = df_2024["created_date"].dt.dayofweek 

# Compute response time in hours
df_2024["response_time"] = (df_2024["closed_date"] - df_2024["created_date"])
df_2024["response_time"] = df_2024["response_time"].dt.total_seconds() / 3600

# Categorize complaint types into three groups
df_2024["complaint_type"] = df_2024["descriptor"].apply(
    lambda x: "Street Flooding" if "Street Flooding" in str(x) else 
              "Catch Basin Clogged" if "Catch Basin Clogged" in str(x) else "Other"
)

# Create a binary variable
df_2024['over3d'] = (df_2024['response_time'] >= 72).astype(int)
```


#### Feature Selection and Encoding

The features used in this example are `hour`, `incident_zip`, `complaint_type`, and `weekday`.  

Since all of them are categorical variables, we use Categorical Naive Bayes, 
which is designed for features with discrete categories.

To prepare the data, we apply `LabelEncoder` to convert string-based features into integer codes.  
Then we split the data and fit the model using scikit-learn's `CategoricalNB` classifier.

```{python}
from sklearn.naive_bayes import CategoricalNB
from sklearn.preprocessing import LabelEncoder
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, confusion_matrix,
    f1_score, roc_curve, auc
)
import matplotlib.pyplot as plt
from sklearn.datasets import make_classification

# Select features
X = df_2024[["hour", "incident_zip", "complaint_type", "weekday"]]

#Encode string-based categorical features into numeric labels
for col in ["incident_zip", "complaint_type"]:
    le = LabelEncoder()
    X.loc[:, col] = le.fit_transform(X[col].astype(str))
y = df_2024['over3d']
```

#### Model Fitting and Evaluation with CategoricalNB

```{python}
# Drop rows with missing values and align X and y
X = X.dropna()
y = y.loc[X.index] 

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Fit the model
cnb = CategoricalNB()
cnb.fit(X_train, y_train)
y_pred = cnb.predict(X_test)

# Evaluate model performance
cm = confusion_matrix(y_test, y_pred)
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)

print("Confusion Matrix:\n", cm)
print(f"Accuracy: {accuracy:.2f}")
print(f"Precision: {precision:.2f}")
print(f"Recall: {recall:.2f}")
```

To further evaluate the classifier's performance, we use the ROC curve and AUC 
score to assess how well the model distinguishes between the two classes across 
different threshold values.

```{python}
from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt

# Calculate the probability
y_prob = cnb.predict_proba(X_test)[:, 1]

# Calculate fpr, tpr, thresholds
fpr, tpr, thresholds = roc_curve(y_test, y_prob)
roc_auc = auc(fpr, tpr)

# Draw ROC curve
plt.figure(figsize=(6, 5))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve for Categorical Naive Bayes')
plt.legend(loc="lower right")
plt.grid(True)
plt.show()
```


#### Interpretation of Results

From these results, the accuracy is 0.78, which means the model correctly predicts 78% 
of all cases. And the precision is 0.52, meaning that when it predicts a request will 
take over 3 days, it is correct 52% of the time. However, recall is only 0.16, indicating 
that it fails to detect most of the actual delayed cases. 

Although the recall is low, the ROC curve shows that adjusting the threshold 
can increase the recall rate.
AUC = 0.75 indicates that the model has a good overall discrimination ability, and 
there is a 75% chance that a delayed request (over 3 days) is ranked higher than a
non-delayed one. ROC curve lies above the diagonal line, which suggests the model 
performs better than random guessing.

The classification Naive Bayes model performs well overall, but it is difficult to
catch delayed requests. We still need some additional adjustments or model 
comparisons to better handle class imbalances and improve the recall rate.


### Further Readings

- [What is Naive Bayes? – IBM](https://www.ibm.com/think/topics/naive-bayes#:~:text=Na%C3%AFve%20Bayes%20is%20part%20of,important%20to%20differentiate%20between%20classes.)
- [Naive Bayes Documentation – scikit-learn](https://scikit-learn.org/stable/modules/naive_bayes.html#complement-naive-bayes)
