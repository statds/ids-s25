[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Data Science",
    "section": "",
    "text": "Preliminaries\nThe notes were developed with Quarto; for details about Quarto, visit https://quarto.org/docs/books.\nThis book free and is licensed under a Creative Commons Attribution-NonCommercial-NoDerivs 3.0 United States License.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#sources-at-github",
    "href": "index.html#sources-at-github",
    "title": "Introduction to Data Science",
    "section": "Sources at GitHub",
    "text": "Sources at GitHub\nThese lecture notes for STAT 3255/5255 in Spring 2025 represent a collaborative effort between Professor Jun Yan and the students enrolled in the course. This cooperative approach to education was facilitated through the use of GitHub, a platform that encourages collaborative coding and content development. To view these contributions and the lecture notes in their entirety, please visit our GitHub repository at https://github.com/statds/ids-s25.\nStudents contributed to the lecture notes by submitting pull requests to our GitHub repository. This method not only enriched the course material but also provided students with practical experience in collaborative software development and version control.\nFor those interested, class notes from Fall 2024, Spring 2024, Spring 2023, and Spring 2022 are also publicly accessible. These archives offer insights into the evolution of the course content and the different perspectives brought by successive student cohorts.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#compiling-the-classnotes",
    "href": "index.html#compiling-the-classnotes",
    "title": "Introduction to Data Science",
    "section": "Compiling the Classnotes",
    "text": "Compiling the Classnotes\nTo reproduce the classnotes output on your own computer, here are the necessary steps. See Section 3.2 Compiling the Classnotes for details.\n\nClone the classnotes repository to an appropriate location on your computer; see Chapter 2  Project Management for using Git.\nSet up a Python virtual environment in the root folder of the source; see Section 4.7 Virtual Environment.\nActivate your virtual environment.\nInstall all the packages specified in requirements.txt in your virtual environment:\n\npip install -r requirements.txt\n\nFor some chapters that need to interact with certain sites that require account information. For example, for Google map services, you need to save your API key in a file named api_key.txt in the root folder of the source.\nRender the book with quarto render from the root folder on a terminal; the rendered book will be stored under _book.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#midterm-project",
    "href": "index.html#midterm-project",
    "title": "Introduction to Data Science",
    "section": "Midterm Project",
    "text": "Midterm Project\nReproduce NYC street flood research (Agonafir, Lakhankar, et al., 2022; Agonafir, Pabon, et al., 2022).\nFour students will be selected to present their work in a workshop at the 2025 NYC Open Data Week. You are welcome to invite your family and friends to join the the workshop.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#final-project",
    "href": "index.html#final-project",
    "title": "Introduction to Data Science",
    "section": "Final Project",
    "text": "Final Project\nStudents are encouraged to start designing their final projects from the beginning of the semester. There are many open data that can be used. Here is a list of data challenges that you may find useful:\n\nASA Data Challenge Expo: big data in 2025\nKaggle\nDrivenData\n15 Data Science Hackathons to Test Your Skills in 2025\nIf you work on sports analytics, you are welcome to submit a poster to Connecticut Sports Analytics Symposium (CSAS) 2025.\nA good resource for sports analytics if ScoreNetwork.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#adapting-to-rapid-skill-acquisition",
    "href": "index.html#adapting-to-rapid-skill-acquisition",
    "title": "Introduction to Data Science",
    "section": "Adapting to Rapid Skill Acquisition",
    "text": "Adapting to Rapid Skill Acquisition\nIn this course, students are expected to rapidly acquire new skills, a critical aspect of data science. To emphasize this, consider this insightful quote from VanderPlas (2016):\n\nWhen a technologically-minded person is asked to help a friend, family member, or colleague with a computer problem, most of the time it’s less a matter of knowing the answer as much as knowing how to quickly find an unknown answer. In data science it’s the same: searchable web resources such as online documentation, mailing-list threads, and StackOverflow answers contain a wealth of information, even (especially?) if it is a topic you’ve found yourself searching before. Being an effective practitioner of data science is less about memorizing the tool or command you should use for every possible situation, and more about learning to effectively find the information you don’t know, whether through a web search engine or another means.\n\nThis quote captures the essence of what we aim to develop in our students: the ability to swiftly navigate and utilize the vast resources available to solve complex problems in data science. Examples tasks are: install needed software (or even hardware); search and find solutions to encountered problems.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#wishlist",
    "href": "index.html#wishlist",
    "title": "Introduction to Data Science",
    "section": "Wishlist",
    "text": "Wishlist\nThis is a wish list from all members of the class (alphabetical order, last name first, comma, then first name). Here is an example.\n\nYan, Jun\n\nMake practical data science tools accessible to undergraduates.\nPass real-world data science project experience to students.\nCo-develop a Quarto book in collaboration with the students.\nTrain students to participate in real data science competitions.\n\n\nAdd yours through a pull request; note the syntax of nested list in Markdown.\n\nStudents in 3255\n\nAckerman, John\n\nGet comfortable with command line interface\nHands-on experience with AI\nLearn practical tools & tricks for professional data scientist\n\nAlsadadi, Ammar Shaker\n\nLearn about the applications of Data Science in Finance\nLearn more about time series and random walk\n\nChen, Yifei\n\nLearn more advanced python programming skills.\nLearn to use github for future projects\nGet a good grade in this class.\n\nEl Zein, Amer Hani\n\nTo gain a deeper undestanding of data preparation.\nTo develop intution on what the best tool for a given project is.\n\nFebles, Xavier Milan\n\nFurther develop skills with git\nLearn more about specific tools used for data science\nBecome more comfortable with sql\n\nHorn, Alyssa Noelle\n\nBe confident in using Git and Github\nLearn how to collaborate with others on projects through Github\n\nJun, Joann\n\nBecome proficient in using GitHub\nLearn more about the applications of data science\n\nKline, Daniel Esteban\nLagutin, Vladislav\n\nLearn how to do data science projects in python and interact with them using git\nLearn how to do good visualizations of the data; explore appropriate libraries\n\nLang, Lang\n\nBecome more proficient with python\nLearn about the applications of Data Science\nLearn how to make collaborative project by using GitHub\nHave a good grade in this course\n\nLi, Shiyi\n\nLearn to visualize the plots and results using the ggplot package.\nLearn to use the common functions of the SciPy, scikit-learn, and statsmodels libraries in Python\nLearn how to query, extract, and manipulate structured and unstructured data in a large database.\nLearn the basics of artificial neural networks, CNNs for image data, NLP techniques.\nLearn some of the data analysis models that will be commonly used in the workplace.\nLearn some common applications of optimization techniques in data analysis.\nPass this course with an A grade.\n\nLin, Selena\n\nGet a good grade in this class.\nLearn and get familier with using GitHub.\nHands on experience with the data science skills learned in this class.\n\nLong, Ethan Kenneth\n\nBecome more comfortable using Git commands and CLI\nLearn more about the data science field\nUnderstand proper coding grammar\nDevelop good learning habits\n\nNasejje, Ruth Nicole\n\nDevelop an organized coding style in python, quarto, & git\nLearn various packages in python related to data science\nDeepen knowledge in statistical modeling and data analysis\n\nPfeifer, Nicholas Theodore\n\nLearn about data science techniques in python\nLearn and thoroughly practice using git and github\nGet more comfortable with decision trees and random forests\n\nReed, Kyle Daniel\n\nGain full confidence using Git/GitHub and corresponding applications.\nUnderstand the workflow in professional data science projects.\nBuild on existing python skills.\n\nRoy, Luke William\n\nHave fun\nDevelop skills in financial data analysis using python and relevant libraries like pandas and numpy.\nLearn advanced data visualization techniques with a focus on the grammar of graphics.\nGet an introduction to machine learning via scikit-learn, and explore applications in financial analysis and forensic accounting.\n\nSchittina, Thomas\n\nBecome more comfortable using git and GitHub\nBecome more familiar with popular data science packages in Python\n\nSymula, Sebastian\n\nLearn SQL\nBecome better at working through each step in the data science pipeline to make better, cleaner looking projects\n\nTamhane, Shubhan\n\nLearn intersection between SQL and Python for a data science project\nLearn machine learning algorithms like random forest and clustering\n\nTomaino, Mario Anthony\nXu, Peiwen\n\nLearn some data analysis techniques\nLearn how to use git and other essential tools for data science\n\n\n\n\nStudents in 5255\n\nEdo, Mezmur Wossenu\n\nI hope to become adept working with github.\nI hope to work on real-World data science projects.\nI hope to learn about the different machine learning techniques.\n\nMundiwala, Mohammad Moiz\n\nBecome more familiar with collaboration process of programming so that I can be more orderly while working with others.\nI hope to become more efficient processing data that is messy, unstructured, or unlabeled.\nPresent engaging, intuitive, and interactive figures and animations for complex math and stat concepts.\n\nVellore, Ajeeth Krishna\n\nUnderstand the utility provided by GitHub and practice using its tools\nLearn how to participate in a large-scale development project like how they are done in industry\nLearn how to code properly and professionally instead of using “backyard” computer science techniques and formatting\nUnderstand principles of coding documentation and readability while practicing their application\n\nZhang, Gaofei\n\nGain confidence in using Git and GitHub for version control and collaboration.\nDevelop a structured approach to data cleaning and preprocessing for complex datasets.\nEnhance skills in statistical modeling and machine learning techniques relevant to public health research.\nImprove efficiency in working with large-scale data using Python and SQL.\n\nKravette, Noah\n\nBecome better at program collaboration.\nBecome adept with git and github.\nBe able to quickly and efficently process and analyze any data.\nGain better skills at data prep, organization, and visulization.\nLearn new helpful statistical tools for data.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#course-logistics",
    "href": "index.html#course-logistics",
    "title": "Introduction to Data Science",
    "section": "Course Logistics",
    "text": "Course Logistics\n\nPresentation Orders\nThe topic presentation order is set up in class.\n\nwith open('rosters/3255.txt', 'r') as file:\n    ug = [line.strip() for line in file]\nwith open('rosters/5255.txt', 'r') as file:\n    gr = [line.strip() for line in file]\npresenters = ug + gr\n\nimport random\n## seed jointly set by the class\nrandom.seed(6895 + 283 + 3184 + 3078 + 5901 + 36)\nrandom.sample(presenters, len(presenters))\n## random.shuffle(presenters) # This would shuffle the list in place\n\n['Li,Shiyi',\n 'Jun,Joann',\n 'Alsadadi,Ammar Shaker',\n 'Lang,Lang',\n 'Ackerman,John',\n 'Horn,Alyssa Noelle',\n 'Xu,Peiwen',\n 'Schittina,Thomas',\n 'Kline,Daniel Esteban',\n 'Edo,Mezmur Wossenu',\n 'Roy,Luke William',\n 'Febles,Xavier Milan',\n 'Tamhane,Shubhan',\n 'Nasejje,Ruth Nicole',\n 'Lagutin,Vladislav',\n 'Zhang,Gaofei',\n 'Long,Ethan Kenneth',\n 'El Zein,Amer Hani',\n 'Kravette,Noah',\n 'Symula,Sebastian',\n 'Tomaino,Mario Anthony',\n 'Reed,Kyle Daniel',\n 'Chen,Yifei',\n 'Mundiwala,Mohammad Moiz',\n 'Lin,Selena',\n 'Pfeifer,Nicholas Theodore',\n 'Vellore,Ajeeth Krishna']\n\n\nSwitching slots is allowed as long as you find someone who is willing to switch with you. In this case, make a pull request to switch the order and let me know.\nYou are welcome to choose a topic that you are interested the most, subject to some order restrictions. For example, decision tree should be presented before random forest or extreme gradient boosting. This justifies certain requests for switching slots.\n\n\nPresentation Task Board\nTalk to the professor about your topics. Here are some example tasks:\n\nMaking presentations with Quarto\nMarkdown jumpstart\nEffective data science communication\nImport/Export data\nData manipulation with Pandas\nUsing package ace_tools\nAccessing US census data\nArrow as a cross-platform data format\nStatistical analysis for proporations and rates\nDatabase operation with Structured query language (SQL)\nGrammer of graphics\nHandling spatial data\nSpatial data with GeoPandas\nVisualize spatial data in a Google map\nAnimation\nSupport vector machine\nRandom forest\nNaive Bayes\nNeural networks\nDeep learning\nTensorFlow\nAutoencoders\nReinforcement learning\nDeveloping a Python package\nWeb scraping\nPersonal webpage on GitHub\n\n\n\nTopic Presentation Schedule\nThe topic presentation is 20 points. It includes:\n\nTopic selection consultation on week in advance (4 points).\nDelivering the presentation in class (10 points).\nContribute to the class notes within two weeks following the presentation (6 points).\n\nTips on topic contribution:\n\nNo plagiarism (see instructions on Contributing to Class Notes).\nAvoid external graphics.\nUse simulated data.\nUse data from homework assignments.\nCite article/book references (learn how from our sources).\nInclude a subsection of Further Readings.\nTest on your own computer before making a pull request.\nSend me your presentation two days in advance for feedbacks.\n\nPlease use the following table to sign up.\n\n\n\n\n\n\n\n\nDate\nPresenter\nTopic\n\n\n\n\n02/10\nLi, Shiyi\nA Primer of Markdown\n\n\n02/12\nJun, Joann\nMaking Presentations with Quarto\n\n\n02/17\nRoy, Luke William\nGrammar of Graphics with Plotnine\n\n\n02/19\nLang, Lang\nData manipulation with Pandas\n\n\n02/24\nAckerman, John\nPerforming Statistical Tests (SciPy)\n\n\n02/26\nHorn, Alyssa Noelle\nDatabase operation with Structured query language (SQL)\n\n\n03/03\nEl Zein, Amer Hani\n\n\n\n03/05\nSchittina, Thomas\nSpatial Data With Geopandas & Google Maps\n\n\n03/10\nKline, Daniel Esteban\n\n\n\n03/12\nEdo, Mezmur Wossenu\n\n\n\n03/24\nAlsadadi, Ammar Shaker\n\n\n\n03/24\nFables, Xavier Milan\n\n\n\n03/31\nTamhane, Shubhan\n\n\n\n03/31\nNasejje, Ruth Nicole\n\n\n\n04/02\nLagutin, Vladislav\n\n\n\n04/02\nZhang, Gaofei\nRandom forest\n\n\n04/07\nLong, Ethan Kenneth\n\n\n\n04/07\nXu, Peiwen\n\n\n\n04/09\nKravette, Noah\n\n\n\n04/09\nSymula, Sebastian\nImputation Methods for Missing Data\n\n\n04/09\nTomaino, Mario Anthony\n\n\n\n04/12\nReed, Kyle Daniel\n\n\n\n04/12\nChen, Yifei\n\n\n\n04/12\nMundiwala, Mohammad Moiz\nMath animations with manim\n\n\n04/16\nLin, Selena\n\n\n\n04/16\nPfeifer, Nicholas Theodore\n\n\n\n04/16\nVellore, Ajeeth Krishna\n\n\n\n\n\n\nFinal Project Presentation Schedule\nWe use the same order as the topic presentation for undergraduate final presentation. An introduction on how to use Quarto to prepare presentation slides is availabe under the templates directory in the classnotes source tree, thank to Zachary Blanchard, which can be used as a template to start with.\n\n\n\n\n\n\n\nDate\nPresenter\n\n\n\n\n04/21\nShiyi Li; Joann Jun; Ammar Alsadadi; Lang Lang; John Ackerman\n\n\n04/23\nAlyssa Horn; Peiwen Xu; Thomas Schittina; Daniel Kline; Luke Roy\n\n\n04/28\nXavier Fables; Shubhan Tamhane; Ruth Nasejje; Vladislav Lagutin; Ethan Long\n\n\n04/30\nAmer El Zein; Sebastian Symula; Mario Tomiano; Kyle Reed; Yifei Chen\n\n\n05/??\nSelena Lin; Nick Pfeifer\n\n\n\n\n\nContributing to the Class Notes\nContribution to the class notes is through a `pull request’.\n\nStart a new branch and switch to the new branch.\nOn the new branch, add a qmd file for your presentation\nIf using Python, create and activate a virtual environment with requirements.txt\nEdit _quarto.yml add a line for your qmd file to include it in the notes.\nWork on your qmd file, test with quarto render.\nWhen satisfied, commit and make a pull request with your quarto files and an updated requirements.txt.\n\nI have added a template file mysection.qmd and a new line to _quarto.yml as an example.\nFor more detailed style guidance, please see my notes on statistical writing.\nPlagiarism is to be prevented. Remember that these class notes are publicly available online with your names attached. Here are some resources on how to avoid plagiarism. In particular, in our course, one convenient way to avoid plagiarism is to use our own data (e.g., NYC Open Data). Combined with your own explanation of the code chunks, it would be hard to plagiarize.\n\n\nHomework Logistics\n\nWorkflow of Submitting Homework Assisngment\n\nClick the GitHub classroom assignment link in HuskCT announcement.\nAccept the assignment and follow the instructions to an empty repository.\nMake a clone of the repo at an appropriate folder on your own computer with git clone.\nGo to this folder, add your qmd source, work on it, and group your changes to different commits.\nPush your work to your GitHub repo with git push.\nCreate a new release and put the generated pdf file in it for ease of grading.\n\n\n\nRequirements\n\nUse the repo from Git Classroom to submit your work. See Chapter 2  Project Management.\n\nKeep the repo clean (no tracking generated files).\n\nNever “Upload” your files; use the git command lines.\nMake commit message informative (think about the readers).\n\nMake at least 10 commits and form a style of frequent small commits.\n\nTrack quarto sources only in your repo. See Chapter 3  Reproducible Data Science.\nFor the convenience of grading, add your standalone html or pdf output to a release in your repo.\nFor standalone pdf output, you will need to have LaTeX installed.\n\n\n\n\nQuizzes about Syllabus\n\nDo I accept late homework?\nCould you list a few examples of email etiquette?\nHow would you lose style points?\nWould you use CLI and GUI?\nHow many students will present at 2025 NYC ODW and when will the presentations be?\nWhat’s the first date on which you have to complete something about your final project?\nCan you use AI for any task in this course?\nAnybody needs a reference letter? How could you help me to help you?",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#practical-tips",
    "href": "index.html#practical-tips",
    "title": "Introduction to Data Science",
    "section": "Practical Tips",
    "text": "Practical Tips\n\nData analysis\n\nUse an IDE so you can play with the data interactively\nCollect codes that have tested out into a script for batch processing\nDuring data cleaning, keep in mind how each variable will be used later\nNo keeping large data files in a repo; assume a reasonable location with your collaborators\n\n\n\nPresentation\n\nDon’t forget to introduce yourself if there is no moderator.\nHighlight your research questions and results, not code.\nGive an outline, carry it out, and summarize.\nUse your own examples to reduce the risk of plagiarism.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#my-presentation-topic-template",
    "href": "index.html#my-presentation-topic-template",
    "title": "Introduction to Data Science",
    "section": "My Presentation Topic (Template)",
    "text": "My Presentation Topic (Template)\nThis section was prepared by John Smith.\nUse Markdown syntax. If not clear on what to do, learn from the class notes sources.\n\nPay attention to the sectioning levels.\nCite references with their bib key.\nIn examples, maximize usage of data set that the class is familiar with.\nCould use datasets in Python packages or downloadable on the fly.\nTest your section by quarto render &lt;filename.qmd&gt;.\n\n\nIntroduction\nHere is an overview.\n\n\nSub Topic 1\nPut materials on topic 1 here\nPython examples can be put into python code chunks:\n\n# import pandas as pd\n\n# do something\n\n\n\nSub Topic 2\nPut materials on topic 2 here.\n\n\nSub Topic 3\nPut matreials on topic 3 here.\n\n\nConclusion\nPut sumaries here.\n\n\nFurther Readings\nPut links to further materials.\n\n\n\n\nAgonafir, C., Lakhankar, T., Khanbilvardi, R., Krakauer, N., Radell, D., & Devineni, N. (2022). A machine learning approach to evaluate the spatial variability of New York City’s 311 street flooding complaints. Computers, Environment and Urban Systems, 97, 101854.\n\n\nAgonafir, C., Pabon, A. R., Lakhankar, T., Khanbilvardi, R., & Devineni, N. (2022). Understanding New York City street flooding through 311 complaints. Journal of Hydrology, 605, 127300.\n\n\nVanderPlas, J. (2016). Python data science handbook: Essential tools for working with data. O’Reilly Media, Inc.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 What Is Data Science?\nData science is a multifaceted field, often conceptualized as resting on three fundamental pillars: mathematics/statistics, computer science, and domain-specific knowledge. This framework helps to underscore the interdisciplinary nature of data science, where expertise in one area is often complemented by foundational knowledge in the others.\nA compelling definition was offered by Prof. Bin Yu in her 2014 Presidential Address to the Institute of Mathematical Statistics. She defines \\[\\begin{equation*}\n\\mbox{Data Science} =\n\\mbox{S}\\mbox{D}\\mbox{C}^3,\n\\end{equation*}\\] where\nComputing underscores the need for proficiency in programming and algorithmic thinking, collaboration/teamwork reflects the inherently collaborative nature of data science projects, often requiring teams with diverse skill sets, and communication to outsiders emphasizes the importance of translating complex data insights into understandable and actionable information for non-experts.\nThis definition neatly captures the essence of data science, emphasizing a balance between technical skills, teamwork, and the ability to communicate effectively.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#what-is-data-science",
    "href": "intro.html#what-is-data-science",
    "title": "1  Introduction",
    "section": "",
    "text": "‘S’ represents Statistics, signifying the crucial role of statistical methods in understanding and interpreting data;\n‘D’ stands for domain or science knowledge, indicating the importance of specialized expertise in a particular field of study;\nthe three ’C’s denotes computing, collaboration/teamwork, and communication to outsiders.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#expectations-from-this-course",
    "href": "intro.html#expectations-from-this-course",
    "title": "1  Introduction",
    "section": "1.2 Expectations from This Course",
    "text": "1.2 Expectations from This Course\nIn this course, students will be expected to achieve the following outcomes:\n\nProficiency in Project Management with Git: Develop a solid understanding of Git for efficient and effective project management. This involves mastering version control, branching, and collaboration through this powerful tool.\nProficiency in Project Reporting with Quarto: Gain expertise in using Quarto for professional-grade project reporting. This encompasses creating comprehensive and visually appealing reports that effectively communicate your findings.\nHands-On Experience with Real-World Data Science Projects: Engage in practical data science projects that reflect real-world scenarios. This hands-on approach is designed to provide you with direct experience in tackling actual data science challenges.\nCompetency in Using Python and Its Extensions for Data Science: Build strong skills in Python, focusing on its extensions relevant to data science. This includes libraries like Pandas, NumPy, and Matplotlib, among others, which are critical for data analysis and visualization.\nFull Grasp of the Meaning of Results from Data Science Algorithms: Learn to not only apply data science algorithms but also to deeply understand the implications and meanings of their results. This is crucial for making informed decisions based on these outcomes.\nBasic Understanding of the Principles of Data Science Methods: Acquire a foundational knowledge of the underlying principles of various data science methods. This understanding is key to effectively applying these methods in practice.\nCommitment to the Ethics of Data Science: Emphasize the importance of ethical considerations in data science. This includes understanding data privacy, bias in data and algorithms, and the broader social implications of data science work.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#computing-environment",
    "href": "intro.html#computing-environment",
    "title": "1  Introduction",
    "section": "1.3 Computing Environment",
    "text": "1.3 Computing Environment\nAll setups are operating system dependent. As soon as possible, stay away from Windows. Otherwise, good luck (you will need it).\n\n1.3.1 Operating System\nYour computer has an operating system (OS), which is responsible for managing the software packages on your computer. Each operating system has its own package management system. For example:\n\nLinux: Linux distributions have a variety of package managers depending on the distribution. For instance, Ubuntu uses APT (Advanced Package Tool), Fedora uses DNF (Dandified Yum), and Arch Linux uses Pacman. These package managers are integral to the Linux experience, allowing users to install, update, and manage software packages easily from repositories.\nmacOS: macOS uses Homebrew as its primary package manager. Homebrew simplifies the installation of software and tools that aren’t included in the standard macOS installation, using simple commands in the terminal.\nWindows: Windows users often rely on the Microsoft Store for apps and software. For more developer-focused package management, tools like Chocolatey and Windows Package Manager (Winget) are used. Additionally, recent versions of Windows have introduced the Windows Subsystem for Linux (WSL). WSL allows Windows users to run a Linux environment directly on Windows, unifying Windows and Linux applications and tools. This is particularly useful for developers and data scientists who need to run Linux-specific software or scripts. It saves a lot of trouble Windows users used to have before its time.\n\nUnderstanding the package management system of your operating system is crucial for effectively managing and installing software, especially for data science tools and applications.\n\n\n1.3.2 File System\nA file system is a fundamental aspect of a computer’s operating system, responsible for managing how data is stored and retrieved on a storage device, such as a hard drive, SSD, or USB flash drive. Essentially, it provides a way for the OS and users to organize and keep track of files. Different operating systems typically use different file systems. For instance, NTFS and FAT32 are common in Windows, APFS and HFS+ in macOS, and Ext4 in many Linux distributions. Each file system has its own set of rules for controlling the allocation of space on the drive and the naming, storage, and access of files, which impacts performance, security, and compatibility. Understanding file systems is crucial for tasks such as data recovery, disk partitioning, and managing file permissions, making it an important concept for anyone working with computers, especially in data science and IT fields.\nNavigating through folders in the command line, especially in Unix-like environments such as Linux or macOS, and Windows Subsystem for Linux (WSL), is an essential skill for effective file management. The command cd (change directory) is central to this process. To move into a specific directory, you use cd followed by the directory name, like cd Documents. To go up one level in the directory hierarchy, you use cd ... To return to the home directory, simply typing cd or cd ~ will suffice. The ls command lists all files and folders in the current directory, providing a clear view of your options for navigation. Mastering these commands, along with others like pwd (print working directory), which displays your current directory, equips you with the basics of moving around the file system in the command line, an indispensable skill for a wide range of computing tasks in Unix-like systems.\n\n\n1.3.3 Command Line Interface\nOn Linux or MacOS, simply open a terminal.\nOn Windows, several options can be considered.\n\nWindows Subsystem Linux (WSL): https://learn.microsoft.com/en-us/windows/wsl/\nCygwin (with X): https://x.cygwin.com\nGit Bash: https://www.gitkraken.com/blog/what-is-git-bash\n\nTo jump start, here is a tutorial: Ubunto Linux for beginners.\nAt least, you need to know how to handle files and traverse across directories. The tab completion and introspection supports are very useful.\nHere are several commonly used shell commands:\n\ncd: change directory; .. means parent directory.\npwd: present working directory.\nls: list the content of a folder; -l long version; -a show hidden files; -t ordered by modification time.\nmkdir: create a new directory.\ncp: copy file/folder from a source to a target.\nmv: move file/folder from a source to a target.\nrm: remove a file a folder.\n\n\n\n1.3.4 Python\nSet up Python on your computer:\n\nPython 3.\nPython package manager miniconda or pip.\nIntegrated Development Environment (IDE) (Jupyter Notebook; RStudio; VS Code; Emacs; etc.)\n\nI will be using VS Code in class.\nReadability is important! Check your Python coding styles against the recommended styles: https://peps.python.org/pep-0008/. A good place to start is the Section on “Code Lay-out”.\nOnline books on Python for data science:\n\n“Python Data Science Handbook: Essential Tools for Working with Data,” First Edition, by Jake VanderPlas, O’Reilly Media, 2016.\n\n\n“Python for Data Analysis: Data Wrangling with Pandas, NumPy, and IPython.” Third Edition, by Wes McK- inney, O’Reilly Media, 2022.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#data-science-ethics",
    "href": "intro.html#data-science-ethics",
    "title": "1  Introduction",
    "section": "1.4 Data Science Ethics",
    "text": "1.4 Data Science Ethics\n\n1.4.1 Introduction\nEthics in data science is a fundamental consideration throughout the lifecycle of any project. Data science ethics refers to the principles and practices that guide responsible and fair use of data to ensure that individual rights are respected, societal welfare is prioritized, and harmful outcomes are avoided. Ethical frameworks like the Belmont Report (Protection of Human Subjects of Biomedical & Research, 1979)} and regulations such as the Health Insurance Portability and Accountability Act (HIPAA) (Health & Services, 1996) have established foundational principles that inspire ethical considerations in research and data use. This section explores key principles of ethical data science and provides guidance on implementing these principles in practice.\n\n\n1.4.2 Principles of Ethical Data Science\n\n1.4.2.1 Respect for Privacy\nSafeguarding privacy is critical in data science. Projects should comply with data protection regulations, such as the General Data Protection Regulation (GDPR) or the California Consumer Privacy Act (CCPA). Techniques like anonymization and pseudonymization must be applied to protect sensitive information. Beyond legal compliance, data scientists should consider the ethical implications of using personal data.\nThe principles established by the Belmont Report emphasize respect for persons, which aligns with safeguarding individual privacy. Protecting privacy also involves limiting data collection to what is strictly necessary. Minimizing the use of identifiable information and implementing secure data storage practices are essential steps. Transparency about how data is used further builds trust with stakeholders.\n\n\n1.4.2.2 Commitment to Fairness\nBias can arise at any stage of the data science pipeline, from data collection to algorithm development. Ethical practice requires actively identifying and addressing biases to prevent harm to underrepresented groups. Fairness should guide the design and deployment of models, ensuring equitable treatment across diverse populations.\nTo achieve fairness, data scientists must assess datasets for representativeness and use tools to detect potential biases. Regular evaluation of model outcomes against fairness metrics helps ensure that systems remain non-discriminatory. The Americans with Disabilities Act (ADA) (Congress, 1990) provides a legal framework emphasizing equitable access, which can inspire fairness in algorithmic design. Collaborating with domain experts and stakeholders can provide additional insights into fairness issues.\n\n\n1.4.2.3 Emphasis on Transparency\nTransparency builds trust and accountability in data science. Models should be interpretable, with clear documentation explaining their design, assumptions, and decision-making processes. Data scientists must communicate results in a way that stakeholders can understand, avoiding unnecessary complexity or obfuscation.\nTransparent practices include providing stakeholders access to relevant information about model performance and limitations. The Federal Data Strategy (Team, 2019) calls for transparency in public sector data use, offering inspiration for practices in broader contexts. Visualizing decision pathways and using tools like LIME or SHAP can enhance interpretability. Establishing clear communication protocols ensures that non-technical audiences can engage with the findings effectively.\n\n\n1.4.2.4 Focus on Social Responsibility\nData science projects must align with ethical goals and anticipate their broader societal and environmental impacts. This includes considering how outputs may be used or misused and avoiding harm to vulnerable populations. Data scientists should aim to use their expertise to promote public welfare, addressing critical societal challenges such as health disparities, climate change, and education access.\nEngaging with diverse perspectives helps align projects with societal values. Ethical codes, such as those from the Association for Computing Machinery (ACM) (Computing Machinery (ACM), 2018), offer guidance on using technology for social good. Collaborating with policymakers and community representatives ensures that data-driven initiatives address real needs and avoid unintended consequences. Regular impact assessments help measure whether projects meet their ethical objectives.\n\n\n1.4.2.5 Adherence to Professional Integrity\nProfessional integrity underpins all ethical practices in data science. Adhering to established ethical guidelines, such as those from the American Statistical Association (ASA) ((ASA), 2018), ensures accountability. Practices like maintaining informed consent, avoiding data manipulation, and upholding rigor in analyses are essential for maintaining public trust in the field.\nEthical integrity also involves fostering a culture of honesty and openness within data science teams. Peer review and independent validation of findings can help identify potential errors or biases. Documenting methodologies and maintaining transparency in reporting further strengthen trust.\n\n\n\n1.4.3 Ensuring Ethics in Practice\n\n1.4.3.1 Building Ethical Awareness\nPromoting ethical awareness begins with education and training. Institutions should integrate ethics into data science curricula, emphasizing real-world scenarios and decision-making. Organizations should conduct regular training to ensure their teams remain informed about emerging ethical challenges.\nWorkshops and case studies can help data scientists understand the complexities of ethical decision-making. Providing access to resources, such as ethical guidelines and tools, supports continuous learning. Leadership support is critical for embedding ethics into organizational culture.\n\n\n1.4.3.2 Embedding Ethics in Workflows\nEthics must be embedded into every stage of the data science pipeline. Establishing frameworks for ethical review, such as ethics boards or peer-review processes, helps identify potential issues early. Tools for bias detection, explainability, and privacy protection should be standard components of workflows.\nStandard operating procedures for ethical reviews can formalize the consideration of ethics in project planning. Developing templates for documenting ethical decisions ensures consistency and accountability. Collaboration across teams enhances the ability to address ethical challenges comprehensively.\n\n\n1.4.3.3 Establishing Accountability Mechanisms\nClear accountability mechanisms are essential for ethical governance. This includes maintaining documentation for all decisions, establishing audit trails, and assigning responsibility for the outputs of data-driven systems. Organizations should encourage open dialogue about ethical concerns and support whistleblowers who raise issues.\nPeriodic audits of data science projects help ensure compliance with ethical standards. Organizations can benefit from external reviews to identify blind spots and improve their practices. Accountability fosters trust and aligns teams with ethical objectives.\n\n\n1.4.3.4 Engaging Stakeholders\nEthical data science requires collaboration with diverse stakeholders. Including perspectives from affected communities, policymakers, and interdisciplinary experts ensures that projects address real needs and avoid unintended consequences. Stakeholder engagement fosters trust and aligns projects with societal values.\nPublic consultations and focus groups can provide valuable feedback on the potential impacts of data science projects. Engaging with regulators and advocacy groups helps align projects with legal and ethical expectations. Transparent communication with stakeholders builds long-term relationships.\n\n\n1.4.3.5 Continuous Improvement\nEthics in data science is not static; it evolves with technology and societal expectations. Continuous improvement requires regular review of ethical practices, learning from past projects, and adapting to new challenges. Organizations should foster a culture of reflection and growth to remain aligned with ethical best practices.\nEstablishing mechanisms for feedback on ethical practices can identify areas for development. Sharing lessons learned through conferences and publications helps the broader community advance its understanding of ethics in data science.\n\n\n\n1.4.4 Conclusion\nData science ethics is a dynamic and integral aspect of the discipline. By adhering to principles of privacy, fairness, transparency, social responsibility, and integrity, data scientists can ensure their work contributes positively to society. Implementing these principles through structured workflows, stakeholder engagement, and continuous improvement establishes a foundation for trustworthy and impactful data science.\n\n\n\n\n(ASA), A. S. A. (2018). Ethical guidelines for statistical practice.\n\n\nComputing Machinery (ACM), A. for. (2018). Code of ethics and professional conduct.\n\n\nCongress, U. S. (1990). Americans with disabilities act of 1990 (ADA).\n\n\nHealth, U. S. D. of, & Services, H. (1996). Health insurance portability and accountability act of 1996 (HIPAA).\n\n\nProtection of Human Subjects of Biomedical, N. C. for the, & Research, B. (1979). The belmont report: Ethical principles and guidelines for the protection of human subjects of research.\n\n\nTeam, F. D. S. D. (2019). Federal data strategy 2020 action plan.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "git.html",
    "href": "git.html",
    "title": "2  Project Management",
    "section": "",
    "text": "2.1 Set Up Git/GitHub\nMany tutorials are available in different formats. Here is a YouTube video ``Git and GitHub for Beginners — Crash Course’’. The video also covers GitHub, a cloud service for Git which provides a cloud back up of your work and makes collaboration with co-workers easy. Similar services are, for example, bitbucket and GitLab.\nThere are tools that make learning Git easy.\nDownload Git if you don’t have it already.\nTo set up GitHub (other services like Bitbucket or GitLab are similar), you need to\nSee how to get started with GitHub account.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Management</span>"
    ]
  },
  {
    "objectID": "git.html#set-up-gitgithub",
    "href": "git.html#set-up-gitgithub",
    "title": "2  Project Management",
    "section": "",
    "text": "Generate an SSH key if you don’t have one already.\nSign up an GitHub account.\nAdd the SSH key to your GitHub account",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Management</span>"
    ]
  },
  {
    "objectID": "git.html#most-frequently-used-git-commands",
    "href": "git.html#most-frequently-used-git-commands",
    "title": "2  Project Management",
    "section": "2.2 Most Frequently Used Git Commands",
    "text": "2.2 Most Frequently Used Git Commands\nThe following seven commands will get you started and they may be all that you need most of the time.\n\ngit clone:\n\nUsed to clone a repository to a local folder.\nRequires either HTTPS link or SSH key to authenticate.\n\ngit pull:\n\nDownloads any updates made to the remote repository and automatically updates the local repository.\n\ngit status:\n\nReturns the state of the working directory.\nLists the files that have been modified, and are yet to be or have been staged and/or committed.\nShows if the local repository is begind or ahead a remote branch.\n\ngit add:\n\nAdds new or modified files to the Git staging area.\nGives the option to select which files are to be sent to the remote repository\n\ngit rm:\n\nUsed to remove files from the staging index or the local repository.\n\ngit commit:\n\nCommits changes made to the local repository and saves it like a snapshot.\nA message is recommended with every commit to keep track of changes made.\n\ngit push:\n\nUsed to send commits made on local repository to the remote repository.\n\n\nFor more advanced usages:\n\ngit diff\ngit branch\ngit reset",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Management</span>"
    ]
  },
  {
    "objectID": "git.html#tips-on-using-git",
    "href": "git.html#tips-on-using-git",
    "title": "2  Project Management",
    "section": "2.3 Tips on using Git:",
    "text": "2.3 Tips on using Git:\n\nUse the command line interface instead of the web interface (e.g., upload on GitHub)\nMake frequent small commits instead of rare large commits.\nMake commit messages informative and meaningful.\nName your files/folders by some reasonable convention.\n\nLower cases are better than upper cases.\nNo blanks in file/folder names.\n\nKeep the repo clean by not tracking generated files.\nCreat a .gitignore file for better output from git status.\nKeep the linewidth of sources to under 80 for better git diff view.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Management</span>"
    ]
  },
  {
    "objectID": "git.html#pull-request",
    "href": "git.html#pull-request",
    "title": "2  Project Management",
    "section": "2.4 Pull Request",
    "text": "2.4 Pull Request\nTo contribute to an open source project (e.g., our classnotes), use pull requests. Pull requests “let you tell others about changes you’ve pushed to a branch in a repository on GitHub. Once a pull request is opened, you can discuss and review the potential changes with collaborators and add follow-up commits before your changes are merged into the base branch.”\nWatch this YouTube video: GitHub pull requests in 100 seconds.\nThe following are step-by-step instructions on how to make a pull request to the class notes contributed by Nick Pfeifer..\n\nCreate a fork of the class repository on the GitHub website.\n\nMake sure your fork is up to date by clicking Sync fork if necessary.\n\nClone your fork into a folder on your computer.\n\ngit clone https://github.com/GitHub_Username/ids-s25.git\nReplace GitHub_Username with your personal GitHub Username.\n\nCheck to see if you can access the folder/cloned repository in your code editor.\n\nThe class notes home page is located in the index.qmd file.\n\nMake a branch and give it a good name.\n\nMove into the directory with the cloned repository.\nCreate a branch using:\n\ngit checkout -b branch_name\nReplace branch_name with a more descriptive name.\n\nYou can check your branches using:\n\ngit branch\nThe branch in use will have an asterisk to the left of it.\n\nIf you are not in the right branch you can use the following command:\n\ngit checkout existing-branch\nReplace existing-branch with the name of the branch you want to use.\n\n\nRun git status to verify that no changes have been made.\nMake changes to a file in the class notes repository.\n\nFor example: add your wishes to the Wishlist in index.qmd using nested list syntax in markdown.\nRemember to save your changes.\n\nRun git status again to see that changes have been made.\nUse the add command.\n\ngit add filename\nExample usage: git add index.qmd\n\nMake a commit.\n\ngit commit -m \"Informative Message\"\nBe clear about what you changed and perhaps include your name in the message.\n\nPush the files to GitHub.\n\ngit push origin branch-name\nReplace branch-name with the name of your current branch.\n\nGo to your forked repository on GitHub and refresh the page, you should see a button that says Compare and Pull Request.\n\nDescribe the changes you made in the pull request.\nClick Create pull request.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Management</span>"
    ]
  },
  {
    "objectID": "quarto.html",
    "href": "quarto.html",
    "title": "3  Reproducible Data Science",
    "section": "",
    "text": "3.1 Introduction to Quarto\nData science projects should be reproducible to be trustworthy. Dynamic documents facilitate reproducibility. Quarto is an open-source dynamic document preparation system, ideal for scientific and technical publishing. From the official websites, Quarto can be used to:\nTo get started with Quarto, see documentation at Quarto.\nFor a clean style, I suggest that you use VS Code as your IDE. The ipynb files have extra formats in plain texts, which are not as clean as qmd files. There are, of course, tools to convert between the two representations of a notebook. For example:\nWe will use Quarto for homework assignments, classnotes, and presentations. You will see them in action through in-class demonstrations. The following sections in the Quarto Guide are immediately useful.\nA template for homework is in this repo (hwtemp.qmd) to get you started with homework assignments.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reproducible Data Science</span>"
    ]
  },
  {
    "objectID": "quarto.html#introduction-to-quarto",
    "href": "quarto.html#introduction-to-quarto",
    "title": "3  Reproducible Data Science",
    "section": "",
    "text": "quarto convert hello.ipynb # converts to qmd\nquarto convert hello.qmd   # converts to ipynb\n\n\nMarkdown basics\nUsing Python\nPresentations",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reproducible Data Science</span>"
    ]
  },
  {
    "objectID": "quarto.html#sec-buildnotes",
    "href": "quarto.html#sec-buildnotes",
    "title": "3  Reproducible Data Science",
    "section": "3.2 Compiling the Classnotes",
    "text": "3.2 Compiling the Classnotes\nThe sources of the classnotes are at https://github.com/statds/ids-s25. This is also the source tree that you will contributed to this semester. I expect that you clone the repository to your own computer, update it frequently, and compile the latest version on your computer (reproducibility).\nTo compile the classnotes, you need the following tools: Git, Quarto, and Python.\n\n3.2.1 Set up your Python Virtual Environment\nI suggest that a Python virtual environment for the classnotes be set up in the current directory for reproducibility. A Python virtual environment is simply a directory with a particular file structure, which contains a specific Python interpreter and software libraries and binaries needed to support a project. It allows us to isolate our Python development projects from our system installed Python and other Python environments.\nTo create a Python virtual environment for our classnotes:\npython3 -m venv .ids-s25-venv\nHere .ids-s25-venv is the name of the virtual environment to be created. Choose an informative name. This only needs to be set up once.\nTo activate this virtual environment:\n. .ids-s25-venv/bin/activate\nAfter activating the virtual environment, you will see (.ids-s25-venv) at the beginning of your shell prompt. Then, the Python interpreter and packages needed will be the local versions in this virtual environment without interfering your system-wide installation or other virtual environments.\nTo install the Python packages that are needed to compile the classnotes, we have a requirements.txt file that specifies the packages and their versions. They can be installed easily with:\npip install -r requirements.txt\nIf you are interested in learning how to create the requirements.txt file, just put your question into a Google search.\nTo exit the virtual environment, simply type deactivate in your command line. This will return you to your system’s global Python environment.\n\n\n3.2.2 Clone the Repository\nClone the repository to your own computer. In a terminal (command line), go to an appropriate directory (folder), and clone the repo. For example, if you use ssh for authentication:\ngit clone git@github.com:statds/ids-s25.git\n\n\n3.2.3 Render the Classnotes\nAssuming quarto has been set up, we render the classnotes in the cloned repository\ncd ids-s25\nquarto render\nIf there are error messages, search and find solutions to clear them. Otherwise, the html version of the notes will be available under _book/index.html, which is default location of the output.\n\n\n3.2.4 Login Requirements\nFor some illustrations, you need to interact with certain sites that require account information. For example, for Google map services, you need to save your API key in a file named api_key.txt in the root folder of the source. Another example is to access the US Census API, where you would need to register an account and get your Census API Key.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reproducible Data Science</span>"
    ]
  },
  {
    "objectID": "quarto.html#a-primer-of-markdown",
    "href": "quarto.html#a-primer-of-markdown",
    "title": "3  Reproducible Data Science",
    "section": "3.3 A Primer of Markdown",
    "text": "3.3 A Primer of Markdown\nThis section was prepared by Shiyi Li. I am a senior majoring in Mathematics/ Statistics and minoring in CSE. I aim to graduate from the University of Connecticut in August and continue my master’s in Data Science program.\n\n3.3.1 Introduction\nToday’s presentation, I will introduce some basic syntax on markdown. Markdown is a plain text format, an easy way to write content for a web interface. It is widely used in open-source documentation, including GitHub, R Markdown, Quarto, Jupyter Notebooks, etc. Its syntax is easy to learn, write, read, and seamlessly convert to HTML, PDF, and DOCX formats. I will divide my presentation into three parts, Structuring the Document, Formatting Text, and Enhancing Content.\n\n\n3.3.2 Structuring the Document\nThere are some elements can define sections and hierarchy to help organize content in a clear and structured manner.\n\n3.3.2.1 Headings\n\nTo create heading levels for topics, sections, or subsections, you can add number signs, “#”, before you write any content for them.\n\n\nExample:\n\nYou can code like this:\n\n# Heading 1 (Main Title)\n\n## Heading 2 (Section)\n\n### Heading 3 (Subsection)\n\n#### Heading 4\n\n##### Heading 5\n\n###### Heading 6\n\nBe careful, you need to make sure that there is a blank line before and after your heading levels and a space between your number sign “#” and the heading name.\n\n\nAlternatively, you can also use any number of double equal sign, “==”, or double dashes sign, “–”, to create a heading.\n\n\nExample:\n\nYou can code like this:\n\nHeading 1 (Main Title)\n======\nHeading 2 (Section)\n------\n\nHowever, this method can only be used to create two heading levels like above.\n\n\n\n3.3.2.2 Horizontal Rules - line Seperator\n\nA horizontal rule adds a visual break between parts of text by adding three or more asterisks, “***“, dashes,”—“, or underscores,”___“, on a line by themselves.\nExample:\n\nYou can code like this:\nThis is the part 1.\n\n--------\n\nThis is the part 2.\n\n********\n\nThis is the part 3.\n\n_________\n\nThis is the part 4.\n\nThis code chunk will output like this:\n\nThis is the part 1.\n\nThis is the part 2.\n\nThis is the part 3.\n\nThis is the part 4.\n\nBe careful. You need to make sure you add a blank line before and after your separator line.\n\n\n\n3.3.2.3 Paragraphs\n\nTo create paragraphs, you can add a blank line between two paragraphs just like you usually create paragraphs in an essay.\nExample:\n\nYou can code like this:\nThis is the first paragraph ........................................\n.....................................................................\n.....................................................................\n... end the first paragraph.\n\nThis is the second paragraph .........................................................\n...................................................................\n.....................................................................\n...end the second paragraph.\n\nThis will output like this:\n\nThis is the first paragraph …………………………………. …………………………………………………………… …………………………………………………………… … end the first paragraph.\nThis is the second paragraph ………………………………………………… …………………………………………………………. …………………………………………………………… …end the second paragraph.\n\n\n3.3.2.4 Blockquotes\n\nTo highlight multiple line important text, cite multiple line references, or quote multiple line important points, you can add a greater than sign, “&gt;”, at the begining of each line of the text.\n\n\nExample:\n\nYou can code like this:\n&gt; This is an important text.\n&gt;\n&gt; This is a citation/reference.\n&gt;\n&gt; This is a quotation.\n\nThis code will output like this:\n\n\nThis is an important text.\nThis is a citation/reference.\nThis is a quotation.\n\n\nWrong Example:\n\nIf you code like this:\n&gt; This is an important text.\n&gt; This is a citation/reference.\n&gt; This is a quotation.\n\nIt will output like this:\n\n\nThis is an important text. This is a citation/reference. This is a quotation.\n\nBe sure to add a blank line with a greater-than sign, “&gt;”, otherwise the output will display multiple lines of text in a single line.\n\nYou can also nest blockquotes by adding two or more greater than signs, “&gt;&gt;”.\n\n\nExample:\n\nYou can code like this:\n&gt; This is an important text.\n&gt;\n&gt;&gt; This is a citation/reference.\n&gt;&gt;\n&gt;&gt;&gt; This is a quotation.\n\nThis code will output like this:\n\n\nThis is an important text.\n\nThis is a citation/reference.\n\nThis is a quotation.\n\n\n\nBe sure to add a blank line before the first line of your blockquotes, otherwise, it will not look right.\n\n\n\n3.3.3 Formatting Text\nThere are some elements to make text stand out, improving emphasis and readability like Bold, Italic, and Strikethrough text.\n\n3.3.3.1 Bold\n\nTo bold text, you can add two asterisks, “**“, or underscores,”__“, before and after the text.\nExample:\n\nYou can code like this:\n**This is the first important text.**\n\n__This is the second important text.__\n\nThis is the __third important__ text.\n\nThis is the **fourth important** text.\n\nThis is the f**if**th important text.\n\nThis will output like this:\n\nThis is the first important text.\nThis is the second important text.\nThis is the third important text.\nThis is the fourth important text.\nThis is the fifth important text.\n\nBe careful do not use underscores, “__“, to bold charachers inside a word, like this”This is the fifth im__portan__t text”.***\n\n\n\n3.3.3.2 Italic\n\nTo italicize a text, you can add one asterisk, “*“, or one underscore,”_“, before and after a text.\nExample:\n\nYou can code like this:\n*ThIs Is the fIrst Important TexT.*\n\n_ThIs Is the second Important TexT._\n\nThis is the _Third ImportanT_ text.\n\nThis is the *fourth ImportanT* text.\n\nFifth im*PORtaN*t text.\n\nThis code chunk will output like this:\n\nThIs Is the fIrst Important TexT.\nThIs Is the second Important TexT.\nThis is the Third ImportanT text.\nThis is the fourth ImportanT text.\nFifth imPORtaNt text.\n\nBe careful don’t use underscores, “_“, to italicize charachers inside a word, like this”Fifth im_porta_nt text”.\n\n\n\n3.3.3.3 Bold & Italic\nTo bold and italicize for the same text, you can use three asterisks, “***“, or three underscores,”___“, before and after a text.\n\nExample:\n\nYou can code like this:\n***This is the first important text.***\n\n___This is the second important text.___\n\nThis is the ___third important___ text.\n\nThis is the ***fourth important*** text.\n\nFifth i***mportan***t text.\n\nThis code chunk will output like this:\n\nThis is the first important text.\nThis is the second important text.\nThis is the third important text.\nThis is the fourth important text.\nFifth important text.\n\nBe careful don’t use underscores, “___“, to bold and italicize charachers inside a word, like this”Fifth im___porta___nt text”.\n\n\n\n3.3.3.4 Highlight\n\nTo highlight a text, you can add this sign, “&lt;mark&gt;”, before the text and add this sign, “&lt;/mark&gt;”, after the text.\nExample:\n\nYou can code like this:\n&lt;mark&gt;This is a text that needs to be highlighted.&lt;/mark&gt;\n\nThis is a te&lt;mark&gt;xt that needs to be high&lt;/mark&gt;lighted.\n\nThis code chunk will output like this:\n\nThis is a text that needs to be highlighted.\nThis is a text that needs to be highlighted.\n\n\n3.3.3.5 Strikethrough - Deleted or Removed Text\n\nTo show a deletion or correction on a text, you can add double tilde signs, “~~”, before and after the part of the deletion or correction on your text.\nExample:\n\nYou can code like this:\n~~This text is struck through~~ This is the correct text.\n\nThis code chunk will output like this:\n\nThis text is struck through This is the correct text.\n\n\n\n3.3.4 Enhancing Content\nTo enhance the illustrative capabilities of your content, there are several elements you can add to your document, including subscript, superscript, lists, tables, footnotes, links, images, math notations, etc.\n\n3.3.4.1 Subscript\n\nTo add a subscript before, after or within a number or word, you can add a tilde symbol, “~”, before and after the text you want to subscript.\nExample:\n\nYou can code like this:\nThis is a subscript before and after a word: \n\n~subscript~word~subscript~\n\nThis is a subscript within a word: \n\nWor~111000~ds\n\nThis is a subscript before and after a number: \n\n~7878~11111~7878~ \n\nThis is a subscript within a number: \n\n999~subscript~999\n\nThis code chunk will output like this:\n\nThis is a subscript before and after a word:\nsubscriptwordsubscript\nThis is a subscript within a word:\nWor111000ds\nThis is a subscript before and after a number:\n7878111117878\nThis is a subscript within a number:\n999subscript999\n\nBe sure not to add any spaces or tabs between the two tilde symbols, “~ ~”.\n\n\n\n3.3.4.2 Superscript\n\nTo add a superscript before, after or within a number or word, you can add a caret symbol, “^”, before and after the text you want to superscript.\nExample:\n\nYou can code like this:\n\nThis is a superscript before and after a word:\n\n^787878^Words^787878^\n\nThis is a superscript within a word:\n\nWor^787878^ds\n\nThis is a superscript before and after a number:\n\n^superscript^1111111^superscript^  \n\nThis is a superscript within a number:\n\n999^superscript^999\n\nThis will output like this:\n\nThis is a superscript before and after a word:\n787878Words787878\nThis is a superscript within a word:\nWor787878ds\nThis is a superscript before and after a number:\nsuperscript1111111superscript\nThis is a superscript within a number:\n999superscript999\n\nBe sure not to add any spaces or tabs between the two caret symbols, “…”.\n\n\n\n3.3.4.3 Lists\nTo organize a list (nested list), you can use ordered or unordered numbers or alphabets followed by a period sign, “.”, dashes, “-”, asterisks, “*“, or plus signs,”+“, in front of line items. Markdown is smart, it will automatically detect and organize a list for you.\n1. Using Ordered numbers followed by a period sign:\n\nExample:\n\nYou can code like this:\n\n1. First item\n2. Second item\n    1. Third item\n    2. Fourth item\n3. Fifth item\n\nThis code chunk will output like this:\n\nUsing Ordered numbers followed by a period sign:\n\nFirst item\nSecond item\n\nThird item\nFourth item\n\nFifth item\n\n2. Using Unordered numbers followed by a period sign:\n\nExample:\n\nYou can code like this:\n\n2. First item\n2. Second item\n    2. Third item\n    2. Fourth item\n2. Fifth item\n\n2. First item\n7. Second item\n    9. Third item\n    2. Fourth item\n10. Fifth item\n\nThis code chunk will output like this:\n\nUsing Unordered numbers followed by a period sign:\n\nFirst item\nSecond item\n\nThird item\nFourth item\n\nFifth item\nFirst item\nSecond item\n\nThird item\nFourth item\n\nFifth item\n\n\nBe careful, for an unordered list to work as you want, you need to take care of the first number or letter of the first item of your (nested) list, because markdown will order the list starting with the first number or alphabet of your (nested) list.\n\n3. Using dashes:\n\nExample:\n\nYou can code like this:\n\n- First item\n- Second item\n    - Third item\n    - Fourth item\n- Fifth item\n\nThis code chunk will output like this:\n\nUsing dashes:\n\nFirst item\nSecond item\n\nThird item\nFourth item\n\nFifth item\n\n4. Using asterisks:\n\nExample:\n\nYou can code like this:\n\n* First item\n* Second item\n    * Third item\n    * Fourth item\n* Fifth item\n\nThis code chunk will output like this:\n\nUsing asterisks:\n\nFirst item\nSecond item\n\nThird item\nFourth item\n\nFifth item\n\n5. Using plus signs:\n\nExample:\n\nYou can code like this:\n\n+ First item\n+ Second item\n    + Third item\n    + Fourth item\n+ Fifth item\n\nThis code chunk will output like this:\n\nUsing plus signs:\n\nFirst item\nSecond item\n\nThird item\nFourth item\n\nFifth item\n\n6. Using alphabets:\n\nExample:\n\nYou can code like this:\n\na. First item\nb. Second item\n    a. Third item\n    b. Fourth item\nc. Fifth item\n\nw. First item\na. Second item\n    c. Third item\n    y. Fourth item\na. Fifth item\n\nThis code chunk will output like this:\n\nUsing alphabets:\n\nFirst item\nSecond item\n\nThird item\nFourth item\n\nFifth item\nFirst item\nSecond item\n\nThird item\nFourth item\n\nFifth item\n\n7. Using different delimiters:\n\nExample:\n\nYou can code like this:\n\n+ First item\n- Second item\n    * Third item\n    + Fourth item\n* Fifth item\n\nThis code chunk will output like this:\n\nUsing different delimiters:\n\nFirst item\nSecond item\n\nThird item\nFourth item\n\nFifth item\n\n\nUsing different delimiters in the same list has no effect on the list organized by markdown.\nBe careful, you need to make sure you add a blank line before the list starts.\nYou can also use a backslash, “\", to escape a period,”.”, if you do not want to create a list and still need a period after a number or alphabet.\n\n\n\n3.3.4.4 Task Lists - To-Do List\n\nTo add a task/to-do list, you can add this sign, “- [ ]”, or this sign, “- [x]”, before each item in your task/to-do list.\nExample:\n\nYou can code like this:\n\n- [ ] Task not completed\n- [x] Task completed\n\nThis code chunk will output like this:\nTask not completed\nTask completed\n\n\nIn your rendered HTML file, you can check or uncheck the completion marks in the small box at the front of each task.\n\n\n\n3.3.4.5 Links\n\nYou can add a link in your text by enclosing your added link with parentheses, “()”. In addition, you can also optionally add a name or a short description for the link by enclosing them with brackets, “[]”, before the link and this will appear as a tooltip when the user hovers over the link\n\n\n\n\n\nExample:\n\nYou can code like this:\n\nWe can use the \n[Markdown Cheat Sheet](https://www.markdownguide.org/basic-syntax/#code-blocks) \nto learn more generally used markdown syntax. \n\nThis code chunk will output like this:\n\nWe can use the Markdown Cheat Sheet to learn more generally used markdown syntax.\n\nYou can add an Reference Style Link by enclosing the name or description of the link and a number/word pointed to the link with brackets, “[]”.\n\n\nExample:\n\nYou can code like this:\n\nWe can use the [Markdown Cheat Sheet][4] to learn more generally used \nmarkdown syntax. \n\n[4]: https://www.markdownguide.org/basic-syntax/#code-blocks\n\nThis code chunk will output like this:\n\nWe can use the Markdown Cheat Sheet to learn more generally used markdown syntax.\n\nRemember to start a new line and add the link as a footnote after the number or word pointed to the reference enclosed by brackets, “[]”.\n\n\n\n3.3.4.6 Images\n\nTo add images from your local computer or a website, you can add an exclamation mark, “!”, followed by a description or other text enclosing with brackets, “[]”, and a path/URL to the image enclosing with parentheses, “()”.\nExample:\n\nYou can code like this:\n\n![This is a description to an online image](https://today.uconn.edu/2023/01/\nuconn-on-campus-construction-update-january-2023/)\n\n![This is a description to a local image](/Users/shiyili/\nDesktop/UCONN.jpg)\n\nThe render output will show like this:\n\n\n\n\nThis is a description to an online image.\n\n\n\n\n3.3.4.7 Tables\n\nTo create a table, you can use three or more hyphens, “—”, and pipes, “|”, to create and separate each column respectively.\nExample:\n\nYou can code like this:\n\n1. Each cell with the same width in code chunk:\n\n|            | 1st Column | 2nd Column | 3rd Column | ...... |\n| ---------- | ---------- | ---------- | ---------- | ------ |\n| 1st Row    |    123     |     123    |     123    |   123  |\n| 2nd Row    |    123     |     123    |     123    |   123  |\n| 3rd Row    |    123     |     123    |     123    |   123  |\n| ......     |    123     |     123    |     123    |   123  |\n\n2. Each cell with vary width in code chunk:\n\n|            | 1st Column | 2nd Column | 3rd Column | ...... |\n| ------ | ---------- | ---------- | ---------- | ------ |\n| 1st Row |     123       |      123      |     123       |    123    |\n| 2nd Row      |     123       |     123       |      123      |   123     |\n| 3rd Row  |      123      |      123      |     123       |    123    |\n| ......         |      123      |     123       |      123      |    123    |\n\nThis code chunk will output like this:\n\n\nEach cell with the same width in code chunk:\n\n\n\n\n\n\n\n\n\n\n\n\n1st Column\n2nd Column\n3rd Column\n……\n\n\n\n\n1st Row\n123\n123\n123\n123\n\n\n2nd Row\n123\n123\n123\n123\n\n\n3rd Row\n123\n123\n123\n123\n\n\n……\n123\n123\n123\n123\n\n\n\n\nEach cell with vary width in code chunk:\n\n\n\n\n\n\n\n\n\n\n\n\n1st Column\n2nd Column\n3rd Column\n……\n\n\n\n\n1st Row\n123\n123\n123\n123\n\n\n2nd Row\n123\n123\n123\n123\n\n\n3rd Row\n123\n123\n123\n123\n\n\n……\n123\n123\n123\n123\n\n\n\n\nCell width can vary, because markdown will automatically detect and organize the table for you with the same width.***\n\n\nYou can align text in the columns to the left, right, or center by adding a colon, “:”, to the left, right, or on both side of the hyphens, “—”, within the header (Cone, 2025).\n\n\nExample:\n\nYou can code like this:\n\n|            | 1st Column | 2nd Column | 3rd Column | ...... |\n| :----------: | :---------- | :----------: | ----------: | ------: |\n| 1st Row    |            123|       123     |  123          |        123|\n| 2nd Row    |123            |    123        |           123 |123        |\n| 3rd Row    |     123       |123            |     123       |   123     |\n| ......     |   123         |            123| 123           |123        |\n\nThis code chunk will output like this:\n\n\n\n\n\n\n\n\n\n\n\n\n1st Column\n2nd Column\n3rd Column\n……\n\n\n\n\n1st Row\n123\n123\n123\n123\n\n\n2nd Row\n123\n123\n123\n123\n\n\n3rd Row\n123\n123\n123\n123\n\n\n……\n123\n123\n123\n123\n\n\n\n\n\n3.3.4.8 Code\n\nTo add an inline code, you can add a backtick, “`”, before and after a word or a text.\n\n\nExample:\n\nYou can code like this:\n\nThis is my `inline` code.\n\n`This is my inline code.`\n\nThis will output like this:\n\nThis is my inline code.\nThis is my inline code.\n\nIf you want to display a text with a backtick, “`”“, as an inline code, you can add a double backtick,”``“, before and after the text.\n\n\nExample:\n\nYou can code like this:\n\n``This is a `text` with backticks.``\n\nThis will output like this:\n\nThis is a `text` with backticks.\n\nTo add a code block, you can indent each line of the text with more than four spaces or one tab.\n\n\nExample:\n\nYou can code like this:\n\n    This is a code block.\n\n        This is a code block.\n\n            This is a code block.\n\n    This is a code block.\n\nThis will output like this:\n\n    This is a code block.\n\n        This is a code block.\n\n            This is a code block.\n\n    This is a code block.\n\nTo add a fenced code block, you can add a blank line begining with three backticks, “```”, or three tilde signs, “~~~”, before and after your code blok.\n\n\nExample:\n\nYou can code like this:\n\n    ```\n    {\n    This is my fenced code block.\n    This is my fenced code block.\n    }\n    ```\n\nThis will output like this:\n\n{\nThis is my fenced code block.\nThis is my fenced code block.\n}\n\nTo add a nonexecutive Python code chunk, you can first add a fence code block for your code, then add a word, “python”, after the three backticks, “```”, in the first line of your fence code block.\n\n\nExample :\n\nYou can code like this:\n    ```python\n    print(\"This is a Python code chunk.\")\n    ```\n\nThis will output like this:\n\nprint(\"This is a Python code chunk.\")\n\nTo add an executive Python code chunk, you can first add a fence code block for your code, then add a word, “python”, with brackets, “{}”after the three backticks, “```”, in the first line of your fence code block.\n\n\nExample:\n\nYou can code like this:\n    ```{python}\n    print(\"This is a Python code chunk.\")\n    ```\n\nThis will output like this:\n\n\nprint(\"This is a Python code chunk.\")\n\nThis is a Python code chunk.\n\n\n\nYou can also make an executive Python code chunk not execute the commands inside the code chunk by adding “#| eval: false” to the first line of your code block.\n\n\nExample:\n\nYou can code like this:\n    ```{python}\n    #| eval: false\n\n    print(\"This is a Python code chunk.\")\n    ```\n\nThis will output like this:\n\n\nprint(\"This is a Python code chunk.\")\n\n\nIf you add just “#| eval: true” to your code chunk, this code chunk will execute the commands as usual and output the results.\nIf you just add “#| echo: false” to your code chunk, then your code chunk will not be displayed in your rendered output, but the commands in your code chunk will still be executed as usual and the result of the code chunk will be displayed.\nIf you just add “#| output: false” to your code chunk, then the commands in your code chunk will be displayed as usual in the rendered output, but the result of your code chunk will not be displayed.\n\n\n\n3.3.4.9 Math Notation - Using LaTeX in Markdown\n\nTo displace an inline math equation, you can add a dollar sign, “$”, before and after the math equation.\n\n\nExample:\n\nYou can code like this:\n\nThis is a quadratic equation, $ax^2+bx+c=0$.\n\nThis is a quadratic equation roots formula, $x = {(-b \\pm \\sqrt{b^2-4ac})}/{2b}$.\n\nThis code chunk will output like this:\n\nThis is a quadratic equation, \\(ax^2+bx+c=0\\).\nThis is a quadratic equation roots formula, \\(x = {(-b \\pm \\sqrt{b^2-4ac})}/{2b}\\).\n\nIn an inline math equation, make sure you use the division symbol instead of “\\frac{}{}”.\n\n\nYou can center an math equation by adding double dollar signs, “$$”, before and after the math equation.\n\n\nExample:\n\nYou can code like this:\n\nThis is a quadratic equation \n$$\nax^2+bx+c=0.\n$$\n\nThis is a quadratic equation roots formula \n$$\nx = {(-b \\pm \\sqrt{b^2-4ac})}/{2b}.\n$$\n\nThis code chunk will output like this:\n\nThis is a quadratic equation \\[\nax^2+bx+c=0.\n\\]\nThis is a quadratic equation roots formula \\[\nx = {(-b \\pm \\sqrt{b^2-4ac})}/{2b}.\n\\]\n\nBe careful not to add punctuation before a centered math equation.\nIf you want to add a period after a centered math equation, make sure you add it after the end of the math equation and before the second double dollar sign, “$$”.\n\n\n\n3.3.4.10 Footnotes\n\nTo add notes and refercences without confusing for body content, you can add a caret sign, “^”, follwed by a identifier number or words without any spaces and tabs inside a brackets, “[]”.\nExample:\n\nyou can code like this:\n\nThis is a body paragraph, and I have a note [^short_footnotes] want to \nadd here. [^long_footnotes]\n\n[^short_footnotes]: This is a notes.\n\n[^long_footnotes]: This is a notes with multiple paragraphs.\n    You can include paragraphs in your footnotes using indentation like this.\n    ```\n    {\n        This is a fenced code block.\n    }\n    ```\n    This is the end of this footnote.\n\nThis code chunk will output like this:\n\nThis is a body paragraph, and I have two notes 1 want to add here. 2\n\nClicking on the footnote number in the body content, will take you to the location where the footnote exists in the document.\nAll footnotes are automatically numbered sequentially at the end of the rendered HTML files and at the bottom of the page where the footnote exists in rendered PDF files.\n\nIn the rendered HTML file, each footnote displayed at the end of the document is followed by a link that, when clicked, takes you back to the specific location of the footnote in your body content. You can also move your mouse over the footnote number in the body content, and the content of the footnote will automatically appear below the footnote number.\n\n\n\n\n\n3.3.5 Conclusion\nMarkdown is a simple yet powerful way to format text for documentation, blogging, and technical writing. With its easy-to-read syntax, you can structure documents, highlight key points, and present data effectively. It’s widely used in open-source projects, academic writing, and web content due to its flexibility and seamless conversion to HTML, PDF, and DOCX. Mastering Markdown can help you create clear, well-organized content with minimal effort.\n\n\n3.3.6 Further Readings\n\n[Markdown Cheet Sheet] (Cone, 2025): A quick reference to the Markdown syntax.\n[Quarto Markdown Basic] (Dervieux, 2025): Markdown basic for Quarto.\n[GitHub Flavored Markdown (GFM)] (MacFarlane, 2019): Markdown features specific to GitHub.\n[Jupyter Notebook Markdown] (MacFarlane, 2006): Use Markdown for interactive data science.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reproducible Data Science</span>"
    ]
  },
  {
    "objectID": "quarto.html#creating-presentations-using-quarto",
    "href": "quarto.html#creating-presentations-using-quarto",
    "title": "3  Reproducible Data Science",
    "section": "3.4 Creating Presentations Using Quarto",
    "text": "3.4 Creating Presentations Using Quarto\n\n3.4.1 Introduction\nHi! This section is written by Joann Jun, a Junior majoring in Statistical Data Science and minoring in Mathematics at the University of Connecticut.\nThis section will focus on how to create a presentation using Quarto. By the end of this section, you will be able to understand how to:\n\nStructure a Quarto presentation\nCustomize themes, transitions, and interactivity\nKeyboard Shortcuts\n\n\n\n3.4.2 Why Use Quarto for Presentations?\n\nSeamlessly integrate code, data analysis, and visualizations into a single document\nGenerate interactive slides with minimal effort\nSupport multiple output formats, such as HTML (reveal.js), PowerPoint (pptx), and PDF (beamer).\n\n\n\n3.4.3 Presentation Formats\n\n3.4.3.1 Formats\nThere are several formats you can use to create a presentation in Quarto. This includes:\n\nrevealjs - reveal.js (HTML)\nbeamer - Beamer (LaTex/PDF).\n\n\n\n3.4.3.2 Differences Between Formats\n\n\n\n\n\n\n\n\nFeature\nrevealjs\nbeamer\n\n\n\n\nOutput Format\nHTML slideshow or PDF\nPDF\n\n\nPros\n- Works well with Markdown  - Interactive and dynamic\n- Good math support  - Professional typesetting (LaTeX)\n\n\nCons\n- Requires a browser to present\n- Requires knowledge of LaTeX\n\n\n\nIn this section, I will focus on using revealjs.\n\n\n3.4.3.3 How to Change Format\nTo change the format of your presentation, in the YAML header next format add revealjs or beamer.\n---\ntitle: Quarto Presentation\nauthor: Joann Jun\nformat: revealjs # This where you edit the format.\n---\n\n\n3.4.3.4 YAML Heading for revealjs\nAnything you put in the YAML heading will become part of the global environment. This means that it will be applied to all slides.\n\nembed-resources: true - Creates self-contained file to distribute more easily.\nmultiplex: true - Allows your audience to follow the slides that you control on their own device.\n\nWhen you render, it’ll create 2 HTML files:\n\npresentations.html - Publish online for the audience to use.\npresentations-speaker.html - File you present from and you don’t publish this.\n\n\nchalkboard - Allows you to draw on your presentation\n\n\n\n3.4.3.5 Some Stylistic YAML Headings for revealjs\n\ntheme: [slide theme] - Allows you to switch to any of Reveals 11 themes (or make your own)\n\ndefault, dark, beige, simple, serif\n\ntransition: [transition] - Adds transitions to slides\n\nnone, slide, fade, convex, concave, zoom\n\nlogo: logo.png - Allows you to add logo to bottom of each slide.\nfooter: \"Footer Note\" - Adds a footer to bottom of each slide.\nslide-number: true - Displays the slide number at bottom of screen.\nincrimental: true - Displays the bullet points one by one.\n\n\n\n3.4.3.6 Example\n---\ntitle: \"How to Make A Presentation Using Quarto\"\nauthor: \"Joann Jun\"\nformat:\n  revealjs: \n    embed-resources: true\n    theme: serif\n    slide-number: true\n    preview-links: true\n    css: [default, styles.css] # Don't need this unless customizing more\n    incremental: true   \n    transition: slide\n    footer: \"STAT 3255\"\n\n---\n\n\n\n3.4.4 Slides syntax\n\n3.4.4.1 How to Create a New Slide\nThe start of all slides are marked by a heading. You can do this by using\n\nLevel 1 header (#) - Used to create title slide.\nLevel 2 header (##) - Used to create headings.\nHorizontal rules (---) - Used when you don’t want to add a heading or title.\nNote: ### will create a subheading in the slide.\n\n\n# Title 1\n\nHello, World!\n\n## Slide Heading 1\n\n### Subjeading 1\n\nHello, World!\n\n--- # Makes slide without title/heading\n\nHello, World!\n\n\n\n3.4.5 Code\n\n3.4.5.1 auto-animate=true\nThis setting will allow smooth transitions across similar slides. You use this when you want to show gradual changes between slides.\nFor example, let’s say you have a block of code and then add another block, we can show the changes by using this. The first slide should only have part of the code, and the second should have the full code.\nSlide 1\n\n\n## Smooth Transition Slide 1 {auto-animate=true}\n\nfrom math import sqrt\n\nSlide 2\n\n\n## Smooth Transition Slide 2 {auto-animate=true}\n\nfrom math import sqrt\n\ndef pythagorean(a,b):\n    c2 = a**2 + b**2\n    c = sqrt(c2)\n    return c\n\n\n\n3.4.5.2 Highlighting Code\nTo highlight code, you can use the code-line-numbers attribute and use \"[start line #]-[end line #]\".\nFor example, in this slide let’s say I wanted to highlight the addition and subtraction function. I do this by putting {.python code-line-numbers=“2-6”} next to my 3 back ticks that start the code fence.\n#| echo: true\n\ndef addition(x,y):\n    return x + y\n\ndef subtraction(x,y):\n    return x - y\n\ndef multiplication(x,y):\n    return x * y\nIf the lines you want to highlight are separated by another line, you can use a comma.\nIn this slide I wanted to highlight return for each function, so I used {.python code-line-numbers=“3,6,9”}\n#| echo: true\n\ndef addition(x,y):\n    return x + y\n\ndef subtraction(x,y):\n    return x - y\n\ndef multiplication(x,y):\n    return x * y\n\n\n3.4.5.3 echo\nBy default, the code block does not echo the source code. By this I mean it does not show the source code and only shows the output by default.\nIn order to show the source code, we set the setting to #| echo: true. We add this part inside of the code fence at the top.\nEx: echo: false\n\n\n\n\n\n\n\n\n\nEx: echo: true\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx = np.linspace(0, 10, 100)\ny = np.sin(x)\n\nplt.figure(figsize=(12, 6))  # Width=12, Height=6\nplt.plot(x, y, label='sin(x)')\nplt.title('Sine Function')\nplt.xlabel('x')\nplt.ylabel('sin(x)')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n3.4.5.4 Figures\n\nPython figure sizes (MatPlotLib and PlotlyExpress) are automatically set to fill the slide area below the title\nR (Knitr figure) have similar default figure width and length sizes\nYou will most likely have to change the size of these figures or change the output location\n\n\n\n3.4.5.5 output-location\nThe output-location option can modify where the output of the code goes. There are several options to choose from such as:\n\ncolumn - Displays the output in a column next to the code.\ncolumn-fragment - Displays the output in a column next to the code and delays showing it until you advance.\nfragment - Displays the output as a Fragment. It delays showing it until you advance.\nslide - Displays the output on the slide after.\n\nEx: This uses column.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx = np.linspace(0, 10, 100)\ny = np.sin(x)\n\nplt.figure(figsize=(12, 6))\nplt.plot(x, y, label='sin(x)')\nplt.title('Sine Function')\nplt.xlabel('x')\nplt.ylabel('sin(x)')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n3.4.6 Interactive Code - HTML Gagdets\n\nHTML Gadget allows you to embed interactive elements into the HTML document.\nEnhance user engagement and data exploration.\nSeamless integration with R and Python in Quarto.\nVisualize complex data interactively.\nSome examples of these are:\n\nInteractive maps (e.g., Google Maps, Leaflet)\nData tables with sorting and searching\nDynamic plots with tooltips and zooming\nShiny apps for data exploration\n\n\n\n3.4.6.1 HTML Gagdget Tools\n\nLeaflet - Used to make interactive maps which allows you to display location based data.\nPlotly - Used for creating interactive plots.\nShiny - Used to make interactive statistical model that allows you to tweak parameters and see impacts.\nDT - Used for creating searchable data tables.\n\n\n\n\n\n\n\n\n\n\n\nTool\nLeaflet\nPlotly\nShiny\nDT\n\n\n\n\nPurpose\nInteractive maps\nInteractive plots\nBuild web apps\nInteractive tables\n\n\nKey Features\n- Markers, popups, and layers- Custom styles- Zoom and pan- Various tile providers\n- Scatter, line, and bar charts- Tooltips, hover effects, zoom\n- Dynamic with sliders and inputs- Real-time updates- Widgets for interactivity\n- Sorting, filtering, and pages- Customizable styling- Searchable columns\n\n\nUse Cases\n- Visualizing locations and routes\n- Analyzing data trends and relationships\n- Dashboards, reports, and data exploration\n- Displaying and exploring large datasets\n\n\n\n\n\n3.4.6.2 How to Get Started\n\nInstall the tools necessary using install.packages(c(\"plotly\", \"shiny\", \"DT\", \"leaflet\"), repos = \"https://cloud.r-project.org/\").\nThe code above should also allow you to avoid the CRAN error.\nNote: Shiny app should be run in a separate R session or browser window.\n\n\n\n3.4.6.3 Map of UCONN (Leaflet)\n\n## Install these R packages if you don't have them already\n## options(repos = c(CRAN = \"https://cloud.r-project.org/\"))\n## install.packages(c(\"leaflet\", \"plotly\", \"shiny\", \"DT\",\n##                             \"htmlwidgets\", \"webshot\", \"knitr\"))\n\nlibrary(leaflet)\n\n# Create the interactive leaflet map\nmap &lt;- leaflet() %&gt;%\n  addTiles() %&gt;%\n  setView(lng = -72.2565, lat = 41.8084, zoom = 15) %&gt;%\n  addMarkers(lng = -72.2565, lat = 41.8084, popup = \"UConn Main Campus\")\n\n## Ensure the 'images' folder exists\nif (!dir.exists(\"images\")) {\n    dir.create(\"images\")\n}\n\n# Conditional rendering based on output format\nif (knitr::is_html_output()) {\n  map  # Show interactive map in HTML\n} else {\n  # Define file paths\n  html_file &lt;- \"images/leaflet_temp.html\"\n  png_file &lt;- \"images/uconn_static.png\"\n\n  # Save the leaflet map as an HTML file\n  htmlwidgets::saveWidget(map, html_file, selfcontained = TRUE)\n\n  # Take a screenshot of the HTML file as a PNG image\n  webshot::webshot(html_file, file = png_file, delay = 2, vwidth = 800, vheight = 600)\n\n  # Include the PNG in the PDF output\n  knitr::include_graphics(png_file)\n}\n\n\n\n\n\n\n\n3.4.6.4 Interactive Plot (PlotLy)\n\nlibrary(plotly)\n\nfig &lt;- plot_ly(mtcars, x = ~mpg, y = ~hp, type = 'scatter', mode = 'markers', \n               marker = list(size = 10, color = ~cyl, colorscale = 'Blue'))\nfig &lt;- fig %&gt;% layout(title = \"Interactive Scatter Plot of MPG vs HP\",\n                      xaxis = list(title = \"Miles Per Gallon\"),\n                      yaxis = list(title = \"Horsepower\"))\n\n# Conditional rendering based on output format\nif (knitr::is_html_output()) {\n  fig  # Show interactive plot in HTML\n} else {\n  # Define file paths\n  html_file &lt;- \"images/plotly_temp.html\"\n  png_file &lt;- \"images/mtcars_static.png\"\n\n  # Save the plotly chart as an HTML file\n  htmlwidgets::saveWidget(fig, html_file, selfcontained = TRUE)\n\n  # Take a screenshot of the HTML file as a PNG image\n  webshot::webshot(html_file, file = png_file, delay = 2, vwidth = 800, vheight = 600)\n\n  knitr::include_graphics(png_file)\n}\n\n\n\n\n\n\n\n\n3.4.7 Rendering Your Presentation\nIn your terminal enter the following code:\n\nquarto render &lt;presentation_name&gt;.qmd --to revealjs\n\nThis will produce a HTML slideshow output that you can present.\n\n\n3.4.8 Keyboard Shortcuts\n\nS - Brings you to speaker view\nF - Fullscreen\n→,SPACE,N - Next slide\n←,P - Previous slide\nAlt →, Alt ← - Navigates without Fragments\nShift →, Shift ← - Navigates to first or last slide\n\n\n\n3.4.9 More Information/Resources\n\nRevealjs\nPowerPoint\nBeamer\nShiny\nPlotLy\nPlotly_Interactive\nDT\nLeaflet\n\n\n\n\n\nCone, M. (2025). Markdown cheat sheet | markdown guide. https://www.markdownguide.org/cheat-sheet/\n\n\nDervieux, C. (2025). Markdown-basics. https://quarto.org/docs/authoring/markdown-basics.html\n\n\nMacFarlane, J. (2006). Pandoc user’s guide. https://pandoc.org/MANUAL.html#pandocs-markdown\n\n\nMacFarlane, J. (2019). GitHub flavored markdown spec. https://github.github.com/gfm/",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reproducible Data Science</span>"
    ]
  },
  {
    "objectID": "quarto.html#footnotes",
    "href": "quarto.html#footnotes",
    "title": "3  Reproducible Data Science",
    "section": "",
    "text": "This is a notes.↩︎\nThis is a notes with multiple paragraphs. You can include paragraphs in your footnotes using indentation like this.\n    print(This is a fenced code block.)\n\n    You can add any thing inside this fenced code block.\nThis is the end of this footnote.↩︎",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reproducible Data Science</span>"
    ]
  },
  {
    "objectID": "python.html",
    "href": "python.html",
    "title": "4  Python Refreshment",
    "section": "",
    "text": "4.1 The Python World\nYou have programmed in Python. Regardless of your skill level, let us do some refreshing.\nSee, for example, how to build a Python libratry.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "python.html#the-python-world",
    "href": "python.html#the-python-world",
    "title": "4  Python Refreshment",
    "section": "",
    "text": "Function: a block of organized, reusable code to complete certain task.\nModule: a file containing a collection of functions, variables, and statements.\nPackage: a structured directory containing collections of modules and an __init.py__ file by which the directory is interpreted as a package.\nLibrary: a collection of related functionality of codes. It is a reusable chunk of code that we can use by importing it in our program, we can just use it by importing that library and calling the method of that library with period(.).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "python.html#standard-library",
    "href": "python.html#standard-library",
    "title": "4  Python Refreshment",
    "section": "4.2 Standard Library",
    "text": "4.2 Standard Library\nPython’s has an extensive standard library that offers a wide range of facilities as indicated by the long table of contents listed below. See documentation online.\n\nThe library contains built-in modules (written in C) that provide access to system functionality such as file I/O that would otherwise be inaccessible to Python programmers, as well as modules written in Python that provide standardized solutions for many problems that occur in everyday programming. Some of these modules are explicitly designed to encourage and enhance the portability of Python programs by abstracting away platform-specifics into platform-neutral APIs.\n\nQuestion: How to get the constant \\(e\\) to an arbitary precision?\nThe constant is only represented by a given double precision.\n\nimport math\nprint(\"%0.20f\" % math.e)\nprint(\"%0.80f\" % math.e)\n\n2.71828182845904509080\n2.71828182845904509079559829842764884233474731445312500000000000000000000000000000\n\n\nNow use package decimal to export with an arbitary precision.\n\nimport decimal  # for what?\n\n## set the required number digits to 150\ndecimal.getcontext().prec = 150\ndecimal.Decimal(1).exp().to_eng_string()\ndecimal.Decimal(1).exp().to_eng_string()[2:]\n\n'71828182845904523536028747135266249775724709369995957496696762772407663035354759457138217852516642742746639193200305992181741359662904357290033429526'",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "python.html#important-libraries",
    "href": "python.html#important-libraries",
    "title": "4  Python Refreshment",
    "section": "4.3 Important Libraries",
    "text": "4.3 Important Libraries\n\nNumPy\npandas\nmatplotlib\nIPython/Jupyter\nSciPy\nscikit-learn\nstatsmodels\n\nQuestion: how to draw a random sample from a normal distribution and evaluate the density and distributions at these points?\n\nfrom scipy.stats import norm\n\nmu, sigma = 2, 4\nmean, var, skew, kurt = norm.stats(mu, sigma, moments='mvsk')\nprint(mean, var, skew, kurt)\nx = norm.rvs(loc = mu, scale = sigma, size = 10)\nx\n\n2.0 16.0 0.0 0.0\n\n\narray([-7.61542223,  4.04410857, -0.66018039, -3.22051506,  3.46732088,\n        8.19518069, -0.249417  , -1.29855302,  4.58570583,  4.19481766])\n\n\nThe pdf and cdf can be evaluated:\n\nnorm.pdf(x, loc = mu, scale = sigma)\n\narray([0.00554702, 0.08752706, 0.07994828, 0.04255689, 0.09324591,\n       0.03005831, 0.08514888, 0.07098782, 0.08093034, 0.08579701])",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "python.html#writing-a-function",
    "href": "python.html#writing-a-function",
    "title": "4  Python Refreshment",
    "section": "4.4 Writing a Function",
    "text": "4.4 Writing a Function\nConsider the Fibonacci Sequence \\(1, 1, 2, 3, 5, 8, 13, 21, 34, ...\\). The next number is found by adding up the two numbers before it. We are going to use 3 ways to solve the problems.\nThe first is a recursive solution.\n\ndef fib_rs(n):\n    if (n==1 or n==2):\n        return 1\n    else:\n        return fib_rs(n - 1) + fib_rs(n - 2)\n\n%timeit fib_rs(10)\n\n8.83 μs ± 51 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n\n\nThe second uses dynamic programming memoization.\n\ndef fib_dm_helper(n, mem):\n    if mem[n] is not None:\n        return mem[n]\n    elif (n == 1 or n == 2):\n        result = 1\n    else:\n        result = fib_dm_helper(n - 1, mem) + fib_dm_helper(n - 2, mem)\n    mem[n] = result\n    return result\n\ndef fib_dm(n):\n    mem = [None] * (n + 1)\n    return fib_dm_helper(n, mem)\n\n%timeit fib_dm(10)\n\n1.93 μs ± 9.57 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)\n\n\nThe third is still dynamic programming but bottom-up.\n\ndef fib_dbu(n):\n    mem = [None] * (n + 1)\n    mem[1] = 1;\n    mem[2] = 1;\n    for i in range(3, n + 1):\n        mem[i] = mem[i - 1] + mem[i - 2]\n    return mem[n]\n\n\n%timeit fib_dbu(500)\n\n66.4 μs ± 918 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n\n\nApparently, the three solutions have very different performance for larger n.\n\n4.4.1 Monty Hall\nHere is a function that performs the Monty Hall experiments. In this version, the host opens only one empty door.\n\nimport numpy as np\n\ndef montyhall(ndoors, ntrials):\n    doors = np.arange(1, ndoors + 1) / 10\n    prize = np.random.choice(doors, size=ntrials)\n    player = np.random.choice(doors, size=ntrials)\n    host = np.array([np.random.choice([d for d in doors\n                                       if d not in [player[x], prize[x]]])\n                     for x in range(ntrials)])\n    player2 = np.array([np.random.choice([d for d in doors\n                                          if d not in [player[x], host[x]]])\n                        for x in range(ntrials)])\n    return {'noswitch': np.sum(prize == player), 'switch': np.sum(prize == player2)}\n\nTest it out:\n\nmontyhall(3, 1000)\nmontyhall(4, 1000)\n\n{'noswitch': np.int64(242), 'switch': np.int64(381)}\n\n\nThe true value for the two strategies with \\(n\\) doors are, respectively, \\(1 / n\\) and \\(\\frac{n - 1}{n (n - 2)}\\).\nIn the homework exercise, the host opens \\(m\\) doors that are empty. An argument nempty could be added to the function.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "python.html#variables-versus-objects",
    "href": "python.html#variables-versus-objects",
    "title": "4  Python Refreshment",
    "section": "4.5 Variables versus Objects",
    "text": "4.5 Variables versus Objects\nIn Python, variables and the objects they point to actually live in two different places in the computer memory. Think of variables as pointers to the objects they’re associated with, rather than being those objects. This matters when multiple variables point to the same object.\n\nx = [1, 2, 3]  # create a list; x points to the list\ny = x          # y also points to the same list in the memory\ny.append(4)    # append to y\nx              # x changed!\n\n[1, 2, 3, 4]\n\n\nNow check their addresses\n\nprint(id(x))   # address of x\nprint(id(y))   # address of y\n\n4696292544\n4696292544\n\n\nNonetheless, some data types in Python are “immutable”, meaning that their values cannot be changed in place. One such example is strings.\n\nx = \"abc\"\ny = x\ny = \"xyz\"\nx\n\n'abc'\n\n\nNow check their addresses\n\nprint(id(x))   # address of x\nprint(id(y))   # address of y\n\n4526902032\n4610332432\n\n\nQuestion: What’s mutable and what’s immutable?\nAnything that is a collection of other objects is mutable, except tuples.\nNot all manipulations of mutable objects change the object rather than create a new object. Sometimes when you do something to a mutable object, you get back a new object. Manipulations that change an existing object, rather than create a new one, are referred to as “in-place mutations” or just “mutations.” So:\n\nAll manipulations of immutable types create new objects.\nSome manipulations of mutable types create new objects.\n\nDifferent variables may all be pointing at the same object is preserved through function calls (a behavior known as “pass by object-reference”). So if you pass a list to a function, and that function manipulates that list using an in-place mutation, that change will affect any variable that was pointing to that same object outside the function.\n\nx = [1, 2, 3]\ny = x\n\ndef append_42(input_list):\n    input_list.append(42)\n    return input_list\n\nappend_42(x)\n\n[1, 2, 3, 42]\n\n\nNote that both x and y have been appended by \\(42\\).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "python.html#number-representation",
    "href": "python.html#number-representation",
    "title": "4  Python Refreshment",
    "section": "4.6 Number Representation",
    "text": "4.6 Number Representation\nNumers in a computer’s memory are represented by binary styles (on and off of bits).\n\n4.6.1 Integers\nIf not careful, It is easy to be bitten by overflow with integers when using Numpy and Pandas in Python.\n\nimport numpy as np\n\nx = np.array(2 ** 63 - 1 , dtype = 'int')\nx\n# This should be the largest number numpy can display, with\n# the default int8 type (64 bits)\n\narray(9223372036854775807)\n\n\nNote: on Windows and other platforms, dtype = 'int' may have to be changed to dtype = np.int64 for the code to execute. Source: Stackoverflow\nWhat if we increment it by 1?\n\ny = np.array(x + 1, dtype = 'int')\ny\n# Because of the overflow, it becomes negative!\n\narray(-9223372036854775808)\n\n\nFor vanilla Python, the overflow errors are checked and more digits are allocated when needed, at the cost of being slow.\n\n2 ** 63 * 1000\n\n9223372036854775808000\n\n\nThis number is 1000 times larger than the prior number, but still displayed perfectly without any overflows\n\n\n4.6.2 Floating Number\nStandard double-precision floating point number uses 64 bits. Among them, 1 is for sign, 11 is for exponent, and 52 are fraction significand, See https://en.wikipedia.org/wiki/Double-precision_floating-point_format. The bottom line is that, of course, not every real number is exactly representable.\nIf you have played the Game 24, here is a tricky one:\n\n8 / (3 - 8 / 3) == 24\n\nFalse\n\n\nSurprise?\nThere are more.\n\n0.1 + 0.1 + 0.1 == 0.3\n\nFalse\n\n\n\n0.3 - 0.2 == 0.1\n\nFalse\n\n\nWhat is really going on?\n\nimport decimal\ndecimal.Decimal(0.1)\n\nDecimal('0.1000000000000000055511151231257827021181583404541015625')\n\n\n\ndecimal.Decimal(8 / (3 - 8 / 3))\n\nDecimal('23.999999999999989341858963598497211933135986328125')\n\n\nBecause the mantissa bits are limited, it can not represent a floating point that’s both very big and very precise. Most computers can represent all integers up to \\(2^{53}\\), after that it starts skipping numbers.\n\n2.1 ** 53 + 1 == 2.1 ** 53\n\n# Find a number larger than 2 to the 53rd\n\nTrue\n\n\n\nx = 2.1 ** 53\nfor i in range(1000000):\n    x = x + 1\nx == 2.1 ** 53\n\nTrue\n\n\nWe add 1 to x by 1000000 times, but it still equal to its initial value, 2.1 ** 53. This is because this number is too big that computer can’t handle it with precision like add 1.\nMachine epsilon is the smallest positive floating-point number x such that 1 + x != 1.\n\nprint(np.finfo(float).eps)\nprint(np.finfo(np.float32).eps)\n\n2.220446049250313e-16\n1.1920929e-07",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "python.html#sec-python-venv",
    "href": "python.html#sec-python-venv",
    "title": "4  Python Refreshment",
    "section": "4.7 Virtual Environment",
    "text": "4.7 Virtual Environment\nVirtual environments in Python are essential tools for managing dependencies and ensuring consistency across projects. They allow you to create isolated environments for each project, with its own set of installed packages, separate from the global Python installation. This isolation prevents conflicts between project dependencies and versions, making your projects more reliable and easier to manage. It’s particularly useful when working on multiple projects with differing requirements, or when collaborating with others who may have different setups.\nTo set up a virtual environment, you first need to ensure that Python is installed on your system. Most modern Python installations come with the venv module, which is used to create virtual environments. Here’s how to set one up:\n\nOpen your command line interface.\nNavigate to your project directory.\nRun python3 -m venv myenv, where myenv is the name of the virtual environment to be created. Choose an informative name.\n\nThis command creates a new directory named myenv (or your chosen name) in your project directory, containing the virtual environment.\nTo start using this environment, you need to activate it. The activation command varies depending on your operating system:\n\nOn Windows, run myenv\\Scripts\\activate.\nOn Linux or MacOS, use source myenv/bin/activate or . myenv/bin/activate.\n\nOnce activated, your command line will typically show the name of the virtual environment, and you can then install and use packages within this isolated environment without affecting your global Python setup.\nTo exit the virtual environment, simply type deactivate in your command line. This will return you to your system’s global Python environment.\nAs an example, let’s install a package, like numpy, in this newly created virtual environment:\n\nEnsure your virtual environment is activated.\nRun pip install numpy.\n\nThis command installs the requests library in your virtual environment. You can verify the installation by running pip list, which should show requests along with its version.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "python.html#numpy",
    "href": "python.html#numpy",
    "title": "4  Python Refreshment",
    "section": "4.8 Numpy",
    "text": "4.8 Numpy",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "manipulation.html",
    "href": "manipulation.html",
    "title": "5  Data Manipulation",
    "section": "",
    "text": "5.1 Introduction\nData manipulation is crucial for transforming raw data into a more analyzable format, essential for uncovering patterns and ensuring accurate analysis. This chapter introduces the core techniques for data manipulation in Python, utilizing the Pandas library, a cornerstone for data handling within Python’s data science toolkit.\nPython’s ecosystem is rich with libraries that facilitate not just data manipulation but comprehensive data analysis. Pandas, in particular, provides extensive functionality for data manipulation tasks including reading, cleaning, transforming, and summarizing data. Using real-world datasets, we will explore how to leverage Python for practical data manipulation tasks.\nBy the end of this chapter, you will learn to:",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Manipulation</span>"
    ]
  },
  {
    "objectID": "manipulation.html#introduction",
    "href": "manipulation.html#introduction",
    "title": "5  Data Manipulation",
    "section": "",
    "text": "Import/export data from/to diverse sources.\nClean and preprocess data efficiently.\nTransform and aggregate data to derive insights.\nMerge and concatenate datasets from various origins.\nAnalyze real-world datasets using these techniques.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Manipulation</span>"
    ]
  },
  {
    "objectID": "manipulation.html#nyc-crash-data",
    "href": "manipulation.html#nyc-crash-data",
    "title": "5  Data Manipulation",
    "section": "5.2 NYC Crash Data",
    "text": "5.2 NYC Crash Data\nConsider a subset of the NYC Crash Data, which contains all NYC motor vehicle collisions data with documentation from NYC Open Data. We downloaded the crash data for the week of June 30, 2024, on February 12, 2025, in CSC format.\n\nimport numpy as np\nimport pandas as pd\n\n# Load the dataset\nfile_path = 'data/nyccrashes_2024w0630_by20250212.csv'\ndf = pd.read_csv(file_path,\n                 dtype={'LATITUDE': np.float32,\n                        'LONGITUDE': np.float32,\n                        'ZIP CODE': str})\n\n# Replace column names: convert to lowercase and replace spaces with underscores\ndf.columns = df.columns.str.lower().str.replace(' ', '_')\n\n# Check for missing values\ndf.isnull().sum()\n\ncrash_date                          0\ncrash_time                          0\nborough                           542\nzip_code                          542\nlatitude                          131\nlongitude                         131\nlocation                          131\non_street_name                    546\ncross_street_name                 932\noff_street_name                  1330\nnumber_of_persons_injured           0\nnumber_of_persons_killed            0\nnumber_of_pedestrians_injured       0\nnumber_of_pedestrians_killed        0\nnumber_of_cyclist_injured           0\nnumber_of_cyclist_killed            0\nnumber_of_motorist_injured          0\nnumber_of_motorist_killed           0\ncontributing_factor_vehicle_1      11\ncontributing_factor_vehicle_2     450\ncontributing_factor_vehicle_3    1702\ncontributing_factor_vehicle_4    1824\ncontributing_factor_vehicle_5    1862\ncollision_id                        0\nvehicle_type_code_1                33\nvehicle_type_code_2               645\nvehicle_type_code_3              1714\nvehicle_type_code_4              1828\nvehicle_type_code_5              1862\ndtype: int64\n\n\nTake a peek at the first five rows:\n\ndf.head()\n\n\n\n\n\n\n\n\ncrash_date\ncrash_time\nborough\nzip_code\nlatitude\nlongitude\nlocation\non_street_name\ncross_street_name\noff_street_name\n...\ncontributing_factor_vehicle_2\ncontributing_factor_vehicle_3\ncontributing_factor_vehicle_4\ncontributing_factor_vehicle_5\ncollision_id\nvehicle_type_code_1\nvehicle_type_code_2\nvehicle_type_code_3\nvehicle_type_code_4\nvehicle_type_code_5\n\n\n\n\n0\n06/30/2024\n23:17\nBRONX\n10460\n40.838844\n-73.878166\n(40.838844, -73.87817)\nEAST 177 STREET\nDEVOE AVENUE\nNaN\n...\nUnspecified\nNaN\nNaN\nNaN\n4737486\nSedan\nPick-up Truck\nNaN\nNaN\nNaN\n\n\n1\n06/30/2024\n8:30\nBRONX\n10468\n40.862732\n-73.903328\n(40.862732, -73.90333)\nWEST FORDHAM ROAD\nGRAND AVENUE\nNaN\n...\nUnspecified\nNaN\nNaN\nNaN\n4737502\nSedan\nNaN\nNaN\nNaN\nNaN\n\n\n2\n06/30/2024\n20:47\nNaN\nNaN\n40.763630\n-73.953300\n(40.76363, -73.9533)\nFDR DRIVE\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\n4737510\nSedan\nNaN\nNaN\nNaN\nNaN\n\n\n3\n06/30/2024\n13:10\nBROOKLYN\n11234\n40.617031\n-73.919891\n(40.61703, -73.91989)\nEAST 57 STREET\nAVENUE O\nNaN\n...\nDriver Inattention/Distraction\nNaN\nNaN\nNaN\n4737499\nSedan\nSedan\nNaN\nNaN\nNaN\n\n\n4\n06/30/2024\n16:42\nNaN\nNaN\nNaN\nNaN\nNaN\n33 STREET\nASTORIA BOULEVARD\nNaN\n...\nUnspecified\nNaN\nNaN\nNaN\n4736925\nSedan\nStation Wagon/Sport Utility Vehicle\nNaN\nNaN\nNaN\n\n\n\n\n5 rows × 29 columns\n\n\n\nA quick summary of the data types of the columns:\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1876 entries, 0 to 1875\nData columns (total 29 columns):\n #   Column                         Non-Null Count  Dtype  \n---  ------                         --------------  -----  \n 0   crash_date                     1876 non-null   object \n 1   crash_time                     1876 non-null   object \n 2   borough                        1334 non-null   object \n 3   zip_code                       1334 non-null   object \n 4   latitude                       1745 non-null   float32\n 5   longitude                      1745 non-null   float32\n 6   location                       1745 non-null   object \n 7   on_street_name                 1330 non-null   object \n 8   cross_street_name              944 non-null    object \n 9   off_street_name                546 non-null    object \n 10  number_of_persons_injured      1876 non-null   int64  \n 11  number_of_persons_killed       1876 non-null   int64  \n 12  number_of_pedestrians_injured  1876 non-null   int64  \n 13  number_of_pedestrians_killed   1876 non-null   int64  \n 14  number_of_cyclist_injured      1876 non-null   int64  \n 15  number_of_cyclist_killed       1876 non-null   int64  \n 16  number_of_motorist_injured     1876 non-null   int64  \n 17  number_of_motorist_killed      1876 non-null   int64  \n 18  contributing_factor_vehicle_1  1865 non-null   object \n 19  contributing_factor_vehicle_2  1426 non-null   object \n 20  contributing_factor_vehicle_3  174 non-null    object \n 21  contributing_factor_vehicle_4  52 non-null     object \n 22  contributing_factor_vehicle_5  14 non-null     object \n 23  collision_id                   1876 non-null   int64  \n 24  vehicle_type_code_1            1843 non-null   object \n 25  vehicle_type_code_2            1231 non-null   object \n 26  vehicle_type_code_3            162 non-null    object \n 27  vehicle_type_code_4            48 non-null     object \n 28  vehicle_type_code_5            14 non-null     object \ndtypes: float32(2), int64(9), object(18)\nmemory usage: 410.5+ KB\n\n\nNow we can do some cleaning after a quick browse.\n\n# Replace invalid coordinates (latitude=0, longitude=0 or NaN) with NaN\ndf.loc[(df['latitude'] == 0) & (df['longitude'] == 0), \n       ['latitude', 'longitude']] = pd.NA\ndf['latitude'] = df['latitude'].replace(0, pd.NA)\ndf['longitude'] = df['longitude'].replace(0, pd.NA)\n\n# Drop the redundant `latitute` and `longitude` columns\ndf = df.drop(columns=['location'])\n\n# Converting 'crash_date' and 'crash_time' columns into a single datetime column\ndf['crash_datetime'] = pd.to_datetime(df['crash_date'] + ' ' \n                       + df['crash_time'], format='%m/%d/%Y %H:%M', errors='coerce')\n\n# Drop the original 'crash_date' and 'crash_time' columns\ndf = df.drop(columns=['crash_date', 'crash_time'])\n\nLet’s get some basic frequency tables of borough and zip_code, whose values could be used to check their validity against the legitmate values.\n\n# Frequency table for 'borough' without filling missing values\nborough_freq = df['borough'].value_counts(dropna=False).reset_index()\nborough_freq.columns = ['borough', 'count']\n\n# Frequency table for 'zip_code' without filling missing values\nzip_code_freq = df['zip_code'].value_counts(dropna=False).reset_index()\nzip_code_freq.columns = ['zip_code', 'count']\nzip_code_freq\n\n\n\n\n\n\n\n\nzip_code\ncount\n\n\n\n\n0\nNaN\n542\n\n\n1\n11207\n31\n\n\n2\n11208\n28\n\n\n3\n11236\n28\n\n\n4\n11101\n23\n\n\n...\n...\n...\n\n\n164\n10470\n1\n\n\n165\n11040\n1\n\n\n166\n11693\n1\n\n\n167\n11415\n1\n\n\n168\n10025\n1\n\n\n\n\n169 rows × 2 columns\n\n\n\nA comprehensive list of ZIP codes by borough can be obtained, for example, from the New York City Department of Health’s UHF Codes. We can use this list to check the validity of the zip codes in the data.\n\n# List of valid NYC ZIP codes compiled from UHF codes\n# Define all_valid_zips based on the earlier extracted ZIP codes\nall_valid_zips = {\n    10463, 10471, 10466, 10469, 10470, 10475, 10458, 10467, 10468,\n    10461, 10462, 10464, 10465, 10472, 10473, 10453, 10457, 10460,\n    10451, 10452, 10456, 10454, 10455, 10459, 10474, 11211, 11222,\n    11201, 11205, 11215, 11217, 11231, 11213, 11212, 11216, 11233,\n    11238, 11207, 11208, 11220, 11232, 11204, 11218, 11219, 11230,\n    11203, 11210, 11225, 11226, 11234, 11236, 11239, 11209, 11214,\n    11228, 11223, 11224, 11229, 11235, 11206, 11221, 11237, 10031,\n    10032, 10033, 10034, 10040, 10026, 10027, 10030, 10037, 10039,\n    10029, 10035, 10023, 10024, 10025, 10021, 10028, 10044, 10128,\n    10001, 10011, 10018, 10019, 10020, 10036, 10010, 10016, 10017,\n    10022, 10012, 10013, 10014, 10002, 10003, 10009, 10004, 10005,\n    10006, 10007, 10038, 10280, 11101, 11102, 11103, 11104, 11105,\n    11106, 11368, 11369, 11370, 11372, 11373, 11377, 11378, 11354,\n    11355, 11356, 11357, 11358, 11359, 11360, 11361, 11362, 11363,\n    11364, 11374, 11375, 11379, 11385, 11365, 11366, 11367, 11414,\n    11415, 11416, 11417, 11418, 11419, 11420, 11421, 11412, 11423,\n    11432, 11433, 11434, 11435, 11436, 11004, 11005, 11411, 11413,\n    11422, 11426, 11427, 11428, 11429, 11691, 11692, 11693, 11694,\n    11695, 11697, 10302, 10303, 10310, 10301, 10304, 10305, 10314,\n    10306, 10307, 10308, 10309, 10312\n}\n\n    \n# Convert set to list of strings\nall_valid_zips = list(map(str, all_valid_zips))\n\n# Identify invalid ZIP codes (including NaN)\ninvalid_zips = df[\n    df['zip_code'].isna() | ~df['zip_code'].isin(all_valid_zips)\n    ]['zip_code']\n\n# Calculate frequency of invalid ZIP codes\ninvalid_zip_freq = invalid_zips.value_counts(dropna=False).reset_index()\ninvalid_zip_freq.columns = ['zip_code', 'frequency']\n\ninvalid_zip_freq\n\n\n\n\n\n\n\n\nzip_code\nfrequency\n\n\n\n\n0\nNaN\n542\n\n\n1\n10065\n7\n\n\n2\n11249\n4\n\n\n3\n10112\n1\n\n\n4\n11040\n1\n\n\n\n\n\n\n\nAs it turns out, the collection of valid NYC zip codes differ from different sources. From United States Zip Codes, 10065 appears to be a valid NYC zip code. Under this circumstance, it might be safer to not remove any zip code from the data.\nTo be safe, let’s concatenate valid and invalid zips.\n\n# Convert invalid ZIP codes to a set of strings\ninvalid_zips_set = set(invalid_zip_freq['zip_code'].dropna().astype(str))\n\n# Convert all_valid_zips to a set of strings (if not already)\nvalid_zips_set = set(map(str, all_valid_zips))\n\n# Merge both sets\nmerged_zips = invalid_zips_set | valid_zips_set  # Union of both sets\n\nAre missing in zip code and borough always co-occur?\n\n# Check if missing values in 'zip_code' and 'borough' always co-occur\n# Count rows where both are missing\nmissing_cooccur = df[['zip_code', 'borough']].isnull().all(axis=1).sum()\n# Count total missing in 'zip_code' and 'borough', respectively\ntotal_missing_zip_code = df['zip_code'].isnull().sum()\ntotal_missing_borough = df['borough'].isnull().sum()\n\n# If missing in both columns always co-occur, the number of missing\n# co-occurrences should be equal to the total missing in either column\nnp.array([missing_cooccur, total_missing_zip_code, total_missing_borough])\n\narray([542, 542, 542])\n\n\nAre there cases where zip_code and borough are missing but the geo codes are not missing? If so, fill in zip_code and borough using the geo codes by reverse geocoding.\nFirst make sure geopy is installed.\npip install geopy\nNow we use model Nominatim in package geopy to reverse geocode.\n\nfrom geopy.geocoders import Nominatim\nimport time\n\n# Initialize the geocoder; the `user_agent` is your identifier \n# when using the service. Be mindful not to crash the server\n# by unlimited number of queries, especially invalid code.\ngeolocator = Nominatim(user_agent=\"jyGeopyTry\")\n\nWe write a function to do the reverse geocoding given lattitude and longitude.\n\n# Function to fill missing zip_code\ndef get_zip_code(latitude, longitude):\n    try:\n        location = geolocator.reverse((latitude, longitude), timeout=10)\n        if location:\n            address = location.raw['address']\n            zip_code = address.get('postcode', None)\n            return zip_code\n        else:\n            return None\n    except Exception as e:\n        print(f\"Error: {e} for coordinates {latitude}, {longitude}\")\n        return None\n    finally:\n        time.sleep(1)  # Delay to avoid overwhelming the service\n\nLet’s try it out:\n\n# Example usage\nlatitude = 40.730610\nlongitude = -73.935242\nget_zip_code(latitude, longitude)\n\n'11101'\n\n\nThe function get_zip_code can then be applied to rows where zip code is missing but geocodes are not to fill the missing zip code.\nOnce zip code is known, figuring out burough is simple because valid zip codes from each borough are known.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Manipulation</span>"
    ]
  },
  {
    "objectID": "manipulation.html#accessing-census-data",
    "href": "manipulation.html#accessing-census-data",
    "title": "5  Data Manipulation",
    "section": "5.3 Accessing Census Data",
    "text": "5.3 Accessing Census Data\nThe U.S. Census Bureau provides extensive demographic, economic, and social data through multiple surveys, including the decennial Census, the American Community Survey (ACS), and the Economic Census. These datasets offer valuable insights into population trends, economic conditions, and community characteristics at multiple geographic levels.\nThere are several ways to access Census data:\n\nCensus API: The Census API allows programmatic access to various datasets. It supports queries for different geographic levels and time periods.\ndata.census.gov: The official web interface for searching and downloading Census data.\nIPUMS USA: Provides harmonized microdata for longitudinal research. Available at IPUMS USA.\nNHGIS: Offers historical Census data with geographic information. Visit NHGIS.\n\nIn addition, Python tools simplify API access and data retrieval.\n\n5.3.1 Python Tools for Accessing Census Data\nSeveral Python libraries facilitate Census data retrieval:\n\ncensusapi: The official API wrapper for direct access to Census datasets.\ncensus: A high-level interface to the Census API, supporting ACS and decennial Census queries. See census on PyPI.\ncensusdata: A package for downloading and processing Census data directly in Python. Available at censusdata documentation.\nuszipcode: A library for retrieving Census and geographic information by ZIP code. See uszipcode on PyPI.\n\n\n\n5.3.2 Zip-Code Level for NYC Crash Data\nNow that we have NYC crash data, we might want to analyze patterns at the zip-code level to understand whether certain demographic or economic factors correlate with traffic incidents. While the crash dataset provides details about individual accidents, such as location, time, and severity, it does not contain contextual information about the neighborhoods where these crashes occur.\nTo perform meaningful zip-code-level analysis, we need additional data sources that provide relevant demographic, economic, and geographic variables. For example, understanding whether high-income areas experience fewer accidents, or whether population density influences crash frequency, requires integrating Census data. Key variables such as population size, median household income, employment rate, and population density can provide valuable context for interpreting crash trends across different zip codes.\nSince the Census Bureau provides detailed estimates for these variables at the zip-code level, we can use the Census API or other tools to retrieve relevant data and merge it with the NYC crash dataset. To access the Census API, you need an API key, which is free and easy to obtain. Visit the Census API Request page and submit your email address to receive a key. Once you have the key, you must include it in your API requests to access Census data. The following demonstration assumes that you have registered, obtained your API key, and saved it in a file called censusAPIkey.txt.\n\n# Import modules\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport geopandas as gpd\nfrom census import Census\nfrom us import states\nimport os\nimport io\n\napi_key = open(\"censusAPIkey.txt\").read().strip()\nc = Census(api_key)\n\nSuppose that we want to get some basic info from ACS data of the year of 2023 for all the NYC zip codes. The variable names can be found in the ACS variable documentation.\n\nACS_YEAR = 2023\nACS_DATASET = \"acs/acs5\"\n\n# Important ACS variables (including land area for density calculation)\nACS_VARIABLES = {\n    \"B01003_001E\": \"Total Population\",\n    \"B19013_001E\": \"Median Household Income\",\n    \"B02001_002E\": \"White Population\",\n    \"B02001_003E\": \"Black Population\",\n    \"B02001_005E\": \"Asian Population\",\n    \"B15003_022E\": \"Bachelor’s Degree Holders\",\n    \"B15003_025E\": \"Graduate Degree Holders\",\n    \"B23025_002E\": \"Labor Force\",\n    \"B23025_005E\": \"Unemployed\",\n    \"B25077_001E\": \"Median Home Value\"\n}\n\n# Convert set to list of strings\nmerged_zips = list(map(str, merged_zips))\n\nLet’s set up the query to request the ACS data, and process the returned data.\n\nacs_data = c.acs5.get(\n    list(ACS_VARIABLES.keys()), \n    {'for': f'zip code tabulation area:{\",\".join(merged_zips)}'}\n    )\n\n# Convert to DataFrame\ndf_acs = pd.DataFrame(acs_data)\n\n# Rename columns\ndf_acs.rename(columns=ACS_VARIABLES, inplace=True)\ndf_acs.rename(columns={\"zip code tabulation area\": \"ZIP Code\"}, inplace=True)\n\nWe could save the ACS data df_acs in feather format (see next Section).\n#| eval: false\ndf_acs.to_feather(\"data/acs2023.feather\")\nThe population density could be an important factor for crash likelihood. To obtain the population densities, we need the areas of the zip codes. The shape files can be obtained from NYC Open Data.\n\nimport requests\nimport zipfile\nimport geopandas as gpd\n\n# Define the NYC MODZCTA shapefile URL and extraction directory\nshapefile_url = \"https://data.cityofnewyork.us/api/geospatial/pri4-ifjk?method=export&format=Shapefile\"\nextract_dir = \"MODZCTA_Shapefile\"\n\n# Create the directory if it doesn't exist\nos.makedirs(extract_dir, exist_ok=True)\n\n# Step 1: Download and extract the shapefile\nprint(\"Downloading MODZCTA shapefile...\")\nresponse = requests.get(shapefile_url)\nwith zipfile.ZipFile(io.BytesIO(response.content), \"r\") as z:\n    z.extractall(extract_dir)\n\nprint(f\"Shapefile extracted to: {extract_dir}\")\n\nDownloading MODZCTA shapefile...\nShapefile extracted to: MODZCTA_Shapefile\n\n\nNow we process the shape file to calculate the areas of the polygons.\n\n# Step 2: Automatically detect the correct .shp file\nshapefile_path = None\nfor file in os.listdir(extract_dir):\n    if file.endswith(\".shp\"):\n        shapefile_path = os.path.join(extract_dir, file)\n        break  # Use the first .shp file found\n\nif not shapefile_path:\n    raise FileNotFoundError(\"No .shp file found in extracted directory.\")\n\nprint(f\"Using shapefile: {shapefile_path}\")\n\n# Step 3: Load the shapefile into GeoPandas\ngdf = gpd.read_file(shapefile_path)\n\n# Step 4: Convert to CRS with meters for accurate area calculation\ngdf = gdf.to_crs(epsg=3857)\n\n# Step 5: Compute land area in square miles\ngdf['land_area_sq_miles'] = gdf['geometry'].area / 2_589_988.11\n# 1 square mile = 2,589,988.11 square meters\n\nprint(gdf[['modzcta', 'land_area_sq_miles']].head())\n\nUsing shapefile: MODZCTA_Shapefile/geo_export_1daca795-0288-4cf1-be6c-e69d6ffefeee.shp\n  modzcta  land_area_sq_miles\n0   10001            1.153516\n1   10002            1.534509\n2   10003            1.008318\n3   10026            0.581848\n4   10004            0.256876\n\n\nLet’s export this data frame for future usage in feather format (see next Section).\n\ngdf[['modzcta', 'land_area_sq_miles']].to_feather('data/nyc_zip_areas.feather')\n\nNow we are ready to merge the two data frames.\n\n# Merge ACS data (`df_acs`) directly with MODZCTA land area (`gdf`)\ngdf = gdf.merge(df_acs, left_on='modzcta', right_on='ZIP Code', how='left')\n\n# Calculate Population Density (people per square mile)\ngdf['popdensity_per_sq_mile'] = (\n    gdf['Total Population'] / gdf['land_area_sq_miles']\n    )\n\n# Display first few rows\nprint(gdf[['modzcta', 'Total Population', 'land_area_sq_miles',\n    'popdensity_per_sq_mile']].head())\n\n  modzcta  Total Population  land_area_sq_miles  popdensity_per_sq_mile\n0   10001           27004.0            1.153516            23410.171200\n1   10002           76518.0            1.534509            49864.797219\n2   10003           53877.0            1.008318            53432.563117\n3   10026           38265.0            0.581848            65764.650082\n4   10004            4579.0            0.256876            17825.700993\n\n\nSome visualization of population density.\n\nimport matplotlib.pyplot as plt\nimport geopandas as gpd\n\n# Set up figure and axis\nfig, ax = plt.subplots(figsize=(10, 12))\n\n# Plot the choropleth map\ngdf.plot(column='popdensity_per_sq_mile', \n         cmap='viridis',  # Use a visually appealing color map\n         linewidth=0.8, \n         edgecolor='black',\n         legend=True,\n         legend_kwds={'label': \"Population Density (per sq mile)\",\n             'orientation': \"horizontal\"},\n         ax=ax)\n\n# Add a title\nax.set_title(\"Population Density by ZCTA in NYC\", fontsize=14)\n\n# Remove axes\nax.set_xticks([])\nax.set_yticks([])\nax.set_frame_on(False)\n\n# Show the plot\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Manipulation</span>"
    ]
  },
  {
    "objectID": "manipulation.html#cross-platform-data-format-arrow",
    "href": "manipulation.html#cross-platform-data-format-arrow",
    "title": "5  Data Manipulation",
    "section": "5.4 Cross-platform Data Format Arrow",
    "text": "5.4 Cross-platform Data Format Arrow\nThe CSV format (and related formats like TSV - tab-separated values) for data tables is ubiquitous, convenient, and can be read or written by many different data analysis environments, including spreadsheets. An advantage of the textual representation of the data in a CSV file is that the entire data table, or portions of it, can be previewed in a text editor. However, the textual representation can be ambiguous and inconsistent. The format of a particular column: Boolean, integer, floating-point, text, factor, etc. must be inferred from text representation, often at the expense of reading the entire file before these inferences can be made. Experienced data scientists are aware that a substantial part of an analysis or report generation is often the “data cleaning” involved in preparing the data for analysis. This can be an open-ended task — it required numerous trial-and-error iterations to create the list of different missing data representations we use for the sample CSV file and even now we are not sure we have them all.\nTo read and export data efficiently, leveraging the Apache Arrow library can significantly improve performance and storage efficiency, especially with large datasets. The IPC (Inter-Process Communication) file format in the context of Apache Arrow is a key component for efficiently sharing data between different processes, potentially written in different programming languages. Arrow’s IPC mechanism is designed around two main file formats:\n\nStream Format: For sending an arbitrary length sequence of Arrow record batches (tables). The stream format is useful for real-time data exchange where the size of the data is not known upfront and can grow indefinitely.\nFile (or Feather) Format: Optimized for storage and memory-mapped access, allowing for fast random access to different sections of the data. This format is ideal for scenarios where the entire dataset is available upfront and can be stored in a file system for repeated reads and writes.\n\nApache Arrow provides a columnar memory format for flat and hierarchical data, optimized for efficient data analytics. It can be used in Python through the pyarrow package. Here’s how you can use Arrow to read, manipulate, and export data, including a demonstration of storage savings.\nFirst, ensure you have pyarrow installed on your computer (and preferrably, in your current virtual environment):\npip install pyarrow\nFeather is a fast, lightweight, and easy-to-use binary file format for storing data frames, optimized for speed and efficiency, particularly for IPC and data sharing between Python and R or Julia.\nThe following code processes the cleaned data in CSV format from Mohammad Mundiwala and write out in Arrow format.\n\nimport pandas as pd\n\n# Read CSV, ensuring 'zip_code' is string and 'crash_datetime' is parsed as datetime\ndf = pd.read_csv('data/nyc_crashes_cleaned_mm.csv',\n                 dtype={'zip_code': str},\n                 parse_dates=['crash_datetime'])\n\n# Drop the 'date' and 'time' columns\ndf = df.drop(columns=['crash_date', 'crash_time'])\n\n# Move 'crash_datetime' to the first column\ndf = df[['crash_datetime'] + df.drop(columns=['crash_datetime']).columns.tolist()]\n\ndf['zip_code'] = df['zip_code'].astype(str).str.rstrip('.0')\n\ndf = df.sort_values(by='crash_datetime')\n\ndf.to_feather('nyccrashes_cleaned.feather')\n\nLet’s compare the file sizes of the feather format and the CSV format.\n\nimport os\n\n# File paths\ncsv_file = 'data/nyccrashes_2024w0630_by20250212.csv'\nfeather_file = 'data/nyccrashes_cleaned.feather'\n\n# Get file sizes in bytes\ncsv_size = os.path.getsize(csv_file)\nfeather_size = os.path.getsize(feather_file)\n\n# Convert bytes to a more readable format (e.g., MB)\ncsv_size_mb = csv_size / (1024 * 1024)\nfeather_size_mb = feather_size / (1024 * 1024)\n\n# Print the file sizes\nprint(f\"CSV file size: {csv_size_mb:.2f} MB\")\nprint(f\"Feather file size: {feather_size_mb:.2f} MB\")\n\nCSV file size: 0.34 MB\nFeather file size: 0.19 MB\n\n\nRead the feather file back in:\n#| eval: false\ndff = pd.read_feather(\"data/nyccrashes_cleaned.feather\")\ndff.shape",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Manipulation</span>"
    ]
  },
  {
    "objectID": "stats.html",
    "href": "stats.html",
    "title": "6  Statistical Tests and Models",
    "section": "",
    "text": "6.1 Tests for Exploratory Data Analysis\nA collection of functions are available from scipy.stats.\nSince R has a richer collections of statistical functions, we can call R function from Python with rpy2. See, for example, a blog on this subject.\nFor example, fisher_exact can only handle 2x2 contingency tables. For contingency tables larger than 2x2, we can call fisher.test() from R through rpy2. See this StackOverflow post. Note that the . in function names and arguments are replaced with _.\nimport pandas as pd\nimport numpy as np\nimport rpy2.robjects.numpy2ri\nfrom rpy2.robjects.packages import importr\nrpy2.robjects.numpy2ri.activate()\n\nstats = importr('stats')\n\nw0630 = pd.read_feather(\"data/nyccrashes_cleaned.feather\")\nw0630[\"injury\"] = np.where(w0630[\"number_of_persons_injured\"] &gt; 0, 1, 0)\nm = pd.crosstab(w0630[\"injury\"], w0630[\"borough\"])\nprint(m)\n\nres = stats.fisher_test(m.to_numpy(), simulate_p_value = True)\nprint(res)\n\nLoading custom .Rprofileborough  BRONX  BROOKLYN  MANHATTAN  QUEENS  STATEN ISLAND\ninjury                                                    \n0          149       345        164     249             65\n1          129       266        127     227             28\n\n    Fisher's Exact Test for Count Data with simulated p-value (based on\n    2000 replicates)\n\ndata:  structure(c(149L, 129L, 345L, 266L, 164L, 127L, 249L, 227L, 65L, 28L), dim = c(2L, 5L))\np-value = 0.03348\nalternative hypothesis: two.sided",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Statistical Tests and Models</span>"
    ]
  },
  {
    "objectID": "stats.html#tests-for-exploratory-data-analysis",
    "href": "stats.html#tests-for-exploratory-data-analysis",
    "title": "6  Statistical Tests and Models",
    "section": "",
    "text": "Comparing the locations of two samples\n\nttest_ind: t-test for two independent samples\nttest_rel: t-test for paired samples\nranksums: Wilcoxon rank-sum test for two independent samples\nwilcoxon: Wilcoxon signed-rank test for paired samples\n\nComparing the locations of multiple samples\n\nf_oneway: one-way ANOVA\nkruskal: Kruskal-Wallis H-test\n\nTests for associations in contigency tables\n\nchi2_contingency: Chi-square test of independence of variables\nfisher_exact: Fisher exact test on a 2x2 contingency table\n\nGoodness of fit\n\ngoodness_of_fit: distribution could contain unspecified parameters\nanderson: Anderson-Darling test\nkstest: Kolmogorov-Smirnov test\nchisquare: one-way chi-square test\nnormaltest: test for normality",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Statistical Tests and Models</span>"
    ]
  },
  {
    "objectID": "stats.html#statistical-modeling",
    "href": "stats.html#statistical-modeling",
    "title": "6  Statistical Tests and Models",
    "section": "6.2 Statistical Modeling",
    "text": "6.2 Statistical Modeling\nStatistical modeling is a cornerstone of data science, offering tools to understand complex relationships within data and to make predictions. Python, with its rich ecosystem for data analysis, features the statsmodels package— a comprehensive library designed for statistical modeling, tests, and data exploration. statsmodels stands out for its focus on classical statistical models and compatibility with the Python scientific stack (numpy, scipy, pandas).\n\n6.2.1 Installation of statsmodels\nTo start with statistical modeling, ensure statsmodels is installed:\nUsing pip:\npip install statsmodels\n\n\n6.2.2 Linear Model\nLet’s simulate some data for illustrations.\n\nimport numpy as np\n\nnobs = 200\nncov = 5\nnp.random.seed(123)\nx = np.random.random((nobs, ncov)) # Uniform over [0, 1)\nbeta = np.repeat(1, ncov)\ny = 2 + np.dot(x, beta) + np.random.normal(size = nobs)\n\nCheck the shape of y:\n\ny.shape\n\n(200,)\n\n\nCheck the shape of x:\n\nx.shape\n\n(200, 5)\n\n\nThat is, the true linear regression model is \\[\ny = 2 + x_1 + x_2 + x_3 + x_4 + x_5 + \\epsilon.\n\\]\nA regression model for the observed data can be fitted as\n\nimport statsmodels.api as sma\nxmat = sma.add_constant(x)\nmymod = sma.OLS(y, xmat)\nmyfit = mymod.fit()\nmyfit.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\ny\nR-squared:\n0.309\n\n\nModel:\nOLS\nAdj. R-squared:\n0.292\n\n\nMethod:\nLeast Squares\nF-statistic:\n17.38\n\n\nDate:\nMon, 24 Feb 2025\nProb (F-statistic):\n3.31e-14\n\n\nTime:\n11:56:15\nLog-Likelihood:\n-272.91\n\n\nNo. Observations:\n200\nAIC:\n557.8\n\n\nDf Residuals:\n194\nBIC:\n577.6\n\n\nDf Model:\n5\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nconst\n1.8754\n0.282\n6.656\n0.000\n1.320\n2.431\n\n\nx1\n1.1703\n0.248\n4.723\n0.000\n0.682\n1.659\n\n\nx2\n0.8988\n0.235\n3.825\n0.000\n0.435\n1.362\n\n\nx3\n0.9784\n0.238\n4.114\n0.000\n0.509\n1.448\n\n\nx4\n1.3418\n0.250\n5.367\n0.000\n0.849\n1.835\n\n\nx5\n0.6027\n0.239\n2.519\n0.013\n0.131\n1.075\n\n\n\n\n\n\n\n\nOmnibus:\n0.810\nDurbin-Watson:\n1.978\n\n\nProb(Omnibus):\n0.667\nJarque-Bera (JB):\n0.903\n\n\nSkew:\n-0.144\nProb(JB):\n0.637\n\n\nKurtosis:\n2.839\nCond. No.\n8.31\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nQuestions to review:\n\nHow are the regression coefficients interpreted? Intercept?\nWhy does it make sense to center the covariates?\n\nNow we form a data frame with the variables\n\nimport pandas as pd\ndf = np.concatenate((y.reshape((nobs, 1)), x), axis = 1)\ndf = pd.DataFrame(data = df,\n                  columns = [\"y\"] + [\"x\" + str(i) for i in range(1,\n                  ncov + 1)])\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 200 entries, 0 to 199\nData columns (total 6 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   y       200 non-null    float64\n 1   x1      200 non-null    float64\n 2   x2      200 non-null    float64\n 3   x3      200 non-null    float64\n 4   x4      200 non-null    float64\n 5   x5      200 non-null    float64\ndtypes: float64(6)\nmemory usage: 9.5 KB\n\n\nLet’s use a formula to specify the regression model as in R, and fit a robust linear model (rlm) instead of OLS. Note that the model specification and the function interface is similar to R.\n\nimport statsmodels.formula.api as smf\nmymod = smf.rlm(formula = \"y ~ x1 + x2 + x3 + x4 + x5\", data = df)\nmyfit = mymod.fit()\nmyfit.summary()\n\n\nRobust linear Model Regression Results\n\n\nDep. Variable:\ny\nNo. Observations:\n200\n\n\nModel:\nRLM\nDf Residuals:\n194\n\n\nMethod:\nIRLS\nDf Model:\n5\n\n\nNorm:\nHuberT\n\n\n\n\nScale Est.:\nmad\n\n\n\n\nCov Type:\nH1\n\n\n\n\nDate:\nMon, 24 Feb 2025\n\n\n\n\nTime:\n11:56:15\n\n\n\n\nNo. Iterations:\n16\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n1.8353\n0.294\n6.246\n0.000\n1.259\n2.411\n\n\nx1\n1.1254\n0.258\n4.355\n0.000\n0.619\n1.632\n\n\nx2\n0.9664\n0.245\n3.944\n0.000\n0.486\n1.447\n\n\nx3\n0.9995\n0.248\n4.029\n0.000\n0.513\n1.486\n\n\nx4\n1.3275\n0.261\n5.091\n0.000\n0.816\n1.839\n\n\nx5\n0.6768\n0.250\n2.712\n0.007\n0.188\n1.166\n\n\n\nIf the model instance has been used for another fit with different fit parameters, then the fit options might not be the correct ones anymore .\n\n\nFor model diagnostics, one can check residual plots.\n\nimport matplotlib.pyplot as plt\n\nmyOlsFit = smf.ols(formula = \"y ~ x1 + x2 + x3 + x4 + x5\", data = df).fit()\nfig = plt.figure(figsize = (6, 6))\n## residual versus x1; can do the same for other covariates\nfig = sma.graphics.plot_regress_exog(myOlsFit, 'x1', fig=fig)\n\n\n\n\n\n\n\n\nSee more on residual diagnostics and specification tests.\n\n\n6.2.3 Generalized Linear Regression\nA linear regression model cannot be applied to presence/absence or count data. Generalized Linear Models (GLM) extend the classical linear regression to accommodate such response variables, that follow distributions other than the normal distribution. GLMs consist of three main components:\n\nRandom Component: This specifies the distribution of the response variable \\(Y\\). It is assumed to be from the exponential family of distributions, such as Binomial for binary data and Poisson for count data.\nSystematic Component: This consists of the linear predictor, a linear combination of unknown parameters and explanatory variables. It is denoted as \\(\\eta = X\\beta\\), where \\(X\\) represents the explanatory variables, and \\(\\beta\\) represents the coefficients.\nLink Function: The link function, \\(g\\), provides the relationship between the linear predictor and the mean of the distribution function. For a GLM, the mean of \\(Y\\) is related to the linear predictor through the link function as \\(\\mu = g^{-1}(\\eta)\\).\n\nGLMs adapt to various data types through the selection of appropriate link functions and probability distributions. Here, we outline four special cases of GLM: normal regression, logistic regression, Poisson regression, and gamma regression.\n\nNormal Regression (Linear Regression). In normal regression, the response variable has a normal distribution. The identity link function is typically used, making this case equivalent to classical linear regression.\n\nUse Case: Modeling continuous data where residuals are normally distributed.\nLink Function: Identity, \\(g(\\mu) = \\mu\\).\nDistribution: Normal.\n\nLogistic Regression. Logistic regression is used for binary response variables. It employs the logit link function to model the probability that an observation falls into one of two categories.\n\nUse Case: Binary outcomes (e.g., success/failure).\nLink Function: Logit, \\(g(\\mu) = \\log\\frac{\\mu}{1-\\mu}\\).\nDistribution: Binomial.\n\nPoisson Regression. Poisson regression models count data using the Poisson distribution. It’s ideal for modeling the rate at which events occur.\n\nUse Case: Count data, such as the number of occurrences of an event.\nLink Function: Log, \\(g(\\mu) = \\log(\\mu)\\)\nDistribution: Poisson.\n\nGamma Regression. Gamma regression is suited for modeling positive continuous variables, especially when data are skewed and variance increases with the mean.\n\nUse Case: Positive continuous outcomes with non-constant variance.\nLink Function: Inverse \\(g(\\mu) = \\frac{1}{\\mu}\\).\nDistribution: Gamma.\n\n\nEach GLM variant addresses specific types of data and research questions, enabling precise modeling and inference based on the underlying data distribution. Prediction will need the inverse link function which transforms the linear predictor to the expectation of the outcome.\nTo demonstrate the validation of logistic regression models, we first create a simulated dataset with binary outcomes. This setup involves generating logistic probabilities and then drawing binary outcomes based on these probabilities.\n\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\n\n# Set seed for reproducibility\nnp.random.seed(42)\n\n# Create a DataFrame with random features named `simdat`\nsimdat = pd.DataFrame(np.random.randn(1000, 5), columns=['x1', 'x2', 'x3', 'x4', 'x5'])\n\n# Calculating the linear combination of inputs plus an intercept\neta = simdat.dot([2, 2, 2, 0, 0]) - 5\n\n# Applying the logistic function to get probabilities using statsmodels' logit link\np = sm.families.links.Logit().inverse(eta)\n\n# Generating binary outcomes based on these probabilities and adding them to `simdat`\nsimdat['yb'] = np.random.binomial(1, p, p.size)\n\n# Display the first few rows of the dataframe\nprint(simdat.head())\n\n         x1        x2        x3        x4        x5  yb\n0  0.496714 -0.138264  0.647689  1.523030 -0.234153   0\n1 -0.234137  1.579213  0.767435 -0.469474  0.542560   0\n2 -0.463418 -0.465730  0.241962 -1.913280 -1.724918   0\n3 -0.562288 -1.012831  0.314247 -0.908024 -1.412304   0\n4  1.465649 -0.225776  0.067528 -1.424748 -0.544383   0\n\n\nFit a logistic regression for y1b with the formula interface.\n\nimport statsmodels.formula.api as smf\n\n# Specify the model formula\nformula = 'yb ~ x1 + x2 + x3 + x4 + x5'\n\n# Fit the logistic regression model using glm and a formula\nfit = smf.glm(formula=formula, data=simdat, family=sm.families.Binomial()).fit()\n\n# Print the summary of the model\nprint(fit.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                     yb   No. Observations:                 1000\nModel:                            GLM   Df Residuals:                      994\nModel Family:                Binomial   Df Model:                            5\nLink Function:                  Logit   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -136.44\nDate:                Mon, 24 Feb 2025   Deviance:                       272.89\nTime:                        11:56:16   Pearson chi2:                 1.09e+03\nNo. Iterations:                     8   Pseudo R-squ. (CS):             0.2793\nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     -5.4564      0.453    -12.049      0.000      -6.344      -4.569\nx1             2.1544      0.244      8.822      0.000       1.676       2.633\nx2             2.0781      0.225      9.234      0.000       1.637       2.519\nx3             1.9260      0.237      8.125      0.000       1.461       2.391\nx4            -0.1085      0.166     -0.652      0.514      -0.434       0.217\nx5             0.2672      0.158      1.695      0.090      -0.042       0.576\n==============================================================================",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Statistical Tests and Models</span>"
    ]
  },
  {
    "objectID": "stats.html#interpreting-logistic-regression-results",
    "href": "stats.html#interpreting-logistic-regression-results",
    "title": "6  Statistical Tests and Models",
    "section": "6.3 Interpreting Logistic Regression Results",
    "text": "6.3 Interpreting Logistic Regression Results\nOnce a logistic regression model is fitted, interpreting its results is crucial for understanding how predictor variables influence the probability of the outcome. Logistic regression models the log-odds of the response variable as a linear function of the predictor variables. To ease the intrepretation, consider a logistic model with a single binary predictor (e.g., treatment indicator):\n\\[\n\\log\\left(\\frac{\\mu}{1 - \\mu}\\right) = \\beta_0 + \\beta_1 X\n\\]\nwhere \\(\\mu = E(Y \\mid X)\\) represents the probability of the positive class, and \\(\\beta_1\\) is the estimated coefficient for the binary predictor \\(X\\).\n\n6.3.1 Interpreting Coefficients\nIf \\(X\\) is a binary variable (e.g., 0 for “No” and 1 for “Yes”), \\(\\beta_1\\) represents the difference in log-odds between the two groups. Exponentiating \\(\\beta_1\\) gives the odds ratio:\n\\[\n\\text{Odds Ratio} = \\frac{\\exp(\\beta_0 + \\beta_1)}{\\exp(\\beta)} = e^{\\beta_1}.\n\\]\n\nIf \\(e^{\\beta_1} &gt; 1\\), the outcome is more likely when \\(X = 1\\) than when \\(X = 0\\).\nIf \\(e^{\\beta_1} &lt; 1\\), the outcome is less likely when \\(X = 1\\).\nIf \\(e^{\\beta_1} = 1\\), there is no effect of \\(X\\) on the odds of the outcome.\n\nEquivalently, \\(\\beta_1\\) is the log odds ratio between the two groups.\nWhen there are multiple predictors, the intrepretation needs to state that all the other predictors are unchanged.\nHow would you intreprete the coefficient of a continuous predictor?\n\n\n6.3.2 Probabilistic Interpretation\nWe can transform the linear predictor into a probability estimate using the inverse logit function:\n\\[\n\\Pr(Y=1 | X) = \\frac{e^{\\beta_0 + \\beta_1 X}}{1 + e^{\\beta_0 + \\beta_1 X}}.\n\\]\nThis allows for a direct interpretation of how being in one category of \\(X\\) influences the predicted probability of the outcome. By construction, this value is always in \\((0, 1)\\).\n\n\n6.3.3 Evaluating Statistical Significance\nThe significance of \\(\\beta_1\\) is assessed using standard errors and p-values:\n\nA small p-value (e.g., &lt; 0.05) suggests that \\(X\\) has a statistically significant effect on the outcome.\nConfidence intervals for \\(e^{\\beta_1}\\) help understand the precision of odds ratio estimates.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Statistical Tests and Models</span>"
    ]
  },
  {
    "objectID": "stats.html#validating-the-results-of-logistic-regression",
    "href": "stats.html#validating-the-results-of-logistic-regression",
    "title": "6  Statistical Tests and Models",
    "section": "6.4 Validating the Results of Logistic Regression",
    "text": "6.4 Validating the Results of Logistic Regression\nValidating the performance of logistic regression models is crucial to assess their effectiveness and reliability. This section explores key metrics used to evaluate the performance of logistic regression models, starting with the confusion matrix, then moving on to accuracy, precision, recall, F1 score, and the area under the ROC curve (AUC). Using simulated data, we will demonstrate how to calculate and interpret these metrics using Python.\n\n6.4.1 Confusion Matrix\nThe confusion matrix is a fundamental tool used for calculating several other classification metrics. It is a table used to describe the performance of a classification model on a set of data for which the true values are known. The matrix displays the actual values against the predicted values, providing insight into the number of correct and incorrect predictions.\n\n\n\nActual\nPredicted Positive\nPredicted Negative\n\n\n\n\nActual Positive\nTrue Positive (TP)\nFalse Negative (FN)\n\n\nActual Negative\nFalse Positive (FP)\nTrue Negative (TN)\n\n\n\nFour entries in the confusion matrix:\n\nTrue Positive (TP): The cases in which the model correctly predicted the positive class.\nFalse Positive (FP): The cases in which the model incorrectly predicted the positive class (i.e., the model predicted positive, but the actual class was negative).\nTrue Negative (TN): The cases in which the model correctly predicted the negative class.\nFalse Negative (FN): The cases in which the model incorrectly predicted the negative class (i.e., the model predicted negative, but the actual class was positive).\n\nFour rates from the confusion matrix with actual (row) margins:\n\nTrue positive rate (TPR): TP / (TP + FN). Also known as sensitivity.\nFalse negative rate (FNR): FN / (TP + FN). Also known as miss rate.\nFalse positive rate (FPR): FP / (FP + TN). Also known as false alarm, fall-out.\nTrue negative rate (TNR): TN / (FP + TN). Also known as specificity.\n\nNote that TPR and FPR do not add up to one. Neither do FNR and FPR.\n\nPositive predictive value (PPV): TP / (TP + FP). Also known as precision.\nFalse discovery rate (FDR): FP / (TP + FP).\nFalse omission rate (FOR): FN / (FN + TN).\nNegative predictive value (NPV): TN / (FN + TN).\n\nNote that PPV and NP do not add up to one.\n\n\n6.4.2 Accuracy\nAccuracy measures the overall correctness of the model and is defined as the ratio of correct predictions (both positive and negative) to the total number of cases examined.\n  Accuracy = (TP + TN) / (TP + TN + FP + FN)\n\nImbalanced Classes: Accuracy can be misleading if there is a significant imbalance between the classes. For instance, in a dataset where 95% of the samples are of one class, a model that naively predicts the majority class for all instances will still achieve 95% accuracy, which does not reflect true predictive performance.\nMisleading Interpretations: High overall accuracy might hide the fact that the model is performing poorly on a smaller, yet important, segment of the data.\n\n\n\n6.4.3 Precision\nPrecision (or PPV) measures the accuracy of positive predictions. It quantifies the number of correct positive predictions made.\n  Precision = TP / (TP + FP)\n\nNeglect of False Negatives: Precision focuses solely on the positive class predictions. It does not take into account false negatives (instances where the actual class is positive but predicted as negative). This can be problematic in cases like disease screening where missing a positive case (disease present) could be dangerous.\nNot a Standalone Metric: High precision alone does not indicate good model performance, especially if recall is low. This situation could mean the model is too conservative in predicting positives, thus missing out on a significant number of true positive instances.\n\n\n\n6.4.4 Recall\nRecall (Sensitivity or TPR) measures the ability of a model to find all relevant cases (all actual positives).\n  Recall = TP / (TP + FN)\n\nNeglect of False Positives: Recall does not consider false positives (instances where the actual class is negative but predicted as positive). High recall can be achieved at the expense of precision, leading to a large number of false positives which can be costly or undesirable in certain contexts, such as in spam detection.\nTrade-off with Precision: Often, increasing recall decreases precision. This trade-off needs to be managed carefully, especially in contexts where both false positives and false negatives carry significant costs or risks.\n\n\n\n6.4.5 F-beta Score\nThe F-beta score is a weighted harmonic mean of precision and recall, taking into account a \\(\\beta\\) parameter such that recall is considered \\(\\beta\\) times as important as precision: \\[\n(1 + \\beta^2) \\frac{\\text{precision} \\cdot \\text{recall}}\n{\\beta^2 \\text{precision} + \\text{recall}}.\n\\]\nSee stackexchange post for the motivation of \\(\\beta^2\\) instead of just \\(\\beta\\).\nThe F-beta score reaches its best value at 1 (perfect precision and recall) and worst at 0.\nIf reducing false negatives is more important (as might be the case in medical diagnostics where missing a positive diagnosis could be critical), you might choose a beta value greater than 1. If reducing false positives is more important (as in spam detection, where incorrectly classifying an email as spam could be inconvenient), a beta value less than 1 might be appropriate.\nThe F1 Score is a specific case of the F-beta score where beta is 1, giving equal weight to precision and recall. It is the harmonic mean of Precision and Recall and is a useful measure when you seek a balance between Precision and Recall and there is an uneven class distribution (large number of actual negatives).\n\n\n6.4.6 Receiver Operating Characteristic (ROC) Curve\nThe Receiver Operating Characteristic (ROC) curve is a plot that illustrates the diagnostic ability of a binary classifier as its discrimination threshold is varied. It shows the trade-off between the TPR and FPR. The ROC plots TPR against FPR as the decision threshold is varied. It can be particularly useful in evaluating the performance of classifiers when the class distribution is imbalanced,\n\nIncreasing from \\((0, 0)\\) to \\((1, 1)\\).\nBest classification passes \\((0, 1)\\).\nClassification by random guess gives the 45-degree line.\nArea between the ROC and the 45-degree line is the Gini coefficient, a measure of inequality.\nArea under the curve (AUC) of ROC thus provides an important metric of classification results.\n\nThe Area Under the ROC Curve (AUC) is a scalar value that summarizes the performance of a classifier. It measures the total area underneath the ROC curve, providing a single metric to compare models. The value of AUC ranges from 0 to 1:\n\nAUC = 1: A perfect classifier, which perfectly separates positive and negative classes.\nAUC = 0.5: A classifier that performs no better than random chance.\nAUC &lt; 0.5: A classifier performing worse than random.\n\nThe AUC value provides insight into the model’s ability to discriminate between positive and negative classes across all possible threshold values.\n\n\n6.4.7 Demonstration\nLet’s apply these metrics to the simdat dataset to understand their practical implications. We will fit a logistic regression model, make predictions, and then compute accuracy, precision, and recall.\n\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, confusion_matrix,\n    f1_score, roc_curve, auc\n)\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_classification\n\n# Generate synthetic data\nX, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n\n# Fit the logistic regression model\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\n\n# Predict labels on the test set\ny_pred = model.predict(X_test)\n\n# Get predicted probabilities for ROC curve and AUC\ny_scores = model.predict_proba(X_test)[:, 1]  # Probability for the positive class\n\n# Compute confusion matrix\ncm = confusion_matrix(y_test, y_pred)\n\n# Calculate accuracy, precision, and recall\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\n\n# Print confusion matrix and metrics\nprint(\"Confusion Matrix:\\n\", cm)\nprint(f\"Accuracy: {accuracy:.2f}\")\nprint(f\"Precision: {precision:.2f}\")\nprint(f\"Recall: {recall:.2f}\")\n\nConfusion Matrix:\n [[104  11]\n [ 26 109]]\nAccuracy: 0.85\nPrecision: 0.91\nRecall: 0.81\n\n\nBy varying threshold, one can plot the whole ROC curve.\n\n# Compute ROC curve and AUC\nfpr, tpr, thresholds = roc_curve(y_test, y_scores)\nroc_auc = auc(fpr, tpr)\n\n# Print AUC\nprint(f\"AUC: {roc_auc:.2f}\")\n\n# Plot ROC curve\nplt.figure()\nplt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')  # Diagonal line (random classifier)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC) Curve')\nplt.legend(loc=\"lower right\")\nplt.show()\n\nAUC: 0.92\n\n\n\n\n\n\n\n\n\nWe could pick the best threshold that optmizes F1-score/\n\n# Compute F1 score for each threshold\nf1_scores = []\nfor thresh in thresholds:\n    y_pred_thresh = (y_scores &gt;= thresh).astype(int)  # Apply threshold to get binary predictions\n    f1 = f1_score(y_test, y_pred_thresh)\n    f1_scores.append(f1)\n\n# Find the best threshold (the one that maximizes F1 score)\nbest_thresh = thresholds[np.argmax(f1_scores)]\nbest_f1 = max(f1_scores)\n\n# Print the best threshold and corresponding F1 score\nprint(f\"Best threshold: {best_thresh:.4f}\")\nprint(f\"Best F1 score: {best_f1:.2f}\")\n\nBest threshold: 0.3960\nBest F1 score: 0.89",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Statistical Tests and Models</span>"
    ]
  },
  {
    "objectID": "stats.html#lasso-logistic-models",
    "href": "stats.html#lasso-logistic-models",
    "title": "6  Statistical Tests and Models",
    "section": "6.5 LASSO Logistic Models",
    "text": "6.5 LASSO Logistic Models\nThe Least Absolute Shrinkage and Selection Operator (LASSO) (Tibshirani, 1996), is a regression method that performs both variable selection and regularization. LASSO imposes an L1 penalty on the regression coefficients, which has the effect of shrinking some coefficients exactly to zero. This results in simpler, more interpretable models, especially in situations where the number of predictors exceeds the number of observations.\n\n6.5.1 Theoretical Formulation of the Problem\nThe objective function for LASSO logistic regression can be expressed as,\n\\[\n\\min_{\\beta}\n\\left\\{ -\\frac{1}{n} \\sum_{i=1}^n \\left[ y_i \\log(\\hat{p}_i) + (1 - y_i) \\log(1 - \\hat{p}_i) \\right] + \\lambda \\sum_{j=1}^p |\\beta_j| \\right\\}\n\\]\nwhere:\n\n\\(\\hat{p}_i = \\frac{1}{1 + e^{-X_i\\beta}}\\) is the predicted probability for the \\(i\\)-th sample.\n\\(y_i\\) represents the actual class label (binary: 0 or 1).\n\\(X_i\\) is the feature vector for the \\(i\\)-th observation.\n\\(\\beta\\) is the vector of model coefficients (including the intercept).\n\\(\\lambda\\) is the regularization parameter that controls the trade-off between model fit and sparsity (higher \\(\\lambda\\)) encourages sparsity by shrinking more coefficients to zero).\n\nThe lasso penalty encourages the sum of the absolute values of the coefficients to be small, effectively shrinking some coefficients to zero. This results in sparser solutions, simplifying the model and reducing variance without substantial increase in bias.\nPractical benefits of LASSO:\n\nDimensionality Reduction: LASSO is particularly useful when the number of features \\(p\\) is large, potentially even larger than the number of observations \\(n\\), as it automatically reduces the number of features.\nPreventing Overfitting: The L1 penalty helps prevent overfitting by constraining the model, especially when \\(p\\) is large or there is multicollinearity among features.\nInterpretability: By selecting only the most important features, LASSO makes the resulting model more interpretable, which is valuable in fields like bioinformatics, economics, and social sciences.\n\n\n\n6.5.2 Solution Path\nTo illustrate the effect of the lasso penalty in logistic regression, we can plot the solution path of the coefficients as a function of the regularization parameter \\(\\lambda\\). This demonstration will use a simulated dataset to show how increasing \\(\\lambda\\) leads to more coefficients being set to zero.\n\nimport numpy as np\nfrom sklearn.datasets import make_classification\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\n\n# Step 1: Generate a classification dataset\nX, y = make_classification(n_samples=100, n_features=20, n_informative=2,\n                               random_state=42)\n\n# Step 2: Get a lambda grid given length of lambda and min_ratio of lambda_max\ndef get_lambda_l1(xs: np.ndarray, y: np.ndarray, nlambda: int, min_ratio: float):\n    ybar = np.mean(y)\n    xbar = np.mean(xs, axis=0)\n    xs_centered = xs - xbar\n    xty = np.dot(xs_centered.T, (y - ybar))\n    lmax = np.max(np.abs(xty))\n    lambdas = np.logspace(np.log10(lmax), np.log10(min_ratio * lmax),\n                              num=nlambda)\n    return lambdas\n\n# Step 3: Calculate lambda values\nnlambda = 100\nmin_ratio = 0.01\nlambda_values = get_lambda_l1(X, y, nlambda, min_ratio)\n\n# Step 4: Standardize the features\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Step 5: Initialize arrays to store the coefficients for each lambda value\ncoefficients = []\n\n# Step 6: Fit logistic regression with L1 regularization (Lasso) for each lambda value\nfor lam in lambda_values:\n    model = LogisticRegression(penalty='l1', solver='liblinear', C=1/lam, max_iter=1000)\n    model.fit(X_scaled, y)\n    coefficients.append(model.coef_.flatten())\n\n# Convert coefficients list to a NumPy array for plotting\ncoefficients = np.array(coefficients)\n\n# Step 7: Plot the solution path for each feature\nplt.figure(figsize=(10, 6))\nfor i in range(coefficients.shape[1]):\n    plt.plot(lambda_values, coefficients[:, i], label=f'Feature {i + 1}')\n    \nplt.xscale('log')\nplt.xlabel('Lambda values (log scale)')\nplt.ylabel('Coefficient value')\nplt.title('Solution Path of Logistic Lasso Regression')\nplt.grid(True)\nplt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n6.5.3 Selection the Tuning Parameter\nIn logistic regression with LASSO regularization, selecting the optimal value of the regularization parameter \\(C\\) (the inverse of \\(\\lambda\\)) is crucial to balancing the model’s bias and variance. A small \\(C\\) value (large \\(\\lambda\\)) increases the regularization effect, shrinking more coefficients to zero and simplifying the model. Conversely, a large \\(C\\) (small \\(\\lambda\\)) allows the model to fit the data more closely.\nThe best way to select the optimal \\(C\\) is through cross-validation. In cross-validation, the dataset is split into several folds, and the model is trained on some folds while evaluated on the remaining fold. This process is repeated for each fold, and the results are averaged to ensure the model generalizes well to unseen data. The \\(C\\) value that results in the best performance is selected.\nThe performance metric used in cross-validation can vary based on the task. Common metrics include:\n\nLog-loss: Measures how well the predicted probabilities match the actual outcomes.\nAccuracy: Measures the proportion of correctly classified instances.\nF1-Score: Balances precision and recall, especially useful for imbalanced classes.\nAUC-ROC: Evaluates how well the model discriminates between the positive and negative classes.\n\nIn Python, the LogisticRegressionCV class from scikit-learn automates cross-validation for logistic regression. It evaluates the model’s performance for a range of \\(C\\) values and selects the best one.\n\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import make_classification\nfrom sklearn.metrics import accuracy_score\n\n# Generate synthetic data\nX, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n\n# Initialize LogisticRegressionCV with L1 penalty for Lasso and cross-validation\nlog_reg_cv = LogisticRegressionCV(\n    Cs=np.logspace(-4, 4, 20),  # Range of C values (inverse of lambda)\n    cv=5,                       # 5-fold cross-validation\n    penalty='l1',               # Lasso regularization (L1 penalty)\n    solver='liblinear',         # Solver for L1 regularization\n    scoring='accuracy',         # Optimize for accuracy\n    max_iter=10000              # Ensure convergence\n)\n\n# Train the model with cross-validation\nlog_reg_cv.fit(X_train, y_train)\n\n# Best C value (inverse of lambda)\nprint(f\"Best C value: {log_reg_cv.C_[0]}\")\n\n# Evaluate the model on the test set\ny_pred = log_reg_cv.predict(X_test)\ntest_accuracy = accuracy_score(y_test, y_pred)\nprint(f\"Test Accuracy: {test_accuracy:.2f}\")\n\n# Display the coefficients of the best model\nprint(\"Model Coefficients:\\n\", log_reg_cv.coef_)\n\nBest C value: 0.08858667904100823\nTest Accuracy: 0.86\nModel Coefficients:\n [[ 0.          0.          0.05552448  0.          0.          1.90889734\n   0.          0.          0.          0.          0.0096863   0.23541942\n   0.          0.         -0.0268928   0.          0.          0.\n   0.          0.        ]]\n\n\n\n\n6.5.4 Preparing for Logistic Regression Fitting\nThe LogisticRegression() function in scikit.learn takes the design matrix of the regression as input, which needs to be prepared with care from the covariates or features that we have.\n\n6.5.4.1 Continuous Variables\nFor continuous variables, it is often desirable to standardized them so that they have mean zero and standard deviation one. There are multiple advantages of doing so. It improves numerical stability in algorithms like logistic regression that rely on gradient descent, ensuring faster convergence and preventing features with large scales from dominating the optimization process. Standardization also enhances the interpretability of model coefficients by allowing for direct comparison of the effects of different features, as coefficients then represent the change in outcome for a one standard deviation increase in each variable. Additionally, it ensures that regularization techniques like Lasso and Ridge treat all features equally, allowing the model to select the most relevant ones without being biased by feature magnitude.\nMoreover, standardization is essential for distance-based models such as k-Nearest Neighbors (k-NN) and Support Vector Machines (SVMs), where differences in feature scale can distort the calculations. It also prevents models from being sensitive to arbitrary changes in the units of measurement, improving robustness and consistency. Finally, standardization facilitates better visualizations and diagnostics by putting all variables on a comparable scale, making patterns and residuals easier to interpret. Overall, it is a simple yet powerful preprocessing step that leads to better model performance and interpretability.\nWe have already seen this with StandardScaler.\n\n\n6.5.4.2 Categorical Variables\nCategorical variables can be classified into two types: nominal and ordinal. Nominal variables represent categories with no inherent order or ranking between them. Examples include variables like “gender” (male, female) or “color” (red, blue, green), where the categories are simply labels and one category does not carry more significance than another. Ordinal variables, on the other hand, represent categories with a meaningful order or ranking. For example, education levels such as “high school,” “bachelor,” “master,” and “PhD” have a clear hierarchy, where each level is ranked higher than the previous one. However, the differences between the ranks are not necessarily uniform or quantifiable, making ordinal variables distinct from numerical variables. Understanding the distinction between nominal and ordinal variables is important when deciding how to encode and interpret them in statistical models.\nCategorical variables needs to be coded into numrical values before further processing. In Python, nominal and ordinal variables are typically encoded differently to account for their unique properties. Nominal variables, which have no inherent order, are often encoded using One-Hot Encoding, where each category is transformed into a binary column (0 or 1). For example, the OneHotEncoder from scikit-learn can be used to convert a “color” variable with categories like “red,” “blue,” and “green” into separate columns color_red, color_blue, and color_green, with only one column being 1 for each observation. On the other hand, ordinal variables, which have a meaningful order, are best encoded using Ordinal Encoding. This method assigns an integer to each category based on their rank. For example, an “education” variable with categories “high school,” “bachelor,” “master,” and “PhD” can be encoded as 0, 1, 2, and 3, respectively. The OrdinalEncoder from scikit-learn can be used to implement this encoding, which ensures that the model respects the order of the categories during analysis.\n\n\n6.5.4.3 An Example\nHere is a demo with pipeline using a simulated dataset.\nFirst we generate data with sample size 1000 from a logistic model with both categorical and numerical covariates.\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nimport numpy as np\nfrom scipy.special import expit  # Sigmoid function\n\n# Generate a dataset with the specified size\ndataset_size = 1000\nnp.random.seed(20241014)\n\n# Simulate categorical and numerical features\ngender = np.random.choice(\n    ['male', 'female'], size=dataset_size)  # Nominal variable\neducation = np.random.choice(\n    ['high_school', 'bachelor', 'master', 'phd'], size=dataset_size)  # Ordinal variable\nage = np.random.randint(18, 65, size=dataset_size)\nincome = np.random.randint(30000, 120000, size=dataset_size)\n\n# Create a logistic relationship between the features and the outcome\ngender_num = np.where(gender == 'male', 0, 1)\n\n# Define the linear predictor with regression coefficients\nlinear_combination = (\n    0.3 * gender_num - 0.02 * age + 0.00002 * income\n)\n\n# Apply sigmoid function to get probabilities\nprobabilities = expit(linear_combination)\n\n# Generate binary outcome based on the probabilities\noutcome = np.random.binomial(1, probabilities)\n\n# Create a DataFrame\ndata = pd.DataFrame({\n    'gender': gender,\n    'education': education,\n    'age': age,\n    'income': income,\n    'outcome': outcome\n})\n\nNext we split the data into features and target and define transformers for each types of feature columns.\n\n# Split the dataset into features (X) and target (y)\nX = data[['gender', 'education', 'age', 'income']]\ny = data['outcome']\n\n# Define categorical and numerical columns\ncategorical_cols = ['gender', 'education']  \nnumerical_cols = ['age', 'income']\n\n# Define transformations for categorical variable\ncategorical_transformer = OneHotEncoder(\n    categories=[['male', 'female'], ['high_school', 'bachelor', 'master', 'phd']],\n    drop='first')\n\n# Define transformations for continuous variables\nnumerical_transformer = StandardScaler()\n\n# Use ColumnTransformer to transform the columns\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('cat', categorical_transformer, categorical_cols),\n        ('num', numerical_transformer, numerical_cols)\n    ]\n)\n\nDefine a pipeline, which preprocess the data and then fits a logistic model.\n\npipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('classifier', LogisticRegression(penalty='l1', solver='liblinear',\n    max_iter=1000))\n])\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=2024)\n\n# Fit the pipeline to the training data\npipeline.fit(X_train, y_train)\n\nPipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('cat',\n                                                  OneHotEncoder(categories=[['male',\n                                                                             'female'],\n                                                                            ['high_school',\n                                                                             'bachelor',\n                                                                             'master',\n                                                                             'phd']],\n                                                                drop='first'),\n                                                  ['gender', 'education']),\n                                                 ('num', StandardScaler(),\n                                                  ['age', 'income'])])),\n                ('classifier',\n                 LogisticRegression(max_iter=1000, penalty='l1',\n                                    solver='liblinear'))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFittedPipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('cat',\n                                                  OneHotEncoder(categories=[['male',\n                                                                             'female'],\n                                                                            ['high_school',\n                                                                             'bachelor',\n                                                                             'master',\n                                                                             'phd']],\n                                                                drop='first'),\n                                                  ['gender', 'education']),\n                                                 ('num', StandardScaler(),\n                                                  ['age', 'income'])])),\n                ('classifier',\n                 LogisticRegression(max_iter=1000, penalty='l1',\n                                    solver='liblinear'))]) preprocessor: ColumnTransformer?Documentation for preprocessor: ColumnTransformerColumnTransformer(transformers=[('cat',\n                                 OneHotEncoder(categories=[['male', 'female'],\n                                                           ['high_school',\n                                                            'bachelor',\n                                                            'master', 'phd']],\n                                               drop='first'),\n                                 ['gender', 'education']),\n                                ('num', StandardScaler(), ['age', 'income'])]) cat['gender', 'education'] OneHotEncoder?Documentation for OneHotEncoderOneHotEncoder(categories=[['male', 'female'],\n                          ['high_school', 'bachelor', 'master', 'phd']],\n              drop='first') num['age', 'income'] StandardScaler?Documentation for StandardScalerStandardScaler() LogisticRegression?Documentation for LogisticRegressionLogisticRegression(max_iter=1000, penalty='l1', solver='liblinear') \n\n\nCheck the coefficients of the fitted logistic regression model.\n\nmodel = pipeline.named_steps['classifier']\nintercept = model.intercept_\ncoefficients = model.coef_\n\n# Check the preprocessor's encoding\nencoded_columns = pipeline.named_steps['preprocessor']\\\n.transformers_[0][1].get_feature_names_out(categorical_cols)\n\n# Show intercept, coefficients, and encoded feature names\nintercept, coefficients, list(encoded_columns)\n\n(array([0.66748582]),\n array([[ 0.30568894,  0.10069842,  0.12087311,  0.22576774, -0.24749201,\n          0.55828424]]),\n ['gender_female', 'education_bachelor', 'education_master', 'education_phd'])\n\n\nNote that the encoded columns has one for gender and three for education, with male and high_school as reference levels, respectively. The reference level was determined when calling oneHotEncoder() with drop = 'first'. If categories were not specified, the first level in alphabetical order would be dropped. With the default drop = 'none', the estimated coefficients will have two columns that are not estimable and were set to zero. Obviously, if no level were dropped in forming the model matrix, the columns of the one hot encoding for each categorical variable would be perfectly linearly dependent because they would sum to one.\nThe regression coefficients returned by the logistic regression model in this case should be interpreted on the standardized scale of the numerical covariates (e.g., age and income). This is because we applied standardization to the numerical features using StandardScaler in the pipeline before fitting the model. For example, the coefficient for age would reflect the change in the log-odds of the outcome for a 1 standard deviation increase in age, rather than a 1-unit increase in years. The coefficients for the one-hot encoded categorical variables (gender and education) are on the original scale because one-hot encoding does not change the scale of the variables. For instance, the coefficient for gender_female tells us how much the log-odds of the outcome changes when the observation is male versus the reference category (male).\n\n\n\n\nTibshirani, R. (1996). Regression shrinkage and selection via the LASSO. Journal of the Royal Statistical Society: Series B (Methodological), 58(1), 267–288.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Statistical Tests and Models</span>"
    ]
  },
  {
    "objectID": "exercises.html",
    "href": "exercises.html",
    "title": "7  Exercises",
    "section": "",
    "text": "Quarto and Git setup Quarto and Git are two important tools for data science. Get familiar with them through the following tasks. Please use the templates/hw.qmd template to document, for each step, what you did, the obstacles you encountered, and how you overcame them. Think of this as a user manual for students who are new to this. Use the command line interface.\n\nSet up SSH authentication between your computer and your GitHub account.\nInstall Quarto onto your computer following the instructions of Get Started.\nPick a tool of your choice (e.g., VS Code, Jupyter Notebook, Emacs, etc.), follow the instructions to reproduce the example of line plot on polar axis.\nRender the homework into a pdf file and put the file into a release in your GitHub repo.\n\nWorking on Homework Problems All the requirements on homework styles have reasons. Reviewing these questions help you to understand them.\n\nWhat are the differences between binary and source files?\nWhy do we not want to track binary files in a repo?\nWhy do I require pdf output via release?\nWhy do I not want your files added via ‘upload’?\nWhy do I require line width under 80?\nWhy is it not a good idea to have spaces in file/folder names?\n\nContributing to the Class Notes To contribute to the classnotes, you need to have a working copy of the sources on your computer. Document the following steps in a qmd file in the form of a step-by-step manual, as if you are explaining them to someone who wants to contribute too. Make at least 10 commits for this task, each with an informative message.\n\nCreate a fork of the notes repo into your own GitHub account.\nClone it to an appropriate folder on your computer.\nRender the classnotes on your computer; document the obstacles and solutions.\nMake a new branch (and name it appropriately) to experiment with your changes.\nCheckout your branch and add your wishes to the wish list; commit with an informative message; and push the changes to your GitHub account.\nMake a pull request to class notes repo from your fork at GitHub. Make sure you have clear messages to document the changes.\n\nMonty Hall Consider a generalized Monty Hall experiment. Suppose that the game start with \\(n\\) doors; after you pick one, the host opens \\(m \\le n - 2\\) doors, that show no award. Include sufficient text around the code chunks to explain them.\n\nWrite a function to simulate the experiment once. The function takes two arguments ndoors and nempty, which represent the number of doors and the number of empty doors showed by the host, respectively, It returns the result of two strategies, switch and no-switch, from playing this game.\nPlay this game with 3 doors and 1 empty a few times.\nPlay this game with 10 doors and 8 empty a few times.\nWrite a function to demonstrate the Monty Hall problem through simulation. The function takes three arguments ndoors, nempty, and ntrials, where ntrial is the number of trials in a simulation. The function should return the proportion of wins for both the switch and no-switch strategy.\nApply your function with 3 doors (1 empty) and 10 doors (8 empty), both with 1000 trials. Summarize your results.\n\nApproximating \\(\\pi\\) Write a function to do a Monte Carlo approximation of \\(\\pi\\). The function takes a Monte Carlo sample size n as input, and returns a point estimate of \\(\\pi\\) and a 95% confidence interval. Apply your function with sample size 1000, 2000, 4000, and 8000. Repeat the experiment 1000 times for each sample size and check the empirical probability that the confidence intervals cover the true value of \\(\\pi\\). Comment on the results.\nGoogle Billboard Ad Find the first 10-digit prime number occurring in consecutive digits of \\(e\\). This was a Google recruiting ad.\nGame 24 The math game 24 is one of the addictive games among number lovers. With four randomly selected cards from a deck of poker cards, use all four values and elementary arithmetic operations (\\(+-\\times /\\)) to come up with 24. Let \\(\\square\\) be one of the four numbers. Let \\(\\bigcirc\\) represent one of the four operators. For example, \\[\\begin{equation*}\n(\\square \\bigcirc \\square) \\bigcirc (\\square \\bigcirc \\square)\n\\end{equation*}\\] is one way to group the the operations.\n\nList all the possible ways to group the four numbers.\nHow many possible ways are there to check for a solution?\nWrite a function to solve the problem in a brutal force way. The inputs of the function are four numbers. The function returns a list of solutions. Some of the solutions will be equivalent, but let us not worry about that for now.\n\nNYC Crash Data Cleaning The NYC motor vehicle collisions data with documentation is available from NYC Open Data. The raw data needs some cleaning.\n\nUse the filter from the website to download the crash data of the week of June 30, 2024 in CSV format; save it under a directory data with an informative name (e.g., nyccrashes_2024w0630_by20240916.csv); read the data into a Panda data frame with careful handling of the date time variables.\nClean up the variable names. Use lower cases and replace spaces with underscores.\nCheck the crash date and time to see if they really match the filter we intented. Remove the extra rows if needed.\nGet the basic summaries of each variables: missing percentage; descriptive statistics for continuous variables; frequency tables for discrete variables.\nAre their invalid longitude and latitude in the data? If so, replace them with NA.\nAre there zip_code values that are not legit NYC zip codes? If so, replace them with NA.\nAre there missing in zip_code and borough? Do they always co-occur?\nAre there cases where zip_code and borough are missing but the geo codes are not missing? If so, fill in zip_code and borough using the geo codes.\nIs it redundant to keep both location and the longitude/latitude at the NYC Open Data server?\nCheck the frequency of crash_time by hour. Is there a matter of bad luck at exactly midnight? How would you interpret this?\nAre the number of persons killed/injured the summation of the numbers of pedestrians, cyclist, and motorists killed/injured? If so, is it redundant to keep these two columns at the NYC Open Data server?\nPrint the whole frequency table of contributing_factor_vehicle_1. Convert lower cases to uppercases and check the frequencies again.\nProvided an opportunity to meet the data provider, what suggestions would you make based on your data exploration experience?\n\nNYC Crash Data Exploration Except for the first question, use the cleaned crash data in feather format.\n\nConstruct a contigency table for missing in geocode (latitude and longitude) by borough. Is the missing pattern the same across boroughs? Formulate a hypothesis and test it.\nConstruct a hour variable with integer values from 0 to 23. Plot the histogram of the number of crashes by hour. Plot it by borough.\nOverlay the locations of the crashes on a map of NYC. The map could be a static map or a Google map.\nCreate a new variable severe which is one if the number of persons injured or deaths is 1 or more; and zero otherwise. Construct a cross table for severe versus borough. Is the severity of the crashes the same across boroughs? Test the null hypothesis that the two variables are not associated with an appropriate test.\nMerge the crash data with the Census zip code database which contains zip-code level demographic or socioeconomic variables.\nFit a logistic model with severe as the outcome variable and covariates that are available in the data or can be engineered from the data. For example, zip code level covariates obtained from merging with the zip code database; crash hour; number of vehicles involved.\n\nNYC Crash severity modeling Using the cleaned NYC crash data, merged with zipcode level information, predict severe of a crash.\n\nSet random seed to 1234. Randomly select 20% of the crashes as testing data and leave the rest 80% as training data.\nFit a logistic model on the training data and validate the performance on the testing data. Explain the confusion matrix result from the testing data. Compute the F1 score.\nFit a logistic model on the training data with \\(L_1\\) regularization. Select the tuning parameter with 5-fold cross-validation in F1 score\nApply the regularized logistic regression to predict the severity of the crashes in the testing data. Compare the performance of the two logistic models in terms of accuracy, precision, recall, F1-score, and AUC.\n\nMidterm project: Street flood in NYC The NYC Open Data of 311 Service Requests contains all service requests from 2010 to the present. This analysis focuses on two sewer-related complaints in 2024: Street Flooding (SF) and Catch Basin (CB). SF complaints serve as a practical indicator of street flooding, while CB complaints provide insights into a key infrastructural factor—when catch basins fail to drain rainwater properly due to blockages or structural issues, water accumulates on the streets. SF complaints are typically filed when residents observe standing water or flooding, whereas CB complaints report clogged basins, defective grates, or other drainage problems. The dataset is available in CSV format as data/nycflood2024.csv. Refer to the online data dictionary for a detailed explanation of variable meanings. Try to tell a story in your report while going through the questions.\n\nData cleaning.\n\nImport the data, rename the columns with our preferred styles.\nSummarize the missing information. Are there variables that are close to completely missing?\nAre there redundant information in the data? Try storing the data using the Arrow format and comment on the efficiency gain.\nAre there invalid NYC zipcode or borough? Can some of the missing values be filled? Fill them if yes.\nAre there date errors? Examples are earlier closed_date than created_date; closed_date and created_date matching to the second; dates exactly at midnight or noon to the second.\nSummarize your suggestions to the data curator in several bullet points.\n\nExploratory analysis.\n\nVisualize the locations of complaints on a NYC map, with different symbols for different descriptors.\nCreate a variable response_time, which is the duration from created_date to closed_date.\nVisualize the comparison of response time by complaint descriptor and borough. The original may not be the best given the long tail or outlers.\nIs there significant difference in response time between SF and CB complaints? Across different boroughs? Does the difference between SF and CB depend on borough? State your hypothesis, justify your test, and summarize your results in plain English.\nCreate a binary variable over3d to indicate that a service request took three days or longer to close.\nDoes over3d depend on the complaint descriptor, borough, or weekday (vs weekend/holiday)? State your hypotheses, justify your test, and summarize your results.\n\nModeling the occurrence of overly long response time.\n\nCreate a data set which contains the outcome variable over3d and variables that might be useful in predicting it. Consider including time-of-day effects (e.g., rush hour vs. late-night), seasonal trends, and neighborhood-level demographics. Zip code level information could be useful too, such as the zip code area and the ACS 2023 variables (data/nyc_zip_areas.feather and data/acs2023.feather).\nRandomly select 20% of the complaints as testing data with seeds 1234. Build a logistic model to predict over3d for the complaints with the training data. If you have tuning parameters, justify how they were selected.\nConstruct the confusion matrix from your prediction with a threshold of 1/2 on both training and testing data. Explain your accuracy, recall, precision, and F1 score to a New Yorker.\nConstruct the ROC curve of your fitted logistic model and obtain the AUROC for both training and testing data. Explain your results to a New Yorker.\nIdentify the most important predictors of over3d. Use model coefficients or feature importance (e.g., odds ratios, standardized coefficients, or SHAP values).\nSummarize your results to a New Yorker who is not data science savvy in several bullet points.\n\nModeling the count of SF complains by zip code.\n\nCreate a data set by aggregate the count of SF and SB complains by day for each zipcode.\nMerge the NYC precipitation (data/rainfall_CP.csv), by day to this data set.\nMerge the NYC zip code level landscape variables (data/nyc_zip_lands.csv) and ACS 2023 variables into the data set.\nFor each day, create two variables representing 1-day lag of the precipitation and the number of CB complaints.\nFilter data from March 1 to November 30, excluding winter months when flooding is less frequent. November 30.\nCompare a Poisson regression with a Negative Binomial regression to account for overdispersion. Which model fits better? Explain the results to a New Yorker.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Exercises</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Agonafir, C., Lakhankar, T., Khanbilvardi, R., Krakauer, N., Radell, D.,\n& Devineni, N. (2022). A machine learning approach to evaluate the\nspatial variability of New York\nCity’s 311 street flooding complaints. Computers,\nEnvironment and Urban Systems, 97, 101854.\n\n\nAgonafir, C., Pabon, A. R., Lakhankar, T., Khanbilvardi, R., &\nDevineni, N. (2022). Understanding New York\nCity street flooding through 311 complaints. Journal of\nHydrology, 605, 127300.\n\n\n(ASA), A. S. A. (2018). Ethical guidelines for statistical\npractice.\n\n\nComputing Machinery (ACM), A. for. (2018). Code of ethics and\nprofessional conduct.\n\n\nCone, M. (2025). Markdown cheat sheet | markdown guide. https://www.markdownguide.org/cheat-sheet/\n\n\nCongress, U. S. (1990). Americans with disabilities act of 1990\n(ADA).\n\n\nDervieux, C. (2025). Markdown-basics. https://quarto.org/docs/authoring/markdown-basics.html\n\n\nHealth, U. S. D. of, & Services, H. (1996). Health insurance\nportability and accountability act of 1996 (HIPAA).\n\n\nMacFarlane, J. (2006). Pandoc user’s guide. https://pandoc.org/MANUAL.html#pandocs-markdown\n\n\nMacFarlane, J. (2019). GitHub flavored markdown spec. https://github.github.com/gfm/\n\n\nProtection of Human Subjects of Biomedical, N. C. for the, &\nResearch, B. (1979). The belmont report: Ethical principles and\nguidelines for the protection of human subjects of research.\n\n\nTeam, F. D. S. D. (2019). Federal data strategy 2020 action\nplan.\n\n\nTibshirani, R. (1996). Regression shrinkage and selection via the\nLASSO. Journal of the Royal Statistical Society: Series\nB (Methodological), 58(1), 267–288.\n\n\nVanderPlas, J. (2016). Python data science handbook:\nEssential tools for working with data. O’Reilly Media,\nInc.",
    "crumbs": [
      "References"
    ]
  }
]