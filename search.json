[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Data Science",
    "section": "",
    "text": "Preliminaries\nThe notes were developed with Quarto; for details about Quarto, visit https://quarto.org/docs/books.\nThis book free and is licensed under a Creative Commons Attribution-NonCommercial-NoDerivs 3.0 United States License.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#sources-at-github",
    "href": "index.html#sources-at-github",
    "title": "Introduction to Data Science",
    "section": "Sources at GitHub",
    "text": "Sources at GitHub\nThese lecture notes for STAT 3255/5255 in Spring 2025 represent a collaborative effort between Professor Jun Yan and the students enrolled in the course. This cooperative approach to education was facilitated through the use of GitHub, a platform that encourages collaborative coding and content development. To view these contributions and the lecture notes in their entirety, please visit our GitHub repository at https://github.com/statds/ids-s25.\nStudents contributed to the lecture notes by submitting pull requests to our GitHub repository. This method not only enriched the course material but also provided students with practical experience in collaborative software development and version control.\nFor those interested, class notes from Fall 2024, Spring 2024, Spring 2023, and Spring 2022 are also publicly accessible. These archives offer insights into the evolution of the course content and the different perspectives brought by successive student cohorts.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#compiling-the-classnotes",
    "href": "index.html#compiling-the-classnotes",
    "title": "Introduction to Data Science",
    "section": "Compiling the Classnotes",
    "text": "Compiling the Classnotes\nTo reproduce the classnotes output on your own computer, here are the necessary steps. See Section 3.2 Compiling the Classnotes for details.\n\nClone the classnotes repository to an appropriate location on your computer; see Chapter 2  Project Management for using Git.\nSet up a Python virtual environment in the root folder of the source; see Section 4.7 Virtual Environment.\nActivate your virtual environment.\nInstall all the packages specified in requirements.txt in your virtual environment:\n\npip install -r requirements.txt\n\nFor some chapters that need to interact with certain sites that require account information. For example, for Google map services, you need to save your API key in a file named api_key.txt in the root folder of the source.\nRender the book with quarto render from the root folder on a terminal; the rendered book will be stored under _book.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#midterm-project",
    "href": "index.html#midterm-project",
    "title": "Introduction to Data Science",
    "section": "Midterm Project",
    "text": "Midterm Project\nReproduce NYC street flood research (Agonafir, Lakhankar, et al., 2022; Agonafir, Pabon, et al., 2022).\nFour students will be selected to present their work in a workshop at the 2025 NYC Open Data Week. You are welcome to invite your family and friends to join the the workshop.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#final-project",
    "href": "index.html#final-project",
    "title": "Introduction to Data Science",
    "section": "Final Project",
    "text": "Final Project\nStudents are encouraged to start designing their final projects from the beginning of the semester. There are many open data that can be used. Here is a list of data challenges that you may find useful:\n\nASA Data Challenge Expo: big data in 2025\nKaggle.\nDrivenData.\n15 Data Science Hackathons to Test Your Skills in 2025\nIf you work on sports analytics, you are welcome to submit a poster to Connecticut Sports Analytics Symposium (CSAS) 2025.\nA good resource for sports analytics is ScoreNetwork.\nPaleobiology Database.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#adapting-to-rapid-skill-acquisition",
    "href": "index.html#adapting-to-rapid-skill-acquisition",
    "title": "Introduction to Data Science",
    "section": "Adapting to Rapid Skill Acquisition",
    "text": "Adapting to Rapid Skill Acquisition\nIn this course, students are expected to rapidly acquire new skills, a critical aspect of data science. To emphasize this, consider this insightful quote from VanderPlas (2016):\n\nWhen a technologically-minded person is asked to help a friend, family member, or colleague with a computer problem, most of the time it’s less a matter of knowing the answer as much as knowing how to quickly find an unknown answer. In data science it’s the same: searchable web resources such as online documentation, mailing-list threads, and StackOverflow answers contain a wealth of information, even (especially?) if it is a topic you’ve found yourself searching before. Being an effective practitioner of data science is less about memorizing the tool or command you should use for every possible situation, and more about learning to effectively find the information you don’t know, whether through a web search engine or another means.\n\nThis quote captures the essence of what we aim to develop in our students: the ability to swiftly navigate and utilize the vast resources available to solve complex problems in data science. Examples tasks are: install needed software (or even hardware); search and find solutions to encountered problems.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#wishlist",
    "href": "index.html#wishlist",
    "title": "Introduction to Data Science",
    "section": "Wishlist",
    "text": "Wishlist\nThis is a wish list from all members of the class (alphabetical order, last name first, comma, then first name). Here is an example.\n\nYan, Jun\n\nMake practical data science tools accessible to undergraduates.\nPass real-world data science project experience to students.\nCo-develop a Quarto book in collaboration with the students.\nTrain students to participate in real data science competitions.\n\n\nAdd yours through a pull request; note the syntax of nested list in Markdown.\n\nStudents in 3255\n\nAckerman, John\n\nGet comfortable with command line interface\nHands-on experience with AI\nLearn practical tools & tricks for professional data scientist\n\nAlsadadi, Ammar Shaker\n\nLearn about the applications of Data Science in Finance\nLearn more about time series and random walk\n\nChen, Yifei\n\nLearn more advanced python programming skills.\nLearn to use github for future projects\nGet a good grade in this class.\n\nEl Zein, Amer Hani\n\nTo gain a deeper undestanding of data preparation.\nTo develop intution on what the best tool for a given project is.\n\nFebles, Xavier Milan\n\nFurther develop skills with git\nLearn more about specific tools used for data science\nBecome more comfortable with sql\n\nHorn, Alyssa Noelle\n\nBe confident in using Git and Github\nLearn how to collaborate with others on projects through Github\n\nJun, Joann\n\nBecome proficient in using GitHub\nLearn more about the applications of data science\n\nKline, Daniel Esteban\nLagutin, Vladislav\n\nLearn how to do data science projects in python and interact with them using git\nLearn how to do good visualizations of the data; explore appropriate libraries\n\nLang, Lang\n\nBecome more proficient with python\nLearn about the applications of Data Science\nLearn how to make collaborative project by using GitHub\nHave a good grade in this course\n\nLi, Shiyi\n\nLearn to visualize the plots and results using the ggplot package.\nLearn to use the common functions of the SciPy, scikit-learn, and statsmodels libraries in Python\nLearn how to query, extract, and manipulate structured and unstructured data in a large database.\nLearn the basics of artificial neural networks, CNNs for image data, NLP techniques.\nLearn some of the data analysis models that will be commonly used in the workplace.\nLearn some common applications of optimization techniques in data analysis.\nPass this course with an A grade.\n\nLin, Selena\n\nGet a good grade in this class.\nLearn and get familier with using GitHub.\nHands on experience with the data science skills learned in this class.\n\nLong, Ethan Kenneth\n\nBecome more comfortable using Git commands and CLI\nLearn more about the data science field\nUnderstand proper coding grammar\nDevelop good learning habits\n\nNasejje, Ruth Nicole\n\nDevelop an organized coding style in python, quarto, & git\nLearn various packages in python related to data science\nDeepen knowledge in statistical modeling and data analysis\n\nPfeifer, Nicholas Theodore\n\nLearn about data science techniques in python\nLearn and thoroughly practice using git and github\nGet more comfortable with decision trees and random forests\n\nReed, Kyle Daniel\n\nGain full confidence using Git/GitHub and corresponding applications.\nUnderstand the workflow in professional data science projects.\nBuild on existing python skills.\n\nRoy, Luke William\n\nHave fun\nDevelop skills in financial data analysis using python and relevant libraries like pandas and numpy.\nLearn advanced data visualization techniques with a focus on the grammar of graphics.\nGet an introduction to machine learning via scikit-learn, and explore applications in financial analysis and forensic accounting.\n\nSchittina, Thomas\n\nBecome more comfortable using git and GitHub\nBecome more familiar with popular data science packages in Python\n\nSymula, Sebastian\n\nLearn SQL\nBecome better at working through each step in the data science pipeline to make better, cleaner looking projects\n\nTamhane, Shubhan\n\nLearn intersection between SQL and Python for a data science project\nLearn machine learning algorithms like random forest and clustering\n\nTomaino, Mario Anthony\nXu, Peiwen\n\nLearn some data analysis techniques\nLearn how to use git and other essential tools for data science\n\n\n\n\nStudents in 5255\n\nEdo, Mezmur Wossenu\n\nI hope to become adept working with github.\nI hope to work on real-World data science projects.\nI hope to learn about the different machine learning techniques.\n\nMundiwala, Mohammad Moiz\n\nBecome more familiar with collaboration process of programming so that I can be more orderly while working with others.\nI hope to become more efficient processing data that is messy, unstructured, or unlabeled.\nPresent engaging, intuitive, and interactive figures and animations for complex math and stat concepts.\n\nVellore, Ajeeth Krishna\n\nUnderstand the utility provided by GitHub and practice using its tools\nLearn how to participate in a large-scale development project like how they are done in industry\nLearn how to code properly and professionally instead of using “backyard” computer science techniques and formatting\nUnderstand principles of coding documentation and readability while practicing their application\n\nZhang, Gaofei\n\nGain confidence in using Git and GitHub for version control and collaboration.\nDevelop a structured approach to data cleaning and preprocessing for complex datasets.\nEnhance skills in statistical modeling and machine learning techniques relevant to public health research.\nImprove efficiency in working with large-scale data using Python and SQL.\n\nKravette, Noah\n\nBecome better at program collaboration.\nBecome adept with git and github.\nBe able to quickly and efficently process and analyze any data.\nGain better skills at data prep, organization, and visulization.\nLearn new helpful statistical tools for data.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#course-logistics",
    "href": "index.html#course-logistics",
    "title": "Introduction to Data Science",
    "section": "Course Logistics",
    "text": "Course Logistics\n\nPresentation Orders\nThe topic presentation order is set up in class.\n\nwith open('rosters/3255.txt', 'r') as file:\n    ug = [line.strip() for line in file]\nwith open('rosters/5255.txt', 'r') as file:\n    gr = [line.strip() for line in file]\npresenters = ug + gr\n\nimport random\n## seed jointly set by the class\nrandom.seed(6895 + 283 + 3184 + 3078 + 5901 + 36)\nrandom.sample(presenters, len(presenters))\n## random.shuffle(presenters) # This would shuffle the list in place\n\n['Li,Shiyi',\n 'Jun,Joann',\n 'Alsadadi,Ammar Shaker',\n 'Lang,Lang',\n 'Ackerman,John',\n 'Horn,Alyssa Noelle',\n 'Xu,Peiwen',\n 'Schittina,Thomas',\n 'Kline,Daniel Esteban',\n 'Edo,Mezmur Wossenu',\n 'Roy,Luke William',\n 'Febles,Xavier Milan',\n 'Tamhane,Shubhan',\n 'Nasejje,Ruth Nicole',\n 'Lagutin,Vladislav',\n 'Zhang,Gaofei',\n 'Long,Ethan Kenneth',\n 'El Zein,Amer Hani',\n 'Kravette,Noah',\n 'Symula,Sebastian',\n 'Tomaino,Mario Anthony',\n 'Reed,Kyle Daniel',\n 'Chen,Yifei',\n 'Mundiwala,Mohammad Moiz',\n 'Lin,Selena',\n 'Pfeifer,Nicholas Theodore',\n 'Vellore,Ajeeth Krishna']\n\n\nSwitching slots is allowed as long as you find someone who is willing to switch with you. In this case, make a pull request to switch the order and let me know.\nYou are welcome to choose a topic that you are interested the most, subject to some order restrictions. For example, decision tree should be presented before random forest or extreme gradient boosting. This justifies certain requests for switching slots.\n\n\nPresentation Task Board\nTalk to the professor about your topics at least one week prior to your scheduled presentation. Here are some example tasks:\n\nMaking presentations with Quarto\nMarkdown jumpstart\nEffective data science communication\nImport/Export data\nData manipulation with Pandas\nAccessing US census data\nArrow as a cross-platform data format\nStatistical analysis for proportions and rates\nDatabase operation with Structured query language (SQL)\nGrammer of graphics\nHandling spatial data\nSpatial data with GeoPandas\nVisualize spatial data in a Google map\nAnimation\nSupport vector machine\nRandom forest\nGradient boosting machine\nNaive Bayes\nNeural networks\nDeep learning\nTensorFlow\nAutoencoders\nK-means clustering\nPrincipal component analysis\nReinforcement learning\nDeveloping a Python package\nWeb scraping\nPersonal webpage on GitHub\n\n\n\nTopic Presentation Schedule\nThe topic presentation is 20 points. It includes:\n\nTopic selection consultation on week in advance (4 points).\nDelivering the presentation in class (10 points).\nContribute to the class notes within two weeks following the presentation (6 points).\n\nTips on topic contribution:\n\nNo plagiarism (see instructions on Contributing to Class Notes).\nAvoid external graphics.\nUse simulated data.\nUse data from homework assignments.\nCite article/book references (learn how from our sources).\nInclude a subsection of Further Readings.\nTest on your own computer before making a pull request.\nSend me your presentation two days in advance for feedbacks.\n\nPlease use the following table to sign up.\n\n\n\n\n\n\n\n\nDate\nPresenter\nTopic\n\n\n\n\n02/10\nLi, Shiyi\nA Primer of Markdown\n\n\n02/12\nJun, Joann\nMaking Presentations with Quarto\n\n\n02/17\nRoy, Luke William\nGrammar of Graphics with Plotnine\n\n\n02/19\nLang, Lang\nData manipulation with Pandas\n\n\n02/24\nAckerman, John\nPerforming Statistical Tests (SciPy)\n\n\n02/26\nHorn, Alyssa Noelle\nDatabase operation with Structured query language (SQL)\n\n\n03/03\nEl Zein, Amer Hani\nEffective Communication in Data Science\n\n\n03/05\nSchittina, Thomas\nSpatial Data With Geopandas & Google Maps\n\n\n03/10\nKline, Daniel Esteban\n\n\n\n03/12\nEdo, Mezmur Wossenu\nPrincipal Component Analysis (PCA)\n\n\n03/26\nAlsadadi, Ammar Shaker\nSentiment Analysis with Python\n\n\n03/26\nFables, Xavier Milan\nVariable Importance Metrics in Supervised Learning\n\n\n03/31\nTamhane, Shubhan\nSupport Vector Machine\n\n\n03/31\nNasejje, Ruth Nicole\nPersonal webpage on GitHub\n\n\n04/02\nLagutin, Vladislav\nGoogle Maps visualizations using Folium library\n\n\n04/02\nZhang, Gaofei\nRandom forest\n\n\n04/07\nLong, Ethan Kenneth\nSynthetic Minority Oversampling Technique (SMOTE)\n\n\n04/07\nXu, Peiwen\nDeveloping a Python package\n\n\n04/09\nKravette, Noah\nWorking with NetCDF Data\n\n\n04/09\nSymula, Sebastian\nImputation Methods for Missing Data\n\n\n04/09\nTomaino, Mario Anthony\nK-Prototypes Clustering\n\n\n04/14\nReed, Kyle Daniel\nAutoencoders\n\n\n04/14\nChen, Yifei\n\n\n\n04/14\nMundiwala, Mohammad Moiz\nMath animations with manim\n\n\n04/16\nLin, Selena\nNaive Bayes\n\n\n04/16\nPfeifer, Nicholas Theodore\nChoosing the number of clusters in clustering\n\n\n04/16\nVellore, Ajeeth Krishna\nNeural Network Basics\n\n\n\n\n\nFinal Project Presentation Schedule\nWe use the same order as the topic presentation for undergraduate final presentation. An introduction on how to use Quarto to prepare presentation slides is availabe under the templates directory in the classnotes source tree, thank to Zachary Blanchard, which can be used as a template to start with.\n\n\n\n\n\n\n\nDate\nPresenter\n\n\n\n\n04/21\nShiyi Li; Joann Jun; Ammar Alsadadi; Lang Lang\n\n\n04/23\nJohn Ackerman; Alyssa Horn; Peiwen Xu; Thomas Schittina\n\n\n04/28\nDaniel Kline; Luke Roy; Xavier Fables; Shubhan Tamhane\n\n\n04/30\nRuth Nasejje; Vladislav Lagutin; Ethan Long; Amer El Zein\n\n\n05/05 (10:30-12:30)\nSebastian Symula; Mario Tomiano; Kyle Reed; Yifei Chen; Selena Lin; Nick Pfeifer\n\n\n\n\n\nContributing to the Class Notes\nContribution to the class notes is through a `pull request’.\n\nStart a new branch and switch to the new branch.\nOn the new branch, add a qmd file for your presentation\nIf using Python, create and activate a virtual environment with requirements.txt\nEdit _quarto.yml add a line for your qmd file to include it in the notes.\nWork on your qmd file, test with quarto render.\nWhen satisfied, commit and make a pull request with your quarto files and an updated requirements.txt.\n\nI have added a template file mysection.qmd and a new line to _quarto.yml as an example.\nFor more detailed style guidance, please see my notes on statistical writing.\nPlagiarism is to be prevented. Remember that these class notes are publicly available online with your names attached. Here are some resources on how to avoid plagiarism. In particular, in our course, one convenient way to avoid plagiarism is to use our own data (e.g., NYC Open Data). Combined with your own explanation of the code chunks, it would be hard to plagiarize.\n\n\nHomework Logistics\n\nWorkflow of Submitting Homework Assisngment\n\nClick the GitHub classroom assignment link in HuskCT announcement.\nAccept the assignment and follow the instructions to an empty repository.\nMake a clone of the repo at an appropriate folder on your own computer with git clone.\nGo to this folder, add your qmd source, work on it, and group your changes to different commits.\nPush your work to your GitHub repo with git push.\nCreate a new release and put the generated pdf file in it for ease of grading.\n\n\n\nRequirements\n\nUse the repo from Git Classroom to submit your work. See Chapter 2  Project Management.\n\nKeep the repo clean (no tracking generated files).\n\nNever “Upload” your files; use the git command lines.\nMake commit message informative (think about the readers).\n\nMake at least 10 commits and form a style of frequent small commits.\n\nTrack quarto sources only in your repo. See Chapter 3  Reproducible Data Science.\nFor the convenience of grading, add your standalone html or pdf output to a release in your repo.\nFor standalone pdf output, you will need to have LaTeX installed.\n\n\n\n\nQuizzes about Syllabus\n\nDo I accept late homework?\nCould you list a few examples of email etiquette?\nHow would you lose style points?\nWould you use CLI and GUI?\nHow many students will present at 2025 NYC ODW and when will the presentations be?\nWhat’s the first date on which you have to complete something about your final project?\nCan you use AI for any task in this course?\nAnybody needs a reference letter? How could you help me to help you?",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#practical-tips",
    "href": "index.html#practical-tips",
    "title": "Introduction to Data Science",
    "section": "Practical Tips",
    "text": "Practical Tips\n\nData analysis\n\nUse an IDE so you can play with the data interactively\nCollect codes that have tested out into a script for batch processing\nDuring data cleaning, keep in mind how each variable will be used later\nNo keeping large data files in a repo; assume a reasonable location with your collaborators\n\n\n\nPresentation\n\nDon’t forget to introduce yourself if there is no moderator.\nHighlight your research questions and results, not code.\nGive an outline, carry it out, and summarize.\nUse your own examples to reduce the risk of plagiarism.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#my-presentation-topic-template",
    "href": "index.html#my-presentation-topic-template",
    "title": "Introduction to Data Science",
    "section": "My Presentation Topic (Template)",
    "text": "My Presentation Topic (Template)\nThis section was prepared by John Smith.\nUse Markdown syntax. If not clear on what to do, learn from the class notes sources.\n\nPay attention to the sectioning levels.\nCite references with their bib key.\nIn examples, maximize usage of data set that the class is familiar with.\nCould use datasets in Python packages or downloadable on the fly.\nTest your section by quarto render &lt;filename.qmd&gt;.\n\n\nIntroduction\nHere is an overview.\n\n\nSub Topic 1\nPut materials on topic 1 here\nPython examples can be put into python code chunks:\n\n# import pandas as pd\n\n# do something\n\n\n\nSub Topic 2\nPut materials on topic 2 here.\n\n\nSub Topic 3\nPut matreials on topic 3 here.\n\n\nConclusion\nPut sumaries here.\n\n\nFurther Readings\nPut links to further materials.\n\n\n\n\nAgonafir, C., Lakhankar, T., Khanbilvardi, R., Krakauer, N., Radell, D., & Devineni, N. (2022). A machine learning approach to evaluate the spatial variability of New York City’s 311 street flooding complaints. Computers, Environment and Urban Systems, 97, 101854.\n\n\nAgonafir, C., Pabon, A. R., Lakhankar, T., Khanbilvardi, R., & Devineni, N. (2022). Understanding New York City street flooding through 311 complaints. Journal of Hydrology, 605, 127300.\n\n\nVanderPlas, J. (2016). Python data science handbook: Essential tools for working with data. O’Reilly Media, Inc.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 What Is Data Science?\nData science is a multifaceted field, often conceptualized as resting on three fundamental pillars: mathematics/statistics, computer science, and domain-specific knowledge. This framework helps to underscore the interdisciplinary nature of data science, where expertise in one area is often complemented by foundational knowledge in the others.\nA compelling definition was offered by Prof. Bin Yu in her 2014 Presidential Address to the Institute of Mathematical Statistics. She defines \\[\\begin{equation*}\n\\mbox{Data Science} =\n\\mbox{S}\\mbox{D}\\mbox{C}^3,\n\\end{equation*}\\] where\nComputing underscores the need for proficiency in programming and algorithmic thinking, collaboration/teamwork reflects the inherently collaborative nature of data science projects, often requiring teams with diverse skill sets, and communication to outsiders emphasizes the importance of translating complex data insights into understandable and actionable information for non-experts.\nThis definition neatly captures the essence of data science, emphasizing a balance between technical skills, teamwork, and the ability to communicate effectively.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#what-is-data-science",
    "href": "intro.html#what-is-data-science",
    "title": "1  Introduction",
    "section": "",
    "text": "‘S’ represents Statistics, signifying the crucial role of statistical methods in understanding and interpreting data;\n‘D’ stands for domain or science knowledge, indicating the importance of specialized expertise in a particular field of study;\nthe three ’C’s denotes computing, collaboration/teamwork, and communication to outsiders.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#expectations-from-this-course",
    "href": "intro.html#expectations-from-this-course",
    "title": "1  Introduction",
    "section": "1.2 Expectations from This Course",
    "text": "1.2 Expectations from This Course\nIn this course, students will be expected to achieve the following outcomes:\n\nProficiency in Project Management with Git: Develop a solid understanding of Git for efficient and effective project management. This involves mastering version control, branching, and collaboration through this powerful tool.\nProficiency in Project Reporting with Quarto: Gain expertise in using Quarto for professional-grade project reporting. This encompasses creating comprehensive and visually appealing reports that effectively communicate your findings.\nHands-On Experience with Real-World Data Science Projects: Engage in practical data science projects that reflect real-world scenarios. This hands-on approach is designed to provide you with direct experience in tackling actual data science challenges.\nCompetency in Using Python and Its Extensions for Data Science: Build strong skills in Python, focusing on its extensions relevant to data science. This includes libraries like Pandas, NumPy, and Matplotlib, among others, which are critical for data analysis and visualization.\nFull Grasp of the Meaning of Results from Data Science Algorithms: Learn to not only apply data science algorithms but also to deeply understand the implications and meanings of their results. This is crucial for making informed decisions based on these outcomes.\nBasic Understanding of the Principles of Data Science Methods: Acquire a foundational knowledge of the underlying principles of various data science methods. This understanding is key to effectively applying these methods in practice.\nCommitment to the Ethics of Data Science: Emphasize the importance of ethical considerations in data science. This includes understanding data privacy, bias in data and algorithms, and the broader social implications of data science work.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#computing-environment",
    "href": "intro.html#computing-environment",
    "title": "1  Introduction",
    "section": "1.3 Computing Environment",
    "text": "1.3 Computing Environment\nAll setups are operating system dependent. As soon as possible, stay away from Windows. Otherwise, good luck (you will need it).\n\n1.3.1 Operating System\nYour computer has an operating system (OS), which is responsible for managing the software packages on your computer. Each operating system has its own package management system. For example:\n\nLinux: Linux distributions have a variety of package managers depending on the distribution. For instance, Ubuntu uses APT (Advanced Package Tool), Fedora uses DNF (Dandified Yum), and Arch Linux uses Pacman. These package managers are integral to the Linux experience, allowing users to install, update, and manage software packages easily from repositories.\nmacOS: macOS uses Homebrew as its primary package manager. Homebrew simplifies the installation of software and tools that aren’t included in the standard macOS installation, using simple commands in the terminal.\nWindows: Windows users often rely on the Microsoft Store for apps and software. For more developer-focused package management, tools like Chocolatey and Windows Package Manager (Winget) are used. Additionally, recent versions of Windows have introduced the Windows Subsystem for Linux (WSL). WSL allows Windows users to run a Linux environment directly on Windows, unifying Windows and Linux applications and tools. This is particularly useful for developers and data scientists who need to run Linux-specific software or scripts. It saves a lot of trouble Windows users used to have before its time.\n\nUnderstanding the package management system of your operating system is crucial for effectively managing and installing software, especially for data science tools and applications.\n\n\n1.3.2 File System\nA file system is a fundamental aspect of a computer’s operating system, responsible for managing how data is stored and retrieved on a storage device, such as a hard drive, SSD, or USB flash drive. Essentially, it provides a way for the OS and users to organize and keep track of files. Different operating systems typically use different file systems. For instance, NTFS and FAT32 are common in Windows, APFS and HFS+ in macOS, and Ext4 in many Linux distributions. Each file system has its own set of rules for controlling the allocation of space on the drive and the naming, storage, and access of files, which impacts performance, security, and compatibility. Understanding file systems is crucial for tasks such as data recovery, disk partitioning, and managing file permissions, making it an important concept for anyone working with computers, especially in data science and IT fields.\nNavigating through folders in the command line, especially in Unix-like environments such as Linux or macOS, and Windows Subsystem for Linux (WSL), is an essential skill for effective file management. The command cd (change directory) is central to this process. To move into a specific directory, you use cd followed by the directory name, like cd Documents. To go up one level in the directory hierarchy, you use cd ... To return to the home directory, simply typing cd or cd ~ will suffice. The ls command lists all files and folders in the current directory, providing a clear view of your options for navigation. Mastering these commands, along with others like pwd (print working directory), which displays your current directory, equips you with the basics of moving around the file system in the command line, an indispensable skill for a wide range of computing tasks in Unix-like systems.\n\n\n1.3.3 Command Line Interface\nOn Linux or MacOS, simply open a terminal.\nOn Windows, several options can be considered.\n\nWindows Subsystem Linux (WSL): https://learn.microsoft.com/en-us/windows/wsl/\nCygwin (with X): https://x.cygwin.com\nGit Bash: https://www.gitkraken.com/blog/what-is-git-bash\n\nTo jump start, here is a tutorial: Ubunto Linux for beginners.\nAt least, you need to know how to handle files and traverse across directories. The tab completion and introspection supports are very useful.\nHere are several commonly used shell commands:\n\ncd: change directory; .. means parent directory.\npwd: present working directory.\nls: list the content of a folder; -l long version; -a show hidden files; -t ordered by modification time.\nmkdir: create a new directory.\ncp: copy file/folder from a source to a target.\nmv: move file/folder from a source to a target.\nrm: remove a file a folder.\n\n\n\n1.3.4 Python\nSet up Python on your computer:\n\nPython 3.\nPython package manager miniconda or pip.\nIntegrated Development Environment (IDE) (Jupyter Notebook; RStudio; VS Code; Emacs; etc.)\n\nI will be using VS Code in class.\nReadability is important! Check your Python coding styles against the recommended styles: https://peps.python.org/pep-0008/. A good place to start is the Section on “Code Lay-out”.\nOnline books on Python for data science:\n\n“Python Data Science Handbook: Essential Tools for Working with Data,” First Edition, by Jake VanderPlas, O’Reilly Media, 2016.\n\n\n“Python for Data Analysis: Data Wrangling with Pandas, NumPy, and IPython.” Third Edition, by Wes McK- inney, O’Reilly Media, 2022.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#data-science-ethics",
    "href": "intro.html#data-science-ethics",
    "title": "1  Introduction",
    "section": "1.4 Data Science Ethics",
    "text": "1.4 Data Science Ethics\n\n1.4.1 Introduction\nEthics in data science is a fundamental consideration throughout the lifecycle of any project. Data science ethics refers to the principles and practices that guide responsible and fair use of data to ensure that individual rights are respected, societal welfare is prioritized, and harmful outcomes are avoided. Ethical frameworks like the Belmont Report (Protection of Human Subjects of Biomedical & Research, 1979)} and regulations such as the Health Insurance Portability and Accountability Act (HIPAA) (Health & Services, 1996) have established foundational principles that inspire ethical considerations in research and data use. This section explores key principles of ethical data science and provides guidance on implementing these principles in practice.\n\n\n1.4.2 Principles of Ethical Data Science\n\n1.4.2.1 Respect for Privacy\nSafeguarding privacy is critical in data science. Projects should comply with data protection regulations, such as the General Data Protection Regulation (GDPR) or the California Consumer Privacy Act (CCPA). Techniques like anonymization and pseudonymization must be applied to protect sensitive information. Beyond legal compliance, data scientists should consider the ethical implications of using personal data.\nThe principles established by the Belmont Report emphasize respect for persons, which aligns with safeguarding individual privacy. Protecting privacy also involves limiting data collection to what is strictly necessary. Minimizing the use of identifiable information and implementing secure data storage practices are essential steps. Transparency about how data is used further builds trust with stakeholders.\n\n\n1.4.2.2 Commitment to Fairness\nBias can arise at any stage of the data science pipeline, from data collection to algorithm development. Ethical practice requires actively identifying and addressing biases to prevent harm to underrepresented groups. Fairness should guide the design and deployment of models, ensuring equitable treatment across diverse populations.\nTo achieve fairness, data scientists must assess datasets for representativeness and use tools to detect potential biases. Regular evaluation of model outcomes against fairness metrics helps ensure that systems remain non-discriminatory. The Americans with Disabilities Act (ADA) (Congress, 1990) provides a legal framework emphasizing equitable access, which can inspire fairness in algorithmic design. Collaborating with domain experts and stakeholders can provide additional insights into fairness issues.\n\n\n1.4.2.3 Emphasis on Transparency\nTransparency builds trust and accountability in data science. Models should be interpretable, with clear documentation explaining their design, assumptions, and decision-making processes. Data scientists must communicate results in a way that stakeholders can understand, avoiding unnecessary complexity or obfuscation.\nTransparent practices include providing stakeholders access to relevant information about model performance and limitations. The Federal Data Strategy (Team, 2019) calls for transparency in public sector data use, offering inspiration for practices in broader contexts. Visualizing decision pathways and using tools like LIME or SHAP can enhance interpretability. Establishing clear communication protocols ensures that non-technical audiences can engage with the findings effectively.\n\n\n1.4.2.4 Focus on Social Responsibility\nData science projects must align with ethical goals and anticipate their broader societal and environmental impacts. This includes considering how outputs may be used or misused and avoiding harm to vulnerable populations. Data scientists should aim to use their expertise to promote public welfare, addressing critical societal challenges such as health disparities, climate change, and education access.\nEngaging with diverse perspectives helps align projects with societal values. Ethical codes, such as those from the Association for Computing Machinery (ACM) (Computing Machinery (ACM), 2018), offer guidance on using technology for social good. Collaborating with policymakers and community representatives ensures that data-driven initiatives address real needs and avoid unintended consequences. Regular impact assessments help measure whether projects meet their ethical objectives.\n\n\n1.4.2.5 Adherence to Professional Integrity\nProfessional integrity underpins all ethical practices in data science. Adhering to established ethical guidelines, such as those from the American Statistical Association (ASA) ((ASA), 2018), ensures accountability. Practices like maintaining informed consent, avoiding data manipulation, and upholding rigor in analyses are essential for maintaining public trust in the field.\nEthical integrity also involves fostering a culture of honesty and openness within data science teams. Peer review and independent validation of findings can help identify potential errors or biases. Documenting methodologies and maintaining transparency in reporting further strengthen trust.\n\n\n\n1.4.3 Ensuring Ethics in Practice\n\n1.4.3.1 Building Ethical Awareness\nPromoting ethical awareness begins with education and training. Institutions should integrate ethics into data science curricula, emphasizing real-world scenarios and decision-making. Organizations should conduct regular training to ensure their teams remain informed about emerging ethical challenges.\nWorkshops and case studies can help data scientists understand the complexities of ethical decision-making. Providing access to resources, such as ethical guidelines and tools, supports continuous learning. Leadership support is critical for embedding ethics into organizational culture.\n\n\n1.4.3.2 Embedding Ethics in Workflows\nEthics must be embedded into every stage of the data science pipeline. Establishing frameworks for ethical review, such as ethics boards or peer-review processes, helps identify potential issues early. Tools for bias detection, explainability, and privacy protection should be standard components of workflows.\nStandard operating procedures for ethical reviews can formalize the consideration of ethics in project planning. Developing templates for documenting ethical decisions ensures consistency and accountability. Collaboration across teams enhances the ability to address ethical challenges comprehensively.\n\n\n1.4.3.3 Establishing Accountability Mechanisms\nClear accountability mechanisms are essential for ethical governance. This includes maintaining documentation for all decisions, establishing audit trails, and assigning responsibility for the outputs of data-driven systems. Organizations should encourage open dialogue about ethical concerns and support whistleblowers who raise issues.\nPeriodic audits of data science projects help ensure compliance with ethical standards. Organizations can benefit from external reviews to identify blind spots and improve their practices. Accountability fosters trust and aligns teams with ethical objectives.\n\n\n1.4.3.4 Engaging Stakeholders\nEthical data science requires collaboration with diverse stakeholders. Including perspectives from affected communities, policymakers, and interdisciplinary experts ensures that projects address real needs and avoid unintended consequences. Stakeholder engagement fosters trust and aligns projects with societal values.\nPublic consultations and focus groups can provide valuable feedback on the potential impacts of data science projects. Engaging with regulators and advocacy groups helps align projects with legal and ethical expectations. Transparent communication with stakeholders builds long-term relationships.\n\n\n1.4.3.5 Continuous Improvement\nEthics in data science is not static; it evolves with technology and societal expectations. Continuous improvement requires regular review of ethical practices, learning from past projects, and adapting to new challenges. Organizations should foster a culture of reflection and growth to remain aligned with ethical best practices.\nEstablishing mechanisms for feedback on ethical practices can identify areas for development. Sharing lessons learned through conferences and publications helps the broader community advance its understanding of ethics in data science.\n\n\n\n1.4.4 Conclusion\nData science ethics is a dynamic and integral aspect of the discipline. By adhering to principles of privacy, fairness, transparency, social responsibility, and integrity, data scientists can ensure their work contributes positively to society. Implementing these principles through structured workflows, stakeholder engagement, and continuous improvement establishes a foundation for trustworthy and impactful data science.\n\n\n\n\n(ASA), A. S. A. (2018). Ethical guidelines for statistical practice.\n\n\nComputing Machinery (ACM), A. for. (2018). Code of ethics and professional conduct.\n\n\nCongress, U. S. (1990). Americans with disabilities act of 1990 (ADA).\n\n\nHealth, U. S. D. of, & Services, H. (1996). Health insurance portability and accountability act of 1996 (HIPAA).\n\n\nProtection of Human Subjects of Biomedical, N. C. for the, & Research, B. (1979). The belmont report: Ethical principles and guidelines for the protection of human subjects of research.\n\n\nTeam, F. D. S. D. (2019). Federal data strategy 2020 action plan.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "git.html",
    "href": "git.html",
    "title": "2  Project Management",
    "section": "",
    "text": "2.1 Set Up Git/GitHub\nMany tutorials are available in different formats. Here is a YouTube video ``Git and GitHub for Beginners — Crash Course’’. The video also covers GitHub, a cloud service for Git which provides a cloud back up of your work and makes collaboration with co-workers easy. Similar services are, for example, bitbucket and GitLab.\nThere are tools that make learning Git easy.\nDownload Git if you don’t have it already.\nTo set up GitHub (other services like Bitbucket or GitLab are similar), you need to\nSee how to get started with GitHub account.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Management</span>"
    ]
  },
  {
    "objectID": "git.html#set-up-gitgithub",
    "href": "git.html#set-up-gitgithub",
    "title": "2  Project Management",
    "section": "",
    "text": "Generate an SSH key if you don’t have one already.\nSign up an GitHub account.\nAdd the SSH key to your GitHub account",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Management</span>"
    ]
  },
  {
    "objectID": "git.html#most-frequently-used-git-commands",
    "href": "git.html#most-frequently-used-git-commands",
    "title": "2  Project Management",
    "section": "2.2 Most Frequently Used Git Commands",
    "text": "2.2 Most Frequently Used Git Commands\nThe following seven commands will get you started and they may be all that you need most of the time.\n\ngit clone:\n\nUsed to clone a repository to a local folder.\nRequires either HTTPS link or SSH key to authenticate.\n\ngit pull:\n\nDownloads any updates made to the remote repository and automatically updates the local repository.\n\ngit status:\n\nReturns the state of the working directory.\nLists the files that have been modified, and are yet to be or have been staged and/or committed.\nShows if the local repository is begind or ahead a remote branch.\n\ngit add:\n\nAdds new or modified files to the Git staging area.\nGives the option to select which files are to be sent to the remote repository\n\ngit rm:\n\nUsed to remove files from the staging index or the local repository.\n\ngit commit:\n\nCommits changes made to the local repository and saves it like a snapshot.\nA message is recommended with every commit to keep track of changes made.\n\ngit push:\n\nUsed to send commits made on local repository to the remote repository.\n\n\nFor more advanced usages:\n\ngit diff\ngit branch\ngit reset",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Management</span>"
    ]
  },
  {
    "objectID": "git.html#tips-on-using-git",
    "href": "git.html#tips-on-using-git",
    "title": "2  Project Management",
    "section": "2.3 Tips on using Git:",
    "text": "2.3 Tips on using Git:\n\nUse the command line interface instead of the web interface (e.g., upload on GitHub)\nMake frequent small commits instead of rare large commits.\nMake commit messages informative and meaningful.\nName your files/folders by some reasonable convention.\n\nLower cases are better than upper cases.\nNo blanks in file/folder names.\n\nKeep the repo clean by not tracking generated files.\nCreat a .gitignore file for better output from git status.\nKeep the linewidth of sources to under 80 for better git diff view.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Management</span>"
    ]
  },
  {
    "objectID": "git.html#pull-request",
    "href": "git.html#pull-request",
    "title": "2  Project Management",
    "section": "2.4 Pull Request",
    "text": "2.4 Pull Request\nTo contribute to an open source project (e.g., our classnotes), use pull requests. Pull requests “let you tell others about changes you’ve pushed to a branch in a repository on GitHub. Once a pull request is opened, you can discuss and review the potential changes with collaborators and add follow-up commits before your changes are merged into the base branch.”\nWatch this YouTube video: GitHub pull requests in 100 seconds.\nThe following are step-by-step instructions on how to make a pull request to the class notes contributed by Nick Pfeifer..\n\nCreate a fork of the class repository on the GitHub website.\n\nMake sure your fork is up to date by clicking Sync fork if necessary.\n\nClone your fork into a folder on your computer.\n\ngit clone https://github.com/GitHub_Username/ids-s25.git\nReplace GitHub_Username with your personal GitHub Username.\n\nCheck to see if you can access the folder/cloned repository in your code editor.\n\nThe class notes home page is located in the index.qmd file.\n\nMake a branch and give it a good name.\n\nMove into the directory with the cloned repository.\nCreate a branch using:\n\ngit checkout -b branch_name\nReplace branch_name with a more descriptive name.\n\nYou can check your branches using:\n\ngit branch\nThe branch in use will have an asterisk to the left of it.\n\nIf you are not in the right branch you can use the following command:\n\ngit checkout existing-branch\nReplace existing-branch with the name of the branch you want to use.\n\n\nRun git status to verify that no changes have been made.\nMake changes to a file in the class notes repository.\n\nFor example: add your wishes to the Wishlist in index.qmd using nested list syntax in markdown.\nRemember to save your changes.\n\nRun git status again to see that changes have been made.\nUse the add command.\n\ngit add filename\nExample usage: git add index.qmd\n\nMake a commit.\n\ngit commit -m \"Informative Message\"\nBe clear about what you changed and perhaps include your name in the message.\n\nPush the files to GitHub.\n\ngit push origin branch-name\nReplace branch-name with the name of your current branch.\n\nGo to your forked repository on GitHub and refresh the page, you should see a button that says Compare and Pull Request.\n\nDescribe the changes you made in the pull request.\nClick Create pull request.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Management</span>"
    ]
  },
  {
    "objectID": "quarto.html",
    "href": "quarto.html",
    "title": "3  Reproducible Data Science",
    "section": "",
    "text": "3.1 Introduction to Quarto\nData science projects should be reproducible to be trustworthy. Dynamic documents facilitate reproducibility. Quarto is an open-source dynamic document preparation system, ideal for scientific and technical publishing. From the official websites, Quarto can be used to:\nTo get started with Quarto, see documentation at Quarto.\nFor a clean style, I suggest that you use VS Code as your IDE. The ipynb files have extra formats in plain texts, which are not as clean as qmd files. There are, of course, tools to convert between the two representations of a notebook. For example:\nWe will use Quarto for homework assignments, classnotes, and presentations. You will see them in action through in-class demonstrations. The following sections in the Quarto Guide are immediately useful.\nA template for homework is in this repo (hwtemp.qmd) to get you started with homework assignments.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reproducible Data Science</span>"
    ]
  },
  {
    "objectID": "quarto.html#introduction-to-quarto",
    "href": "quarto.html#introduction-to-quarto",
    "title": "3  Reproducible Data Science",
    "section": "",
    "text": "quarto convert hello.ipynb # converts to qmd\nquarto convert hello.qmd   # converts to ipynb\n\n\nMarkdown basics\nUsing Python\nPresentations",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reproducible Data Science</span>"
    ]
  },
  {
    "objectID": "quarto.html#sec-buildnotes",
    "href": "quarto.html#sec-buildnotes",
    "title": "3  Reproducible Data Science",
    "section": "3.2 Compiling the Classnotes",
    "text": "3.2 Compiling the Classnotes\nThe sources of the classnotes are at https://github.com/statds/ids-s25. This is also the source tree that you will contributed to this semester. I expect that you clone the repository to your own computer, update it frequently, and compile the latest version on your computer (reproducibility).\nTo compile the classnotes, you need the following tools: Git, Quarto, and Python.\n\n3.2.1 Set up your Python Virtual Environment\nI suggest that a Python virtual environment for the classnotes be set up in the current directory for reproducibility. A Python virtual environment is simply a directory with a particular file structure, which contains a specific Python interpreter and software libraries and binaries needed to support a project. It allows us to isolate our Python development projects from our system installed Python and other Python environments.\nTo create a Python virtual environment for our classnotes:\npython3 -m venv .ids-s25-venv\nHere .ids-s25-venv is the name of the virtual environment to be created. Choose an informative name. This only needs to be set up once.\nTo activate this virtual environment:\n. .ids-s25-venv/bin/activate\nAfter activating the virtual environment, you will see (.ids-s25-venv) at the beginning of your shell prompt. Then, the Python interpreter and packages needed will be the local versions in this virtual environment without interfering your system-wide installation or other virtual environments.\nTo install the Python packages that are needed to compile the classnotes, we have a requirements.txt file that specifies the packages and their versions. They can be installed easily with:\npip install -r requirements.txt\nIf you are interested in learning how to create the requirements.txt file, just put your question into a Google search.\nTo exit the virtual environment, simply type deactivate in your command line. This will return you to your system’s global Python environment.\n\n\n3.2.2 Clone the Repository\nClone the repository to your own computer. In a terminal (command line), go to an appropriate directory (folder), and clone the repo. For example, if you use ssh for authentication:\ngit clone git@github.com:statds/ids-s25.git\n\n\n3.2.3 Render the Classnotes\nAssuming quarto has been set up, we render the classnotes in the cloned repository\ncd ids-s25\nquarto render\nIf there are error messages, search and find solutions to clear them. Otherwise, the html version of the notes will be available under _book/index.html, which is default location of the output.\n\n\n3.2.4 Login Requirements\nFor some illustrations, you need to interact with certain sites that require account information. For example, for Google map services, you need to save your API key in a file named api_key.txt in the root folder of the source. Another example is to access the US Census API, where you would need to register an account and get your Census API Key.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reproducible Data Science</span>"
    ]
  },
  {
    "objectID": "quarto.html#a-primer-of-markdown",
    "href": "quarto.html#a-primer-of-markdown",
    "title": "3  Reproducible Data Science",
    "section": "3.3 A Primer of Markdown",
    "text": "3.3 A Primer of Markdown\nThis section was prepared by Shiyi Li. I am a senior majoring in Mathematics/ Statistics and minoring in CSE. I aim to graduate from the University of Connecticut in August and continue my master’s in Data Science program.\n\n3.3.1 Introduction\nToday’s presentation, I will introduce some basic syntax on markdown. Markdown is a plain text format, an easy way to write content for a web interface. It is widely used in open-source documentation, including GitHub, R Markdown, Quarto, Jupyter Notebooks, etc. Its syntax is easy to learn, write, read, and seamlessly convert to HTML, PDF, and DOCX formats. I will divide my presentation into three parts, Structuring the Document, Formatting Text, and Enhancing Content.\n\n\n3.3.2 Structuring the Document\nThere are some elements can define sections and hierarchy to help organize content in a clear and structured manner.\n\n3.3.2.1 Headings\n\nTo create heading levels for topics, sections, or subsections, you can add number signs, “#”, before you write any content for them.\n\n\nExample:\n\nYou can code like this:\n\n# Heading 1 (Main Title)\n\n## Heading 2 (Section)\n\n### Heading 3 (Subsection)\n\n#### Heading 4\n\n##### Heading 5\n\n###### Heading 6\n\nBe careful, you need to make sure that there is a blank line before and after your heading levels and a space between your number sign “#” and the heading name.\n\n\nAlternatively, you can also use any number of double equal sign, “==”, or double dashes sign, “–”, to create a heading.\n\n\nExample:\n\nYou can code like this:\n\nHeading 1 (Main Title)\n======\nHeading 2 (Section)\n------\n\nHowever, this method can only be used to create two heading levels like above.\n\n\n\n3.3.2.2 Horizontal Rules - line Seperator\n\nA horizontal rule adds a visual break between parts of text by adding three or more asterisks, “***“, dashes,”—“, or underscores,”___“, on a line by themselves.\nExample:\n\nYou can code like this:\nThis is the part 1.\n\n--------\n\nThis is the part 2.\n\n********\n\nThis is the part 3.\n\n_________\n\nThis is the part 4.\n\nThis code chunk will output like this:\n\nThis is the part 1.\n\nThis is the part 2.\n\nThis is the part 3.\n\nThis is the part 4.\n\nBe careful. You need to make sure you add a blank line before and after your separator line.\n\n\n\n3.3.2.3 Paragraphs\n\nTo create paragraphs, you can add a blank line between two paragraphs just like you usually create paragraphs in an essay.\nExample:\n\nYou can code like this:\nThis is the first paragraph ........................................\n.....................................................................\n.....................................................................\n... end the first paragraph.\n\nThis is the second paragraph .........................................................\n...................................................................\n.....................................................................\n...end the second paragraph.\n\nThis will output like this:\n\nThis is the first paragraph …………………………………. …………………………………………………………… …………………………………………………………… … end the first paragraph.\nThis is the second paragraph ………………………………………………… …………………………………………………………. …………………………………………………………… …end the second paragraph.\n\n\n3.3.2.4 Blockquotes\n\nTo highlight multiple line important text, cite multiple line references, or quote multiple line important points, you can add a greater than sign, “&gt;”, at the begining of each line of the text.\n\n\nExample:\n\nYou can code like this:\n&gt; This is an important text.\n&gt;\n&gt; This is a citation/reference.\n&gt;\n&gt; This is a quotation.\n\nThis code will output like this:\n\n\nThis is an important text.\nThis is a citation/reference.\nThis is a quotation.\n\n\nWrong Example:\n\nIf you code like this:\n&gt; This is an important text.\n&gt; This is a citation/reference.\n&gt; This is a quotation.\n\nIt will output like this:\n\n\nThis is an important text. This is a citation/reference. This is a quotation.\n\nBe sure to add a blank line with a greater-than sign, “&gt;”, otherwise the output will display multiple lines of text in a single line.\n\nYou can also nest blockquotes by adding two or more greater than signs, “&gt;&gt;”.\n\n\nExample:\n\nYou can code like this:\n&gt; This is an important text.\n&gt;\n&gt;&gt; This is a citation/reference.\n&gt;&gt;\n&gt;&gt;&gt; This is a quotation.\n\nThis code will output like this:\n\n\nThis is an important text.\n\nThis is a citation/reference.\n\nThis is a quotation.\n\n\n\nBe sure to add a blank line before the first line of your blockquotes, otherwise, it will not look right.\n\n\n\n3.3.3 Formatting Text\nThere are some elements to make text stand out, improving emphasis and readability like Bold, Italic, and Strikethrough text.\n\n3.3.3.1 Bold\n\nTo bold text, you can add two asterisks, “**“, or underscores,”__“, before and after the text.\nExample:\n\nYou can code like this:\n**This is the first important text.**\n\n__This is the second important text.__\n\nThis is the __third important__ text.\n\nThis is the **fourth important** text.\n\nThis is the f**if**th important text.\n\nThis will output like this:\n\nThis is the first important text.\nThis is the second important text.\nThis is the third important text.\nThis is the fourth important text.\nThis is the fifth important text.\n\nBe careful do not use underscores, “__“, to bold charachers inside a word, like this”This is the fifth im__portan__t text”.***\n\n\n\n3.3.3.2 Italic\n\nTo italicize a text, you can add one asterisk, “*“, or one underscore,”_“, before and after a text.\nExample:\n\nYou can code like this:\n*ThIs Is the fIrst Important TexT.*\n\n_ThIs Is the second Important TexT._\n\nThis is the _Third ImportanT_ text.\n\nThis is the *fourth ImportanT* text.\n\nFifth im*PORtaN*t text.\n\nThis code chunk will output like this:\n\nThIs Is the fIrst Important TexT.\nThIs Is the second Important TexT.\nThis is the Third ImportanT text.\nThis is the fourth ImportanT text.\nFifth imPORtaNt text.\n\nBe careful don’t use underscores, “_“, to italicize charachers inside a word, like this”Fifth im_porta_nt text”.\n\n\n\n3.3.3.3 Bold & Italic\nTo bold and italicize for the same text, you can use three asterisks, “***“, or three underscores,”___“, before and after a text.\n\nExample:\n\nYou can code like this:\n***This is the first important text.***\n\n___This is the second important text.___\n\nThis is the ___third important___ text.\n\nThis is the ***fourth important*** text.\n\nFifth i***mportan***t text.\n\nThis code chunk will output like this:\n\nThis is the first important text.\nThis is the second important text.\nThis is the third important text.\nThis is the fourth important text.\nFifth important text.\n\nBe careful don’t use underscores, “___“, to bold and italicize charachers inside a word, like this”Fifth im___porta___nt text”.\n\n\n\n3.3.3.4 Highlight\n\nTo highlight a text, you can add this sign, “&lt;mark&gt;”, before the text and add this sign, “&lt;/mark&gt;”, after the text.\nExample:\n\nYou can code like this:\n&lt;mark&gt;This is a text that needs to be highlighted.&lt;/mark&gt;\n\nThis is a te&lt;mark&gt;xt that needs to be high&lt;/mark&gt;lighted.\n\nThis code chunk will output like this:\n\nThis is a text that needs to be highlighted.\nThis is a text that needs to be highlighted.\n\n\n3.3.3.5 Strikethrough - Deleted or Removed Text\n\nTo show a deletion or correction on a text, you can add double tilde signs, “~~”, before and after the part of the deletion or correction on your text.\nExample:\n\nYou can code like this:\n~~This text is struck through~~ This is the correct text.\n\nThis code chunk will output like this:\n\nThis text is struck through This is the correct text.\n\n\n\n3.3.4 Enhancing Content\nTo enhance the illustrative capabilities of your content, there are several elements you can add to your document, including subscript, superscript, lists, tables, footnotes, links, images, math notations, etc.\n\n3.3.4.1 Subscript\n\nTo add a subscript before, after or within a number or word, you can add a tilde symbol, “~”, before and after the text you want to subscript.\nExample:\n\nYou can code like this:\nThis is a subscript before and after a word: \n\n~subscript~word~subscript~\n\nThis is a subscript within a word: \n\nWor~111000~ds\n\nThis is a subscript before and after a number: \n\n~7878~11111~7878~ \n\nThis is a subscript within a number: \n\n999~subscript~999\n\nThis code chunk will output like this:\n\nThis is a subscript before and after a word:\nsubscriptwordsubscript\nThis is a subscript within a word:\nWor111000ds\nThis is a subscript before and after a number:\n7878111117878\nThis is a subscript within a number:\n999subscript999\n\nBe sure not to add any spaces or tabs between the two tilde symbols, “~ ~”.\n\n\n\n3.3.4.2 Superscript\n\nTo add a superscript before, after or within a number or word, you can add a caret symbol, “^”, before and after the text you want to superscript.\nExample:\n\nYou can code like this:\n\nThis is a superscript before and after a word:\n\n^787878^Words^787878^\n\nThis is a superscript within a word:\n\nWor^787878^ds\n\nThis is a superscript before and after a number:\n\n^superscript^1111111^superscript^  \n\nThis is a superscript within a number:\n\n999^superscript^999\n\nThis will output like this:\n\nThis is a superscript before and after a word:\n787878Words787878\nThis is a superscript within a word:\nWor787878ds\nThis is a superscript before and after a number:\nsuperscript1111111superscript\nThis is a superscript within a number:\n999superscript999\n\nBe sure not to add any spaces or tabs between the two caret symbols, “…”.\n\n\n\n3.3.4.3 Lists\nTo organize a list (nested list), you can use ordered or unordered numbers or alphabets followed by a period sign, “.”, dashes, “-”, asterisks, “*“, or plus signs,”+“, in front of line items. Markdown is smart, it will automatically detect and organize a list for you.\n1. Using Ordered numbers followed by a period sign:\n\nExample:\n\nYou can code like this:\n\n1. First item\n2. Second item\n    1. Third item\n    2. Fourth item\n3. Fifth item\n\nThis code chunk will output like this:\n\nUsing Ordered numbers followed by a period sign:\n\nFirst item\nSecond item\n\nThird item\nFourth item\n\nFifth item\n\n2. Using Unordered numbers followed by a period sign:\n\nExample:\n\nYou can code like this:\n\n2. First item\n2. Second item\n    2. Third item\n    2. Fourth item\n2. Fifth item\n\n2. First item\n7. Second item\n    9. Third item\n    2. Fourth item\n10. Fifth item\n\nThis code chunk will output like this:\n\nUsing Unordered numbers followed by a period sign:\n\nFirst item\nSecond item\n\nThird item\nFourth item\n\nFifth item\nFirst item\nSecond item\n\nThird item\nFourth item\n\nFifth item\n\n\nBe careful, for an unordered list to work as you want, you need to take care of the first number or letter of the first item of your (nested) list, because markdown will order the list starting with the first number or alphabet of your (nested) list.\n\n3. Using dashes:\n\nExample:\n\nYou can code like this:\n\n- First item\n- Second item\n    - Third item\n    - Fourth item\n- Fifth item\n\nThis code chunk will output like this:\n\nUsing dashes:\n\nFirst item\nSecond item\n\nThird item\nFourth item\n\nFifth item\n\n4. Using asterisks:\n\nExample:\n\nYou can code like this:\n\n* First item\n* Second item\n    * Third item\n    * Fourth item\n* Fifth item\n\nThis code chunk will output like this:\n\nUsing asterisks:\n\nFirst item\nSecond item\n\nThird item\nFourth item\n\nFifth item\n\n5. Using plus signs:\n\nExample:\n\nYou can code like this:\n\n+ First item\n+ Second item\n    + Third item\n    + Fourth item\n+ Fifth item\n\nThis code chunk will output like this:\n\nUsing plus signs:\n\nFirst item\nSecond item\n\nThird item\nFourth item\n\nFifth item\n\n6. Using alphabets:\n\nExample:\n\nYou can code like this:\n\na. First item\nb. Second item\n    a. Third item\n    b. Fourth item\nc. Fifth item\n\nw. First item\na. Second item\n    c. Third item\n    y. Fourth item\na. Fifth item\n\nThis code chunk will output like this:\n\nUsing alphabets:\n\nFirst item\nSecond item\n\nThird item\nFourth item\n\nFifth item\nFirst item\nSecond item\n\nThird item\nFourth item\n\nFifth item\n\n7. Using different delimiters:\n\nExample:\n\nYou can code like this:\n\n+ First item\n- Second item\n    * Third item\n    + Fourth item\n* Fifth item\n\nThis code chunk will output like this:\n\nUsing different delimiters:\n\nFirst item\nSecond item\n\nThird item\nFourth item\n\nFifth item\n\n\nUsing different delimiters in the same list has no effect on the list organized by markdown.\nBe careful, you need to make sure you add a blank line before the list starts.\nYou can also use a backslash, “\", to escape a period,”.”, if you do not want to create a list and still need a period after a number or alphabet.\n\n\n\n3.3.4.4 Task Lists - To-Do List\n\nTo add a task/to-do list, you can add this sign, “- [ ]”, or this sign, “- [x]”, before each item in your task/to-do list.\nExample:\n\nYou can code like this:\n\n- [ ] Task not completed\n- [x] Task completed\n\nThis code chunk will output like this:\nTask not completed\nTask completed\n\n\nIn your rendered HTML file, you can check or uncheck the completion marks in the small box at the front of each task.\n\n\n\n3.3.4.5 Links\n\nYou can add a link in your text by enclosing your added link with parentheses, “()”. In addition, you can also optionally add a name or a short description for the link by enclosing them with brackets, “[]”, before the link and this will appear as a tooltip when the user hovers over the link\n\n\n\n\n\nExample:\n\nYou can code like this:\n\nWe can use the \n[Markdown Cheat Sheet](https://www.markdownguide.org/basic-syntax/#code-blocks) \nto learn more generally used markdown syntax. \n\nThis code chunk will output like this:\n\nWe can use the Markdown Cheat Sheet to learn more generally used markdown syntax.\n\nYou can add an Reference Style Link by enclosing the name or description of the link and a number/word pointed to the link with brackets, “[]”.\n\n\nExample:\n\nYou can code like this:\n\nWe can use the [Markdown Cheat Sheet][4] to learn more generally used \nmarkdown syntax. \n\n[4]: https://www.markdownguide.org/basic-syntax/#code-blocks\n\nThis code chunk will output like this:\n\nWe can use the Markdown Cheat Sheet to learn more generally used markdown syntax.\n\nRemember to start a new line and add the link as a footnote after the number or word pointed to the reference enclosed by brackets, “[]”.\n\n\n\n3.3.4.6 Images\n\nTo add images from your local computer or a website, you can add an exclamation mark, “!”, followed by a description or other text enclosing with brackets, “[]”, and a path/URL to the image enclosing with parentheses, “()”.\nExample:\n\nYou can code like this:\n\n![This is a description to an online image](https://today.uconn.edu/2023/01/\nuconn-on-campus-construction-update-january-2023/)\n\n![This is a description to a local image](/Users/shiyili/\nDesktop/UCONN.jpg)\n\nThe render output will show like this:\n\n\n\n\nThis is a description to an online image.\n\n\n\n\n3.3.4.7 Tables\n\nTo create a table, you can use three or more hyphens, “—”, and pipes, “|”, to create and separate each column respectively.\nExample:\n\nYou can code like this:\n\n1. Each cell with the same width in code chunk:\n\n|            | 1st Column | 2nd Column | 3rd Column | ...... |\n| ---------- | ---------- | ---------- | ---------- | ------ |\n| 1st Row    |    123     |     123    |     123    |   123  |\n| 2nd Row    |    123     |     123    |     123    |   123  |\n| 3rd Row    |    123     |     123    |     123    |   123  |\n| ......     |    123     |     123    |     123    |   123  |\n\n2. Each cell with vary width in code chunk:\n\n|            | 1st Column | 2nd Column | 3rd Column | ...... |\n| ------ | ---------- | ---------- | ---------- | ------ |\n| 1st Row |     123       |      123      |     123       |    123    |\n| 2nd Row      |     123       |     123       |      123      |   123     |\n| 3rd Row  |      123      |      123      |     123       |    123    |\n| ......         |      123      |     123       |      123      |    123    |\n\nThis code chunk will output like this:\n\n\nEach cell with the same width in code chunk:\n\n\n\n\n\n\n\n\n\n\n\n\n1st Column\n2nd Column\n3rd Column\n……\n\n\n\n\n1st Row\n123\n123\n123\n123\n\n\n2nd Row\n123\n123\n123\n123\n\n\n3rd Row\n123\n123\n123\n123\n\n\n……\n123\n123\n123\n123\n\n\n\n\nEach cell with vary width in code chunk:\n\n\n\n\n\n\n\n\n\n\n\n\n1st Column\n2nd Column\n3rd Column\n……\n\n\n\n\n1st Row\n123\n123\n123\n123\n\n\n2nd Row\n123\n123\n123\n123\n\n\n3rd Row\n123\n123\n123\n123\n\n\n……\n123\n123\n123\n123\n\n\n\n\nCell width can vary, because markdown will automatically detect and organize the table for you with the same width.***\n\n\nYou can align text in the columns to the left, right, or center by adding a colon, “:”, to the left, right, or on both side of the hyphens, “—”, within the header (Cone, 2025).\n\n\nExample:\n\nYou can code like this:\n\n|            | 1st Column | 2nd Column | 3rd Column | ...... |\n| :----------: | :---------- | :----------: | ----------: | ------: |\n| 1st Row    |            123|       123     |  123          |        123|\n| 2nd Row    |123            |    123        |           123 |123        |\n| 3rd Row    |     123       |123            |     123       |   123     |\n| ......     |   123         |            123| 123           |123        |\n\nThis code chunk will output like this:\n\n\n\n\n\n\n\n\n\n\n\n\n1st Column\n2nd Column\n3rd Column\n……\n\n\n\n\n1st Row\n123\n123\n123\n123\n\n\n2nd Row\n123\n123\n123\n123\n\n\n3rd Row\n123\n123\n123\n123\n\n\n……\n123\n123\n123\n123\n\n\n\n\n\n3.3.4.8 Code\n\nTo add an inline code, you can add a backtick, “`”, before and after a word or a text.\n\n\nExample:\n\nYou can code like this:\n\nThis is my `inline` code.\n\n`This is my inline code.`\n\nThis will output like this:\n\nThis is my inline code.\nThis is my inline code.\n\nIf you want to display a text with a backtick, “`”“, as an inline code, you can add a double backtick,”``“, before and after the text.\n\n\nExample:\n\nYou can code like this:\n\n``This is a `text` with backticks.``\n\nThis will output like this:\n\nThis is a `text` with backticks.\n\nTo add a code block, you can indent each line of the text with more than four spaces or one tab.\n\n\nExample:\n\nYou can code like this:\n\n    This is a code block.\n\n        This is a code block.\n\n            This is a code block.\n\n    This is a code block.\n\nThis will output like this:\n\n    This is a code block.\n\n        This is a code block.\n\n            This is a code block.\n\n    This is a code block.\n\nTo add a fenced code block, you can add a blank line begining with three backticks, “```”, or three tilde signs, “~~~”, before and after your code blok.\n\n\nExample:\n\nYou can code like this:\n\n    ```\n    {\n    This is my fenced code block.\n    This is my fenced code block.\n    }\n    ```\n\nThis will output like this:\n\n{\nThis is my fenced code block.\nThis is my fenced code block.\n}\n\nTo add a nonexecutive Python code chunk, you can first add a fence code block for your code, then add a word, “python”, after the three backticks, “```”, in the first line of your fence code block.\n\n\nExample :\n\nYou can code like this:\n    ```python\n    print(\"This is a Python code chunk.\")\n    ```\n\nThis will output like this:\n\nprint(\"This is a Python code chunk.\")\n\nTo add an executive Python code chunk, you can first add a fence code block for your code, then add a word, “python”, with brackets, “{}”after the three backticks, “```”, in the first line of your fence code block.\n\n\nExample:\n\nYou can code like this:\n    ```{python}\n    print(\"This is a Python code chunk.\")\n    ```\n\nThis will output like this:\n\n\nprint(\"This is a Python code chunk.\")\n\nThis is a Python code chunk.\n\n\n\nYou can also make an executive Python code chunk not execute the commands inside the code chunk by adding “#| eval: false” to the first line of your code block.\n\n\nExample:\n\nYou can code like this:\n    ```{python}\n    #| eval: false\n\n    print(\"This is a Python code chunk.\")\n    ```\n\nThis will output like this:\n\n\nprint(\"This is a Python code chunk.\")\n\n\nIf you add just “#| eval: true” to your code chunk, this code chunk will execute the commands as usual and output the results.\nIf you just add “#| echo: false” to your code chunk, then your code chunk will not be displayed in your rendered output, but the commands in your code chunk will still be executed as usual and the result of the code chunk will be displayed.\nIf you just add “#| output: false” to your code chunk, then the commands in your code chunk will be displayed as usual in the rendered output, but the result of your code chunk will not be displayed.\n\n\n\n3.3.4.9 Math Notation - Using LaTeX in Markdown\n\nTo displace an inline math equation, you can add a dollar sign, “$”, before and after the math equation.\n\n\nExample:\n\nYou can code like this:\n\nThis is a quadratic equation, $ax^2+bx+c=0$.\n\nThis is a quadratic equation roots formula, $x = {(-b \\pm \\sqrt{b^2-4ac})}/{2b}$.\n\nThis code chunk will output like this:\n\nThis is a quadratic equation, \\(ax^2+bx+c=0\\).\nThis is a quadratic equation roots formula, \\(x = {(-b \\pm \\sqrt{b^2-4ac})}/{2b}\\).\n\nIn an inline math equation, make sure you use the division symbol instead of “\\frac{}{}”.\n\n\nYou can center an math equation by adding double dollar signs, “$$”, before and after the math equation.\n\n\nExample:\n\nYou can code like this:\n\nThis is a quadratic equation \n$$\nax^2+bx+c=0.\n$$\n\nThis is a quadratic equation roots formula \n$$\nx = {(-b \\pm \\sqrt{b^2-4ac})}/{2b}.\n$$\n\nThis code chunk will output like this:\n\nThis is a quadratic equation \\[\nax^2+bx+c=0.\n\\]\nThis is a quadratic equation roots formula \\[\nx = {(-b \\pm \\sqrt{b^2-4ac})}/{2b}.\n\\]\n\nBe careful not to add punctuation before a centered math equation.\nIf you want to add a period after a centered math equation, make sure you add it after the end of the math equation and before the second double dollar sign, “$$”.\n\n\n\n3.3.4.10 Footnotes\n\nTo add notes and refercences without confusing for body content, you can add a caret sign, “^”, follwed by a identifier number or words without any spaces and tabs inside a brackets, “[]”.\nExample:\n\nyou can code like this:\n\nThis is a body paragraph, and I have a note [^short_footnotes] want to \nadd here. [^long_footnotes]\n\n[^short_footnotes]: This is a notes.\n\n[^long_footnotes]: This is a notes with multiple paragraphs.\n    You can include paragraphs in your footnotes using indentation like this.\n    ```\n    {\n        This is a fenced code block.\n    }\n    ```\n    This is the end of this footnote.\n\nThis code chunk will output like this:\n\nThis is a body paragraph, and I have two notes 1 want to add here. 2\n\nClicking on the footnote number in the body content, will take you to the location where the footnote exists in the document.\nAll footnotes are automatically numbered sequentially at the end of the rendered HTML files and at the bottom of the page where the footnote exists in rendered PDF files.\n\nIn the rendered HTML file, each footnote displayed at the end of the document is followed by a link that, when clicked, takes you back to the specific location of the footnote in your body content. You can also move your mouse over the footnote number in the body content, and the content of the footnote will automatically appear below the footnote number.\n\n\n\n\n\n3.3.5 Conclusion\nMarkdown is a simple yet powerful way to format text for documentation, blogging, and technical writing. With its easy-to-read syntax, you can structure documents, highlight key points, and present data effectively. It’s widely used in open-source projects, academic writing, and web content due to its flexibility and seamless conversion to HTML, PDF, and DOCX. Mastering Markdown can help you create clear, well-organized content with minimal effort.\n\n\n3.3.6 Further Readings\n\n[Markdown Cheet Sheet] (Cone, 2025): A quick reference to the Markdown syntax.\n[Quarto Markdown Basic] (Dervieux, 2025): Markdown basic for Quarto.\n[GitHub Flavored Markdown (GFM)] (MacFarlane, 2019): Markdown features specific to GitHub.\n[Jupyter Notebook Markdown] (MacFarlane, 2006): Use Markdown for interactive data science.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reproducible Data Science</span>"
    ]
  },
  {
    "objectID": "quarto.html#creating-presentations-using-quarto",
    "href": "quarto.html#creating-presentations-using-quarto",
    "title": "3  Reproducible Data Science",
    "section": "3.4 Creating Presentations Using Quarto",
    "text": "3.4 Creating Presentations Using Quarto\n\n3.4.1 Introduction\nHi! This section is written by Joann Jun, a Junior majoring in Statistical Data Science and minoring in Mathematics at the University of Connecticut.\nThis section will focus on how to create a presentation using Quarto. By the end of this section, you will be able to understand how to:\n\nStructure a Quarto presentation\nCustomize themes, transitions, and interactivity\nKeyboard Shortcuts\n\n\n\n3.4.2 Why Use Quarto for Presentations?\n\nSeamlessly integrate code, data analysis, and visualizations into a single document\nGenerate interactive slides with minimal effort\nSupport multiple output formats, such as HTML (reveal.js), PowerPoint (pptx), and PDF (beamer).\n\n\n\n3.4.3 Presentation Formats\n\n3.4.3.1 Formats\nThere are several formats you can use to create a presentation in Quarto. This includes:\n\nrevealjs - reveal.js (HTML)\nbeamer - Beamer (LaTex/PDF).\n\n\n\n3.4.3.2 Differences Between Formats\n\n\n\n\n\n\n\n\nFeature\nrevealjs\nbeamer\n\n\n\n\nOutput Format\nHTML slideshow or PDF\nPDF\n\n\nPros\n- Works well with Markdown  - Interactive and dynamic\n- Good math support  - Professional typesetting (LaTeX)\n\n\nCons\n- Requires a browser to present\n- Requires knowledge of LaTeX\n\n\n\nIn this section, I will focus on using revealjs.\n\n\n3.4.3.3 How to Change Format\nTo change the format of your presentation, in the YAML header next format add revealjs or beamer.\n---\ntitle: Quarto Presentation\nauthor: Joann Jun\nformat: revealjs # This where you edit the format.\n---\n\n\n3.4.3.4 YAML Heading for revealjs\nAnything you put in the YAML heading will become part of the global environment. This means that it will be applied to all slides.\n\nembed-resources: true - Creates self-contained file to distribute more easily.\nmultiplex: true - Allows your audience to follow the slides that you control on their own device.\n\nWhen you render, it’ll create 2 HTML files:\n\npresentations.html - Publish online for the audience to use.\npresentations-speaker.html - File you present from and you don’t publish this.\n\n\nchalkboard - Allows you to draw on your presentation\n\n\n\n3.4.3.5 Some Stylistic YAML Headings for revealjs\n\ntheme: [slide theme] - Allows you to switch to any of Reveals 11 themes (or make your own)\n\ndefault, dark, beige, simple, serif\n\ntransition: [transition] - Adds transitions to slides\n\nnone, slide, fade, convex, concave, zoom\n\nlogo: logo.png - Allows you to add logo to bottom of each slide.\nfooter: \"Footer Note\" - Adds a footer to bottom of each slide.\nslide-number: true - Displays the slide number at bottom of screen.\nincrimental: true - Displays the bullet points one by one.\n\n\n\n3.4.3.6 Example\n---\ntitle: \"How to Make A Presentation Using Quarto\"\nauthor: \"Joann Jun\"\nformat:\n  revealjs: \n    embed-resources: true\n    theme: serif\n    slide-number: true\n    preview-links: true\n    css: [default, styles.css] # Don't need this unless customizing more\n    incremental: true   \n    transition: slide\n    footer: \"STAT 3255\"\n\n---\n\n\n\n3.4.4 Slides syntax\n\n3.4.4.1 How to Create a New Slide\nThe start of all slides are marked by a heading. You can do this by using\n\nLevel 1 header (#) - Used to create title slide.\nLevel 2 header (##) - Used to create headings.\nHorizontal rules (---) - Used when you don’t want to add a heading or title.\nNote: ### will create a subheading in the slide.\n\n\n# Title 1\n\nHello, World!\n\n## Slide Heading 1\n\n### Subjeading 1\n\nHello, World!\n\n--- # Makes slide without title/heading\n\nHello, World!\n\n\n\n3.4.5 Code\n\n3.4.5.1 auto-animate=true\nThis setting will allow smooth transitions across similar slides. You use this when you want to show gradual changes between slides.\nFor example, let’s say you have a block of code and then add another block, we can show the changes by using this. The first slide should only have part of the code, and the second should have the full code.\nSlide 1\n\n\n## Smooth Transition Slide 1 {auto-animate=true}\n\nfrom math import sqrt\n\nSlide 2\n\n\n## Smooth Transition Slide 2 {auto-animate=true}\n\nfrom math import sqrt\n\ndef pythagorean(a,b):\n    c2 = a**2 + b**2\n    c = sqrt(c2)\n    return c\n\n\n\n3.4.5.2 Highlighting Code\nTo highlight code, you can use the code-line-numbers attribute and use \"[start line #]-[end line #]\".\nFor example, in this slide let’s say I wanted to highlight the addition and subtraction function. I do this by putting {.python code-line-numbers=“2-6”} next to my 3 back ticks that start the code fence.\n#| echo: true\n\ndef addition(x,y):\n    return x + y\n\ndef subtraction(x,y):\n    return x - y\n\ndef multiplication(x,y):\n    return x * y\nIf the lines you want to highlight are separated by another line, you can use a comma.\nIn this slide I wanted to highlight return for each function, so I used {.python code-line-numbers=“3,6,9”}\n#| echo: true\n\ndef addition(x,y):\n    return x + y\n\ndef subtraction(x,y):\n    return x - y\n\ndef multiplication(x,y):\n    return x * y\n\n\n3.4.5.3 echo\nBy default, the code block does not echo the source code. By this I mean it does not show the source code and only shows the output by default.\nIn order to show the source code, we set the setting to #| echo: true. We add this part inside of the code fence at the top.\nEx: echo: false\n\n\n\n\n\n\n\n\n\nEx: echo: true\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx = np.linspace(0, 10, 100)\ny = np.sin(x)\n\nplt.figure(figsize=(12, 6))  # Width=12, Height=6\nplt.plot(x, y, label='sin(x)')\nplt.title('Sine Function')\nplt.xlabel('x')\nplt.ylabel('sin(x)')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n3.4.5.4 Figures\n\nPython figure sizes (MatPlotLib and PlotlyExpress) are automatically set to fill the slide area below the title\nR (Knitr figure) have similar default figure width and length sizes\nYou will most likely have to change the size of these figures or change the output location\n\n\n\n3.4.5.5 output-location\nThe output-location option can modify where the output of the code goes. There are several options to choose from such as:\n\ncolumn - Displays the output in a column next to the code.\ncolumn-fragment - Displays the output in a column next to the code and delays showing it until you advance.\nfragment - Displays the output as a Fragment. It delays showing it until you advance.\nslide - Displays the output on the slide after.\n\nEx: This uses column.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx = np.linspace(0, 10, 100)\ny = np.sin(x)\n\nplt.figure(figsize=(12, 6))\nplt.plot(x, y, label='sin(x)')\nplt.title('Sine Function')\nplt.xlabel('x')\nplt.ylabel('sin(x)')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n3.4.6 Interactive Code - HTML Gagdets\n\nHTML Gadget allows you to embed interactive elements into the HTML document.\nEnhance user engagement and data exploration.\nSeamless integration with R and Python in Quarto.\nVisualize complex data interactively.\nSome examples of these are:\n\nInteractive maps (e.g., Google Maps, Leaflet)\nData tables with sorting and searching\nDynamic plots with tooltips and zooming\nShiny apps for data exploration\n\n\n\n3.4.6.1 HTML Gagdget Tools\n\nLeaflet - Used to make interactive maps which allows you to display location based data.\nPlotly - Used for creating interactive plots.\nShiny - Used to make interactive statistical model that allows you to tweak parameters and see impacts.\nDT - Used for creating searchable data tables.\n\n\n\n\n\n\n\n\n\n\n\nTool\nLeaflet\nPlotly\nShiny\nDT\n\n\n\n\nPurpose\nInteractive maps\nInteractive plots\nBuild web apps\nInteractive tables\n\n\nKey Features\n- Markers, popups, and layers- Custom styles- Zoom and pan- Various tile providers\n- Scatter, line, and bar charts- Tooltips, hover effects, zoom\n- Dynamic with sliders and inputs- Real-time updates- Widgets for interactivity\n- Sorting, filtering, and pages- Customizable styling- Searchable columns\n\n\nUse Cases\n- Visualizing locations and routes\n- Analyzing data trends and relationships\n- Dashboards, reports, and data exploration\n- Displaying and exploring large datasets\n\n\n\n\n\n3.4.6.2 How to Get Started\n\nInstall the tools necessary using install.packages(c(\"plotly\", \"shiny\", \"DT\", \"leaflet\"), repos = \"https://cloud.r-project.org/\").\nThe code above should also allow you to avoid the CRAN error.\nNote: Shiny app should be run in a separate R session or browser window.\n\n\n\n3.4.6.3 Map of UCONN (Leaflet)\n\n## Install these R packages if you don't have them already\n## options(repos = c(CRAN = \"https://cloud.r-project.org/\"))\n## install.packages(c(\"leaflet\", \"plotly\", \"shiny\", \"DT\",\n##                             \"htmlwidgets\", \"webshot\", \"knitr\"))\n\nlibrary(leaflet)\n\n# Create the interactive leaflet map\nmap &lt;- leaflet() %&gt;%\n  addTiles() %&gt;%\n  setView(lng = -72.2565, lat = 41.8084, zoom = 15) %&gt;%\n  addMarkers(lng = -72.2565, lat = 41.8084, popup = \"UConn Main Campus\")\n\n## Ensure the 'images' folder exists\nif (!dir.exists(\"images\")) {\n    dir.create(\"images\")\n}\n\n# Conditional rendering based on output format\nif (knitr::is_html_output()) {\n  map  # Show interactive map in HTML\n} else {\n  # Define file paths\n  html_file &lt;- \"images/leaflet_temp.html\"\n  png_file &lt;- \"images/uconn_static.png\"\n\n  # Save the leaflet map as an HTML file\n  htmlwidgets::saveWidget(map, html_file, selfcontained = TRUE)\n\n  # Take a screenshot of the HTML file as a PNG image\n  webshot::webshot(html_file, file = png_file, delay = 2, vwidth = 800, vheight = 600)\n\n  # Include the PNG in the PDF output\n  knitr::include_graphics(png_file)\n}\n\n\n\n\n\n\n\n3.4.6.4 Interactive Plot (PlotLy)\n\nlibrary(plotly)\n\nfig &lt;- plot_ly(mtcars, x = ~mpg, y = ~hp, type = 'scatter', mode = 'markers', \n               marker = list(size = 10, color = ~cyl, colorscale = 'Blue'))\nfig &lt;- fig %&gt;% layout(title = \"Interactive Scatter Plot of MPG vs HP\",\n                      xaxis = list(title = \"Miles Per Gallon\"),\n                      yaxis = list(title = \"Horsepower\"))\n\n# Conditional rendering based on output format\nif (knitr::is_html_output()) {\n  fig  # Show interactive plot in HTML\n} else {\n  # Define file paths\n  html_file &lt;- \"images/plotly_temp.html\"\n  png_file &lt;- \"images/mtcars_static.png\"\n\n  # Save the plotly chart as an HTML file\n  htmlwidgets::saveWidget(fig, html_file, selfcontained = TRUE)\n\n  # Take a screenshot of the HTML file as a PNG image\n  webshot::webshot(html_file, file = png_file, delay = 2, vwidth = 800, vheight = 600)\n\n  knitr::include_graphics(png_file)\n}\n\n\n\n\n\n\n\n\n3.4.7 Rendering Your Presentation\nIn your terminal enter the following code:\n\nquarto render &lt;presentation_name&gt;.qmd --to revealjs\n\nThis will produce a HTML slideshow output that you can present.\n\n\n3.4.8 Keyboard Shortcuts\n\nS - Brings you to speaker view\nF - Fullscreen\n→,SPACE,N - Next slide\n←,P - Previous slide\nAlt →, Alt ← - Navigates without Fragments\nShift →, Shift ← - Navigates to first or last slide\n\n\n\n3.4.9 More Information/Resources\n\nRevealjs\nPowerPoint\nBeamer\nShiny\nPlotLy\nPlotly_Interactive\nDT\nLeaflet\n\n\n\n\n\nCone, M. (2025). Markdown cheat sheet | markdown guide. https://www.markdownguide.org/cheat-sheet/\n\n\nDervieux, C. (2025). Markdown-basics. https://quarto.org/docs/authoring/markdown-basics.html\n\n\nMacFarlane, J. (2006). Pandoc user’s guide. https://pandoc.org/MANUAL.html#pandocs-markdown\n\n\nMacFarlane, J. (2019). GitHub flavored markdown spec. https://github.github.com/gfm/",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reproducible Data Science</span>"
    ]
  },
  {
    "objectID": "quarto.html#footnotes",
    "href": "quarto.html#footnotes",
    "title": "3  Reproducible Data Science",
    "section": "",
    "text": "This is a notes.↩︎\nThis is a notes with multiple paragraphs. You can include paragraphs in your footnotes using indentation like this.\n    print(This is a fenced code block.)\n\n    You can add any thing inside this fenced code block.\nThis is the end of this footnote.↩︎",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reproducible Data Science</span>"
    ]
  },
  {
    "objectID": "python.html",
    "href": "python.html",
    "title": "4  Python Refreshment",
    "section": "",
    "text": "4.1 The Python World\nYou have programmed in Python. Regardless of your skill level, let us do some refreshing.\nSee, for example, how to build a Python libratry.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "python.html#the-python-world",
    "href": "python.html#the-python-world",
    "title": "4  Python Refreshment",
    "section": "",
    "text": "Function: a block of organized, reusable code to complete certain task.\nModule: a file containing a collection of functions, variables, and statements.\nPackage: a structured directory containing collections of modules and an __init.py__ file by which the directory is interpreted as a package.\nLibrary: a collection of related functionality of codes. It is a reusable chunk of code that we can use by importing it in our program, we can just use it by importing that library and calling the method of that library with period(.).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "python.html#standard-library",
    "href": "python.html#standard-library",
    "title": "4  Python Refreshment",
    "section": "4.2 Standard Library",
    "text": "4.2 Standard Library\nPython’s has an extensive standard library that offers a wide range of facilities as indicated by the long table of contents listed below. See documentation online.\n\nThe library contains built-in modules (written in C) that provide access to system functionality such as file I/O that would otherwise be inaccessible to Python programmers, as well as modules written in Python that provide standardized solutions for many problems that occur in everyday programming. Some of these modules are explicitly designed to encourage and enhance the portability of Python programs by abstracting away platform-specifics into platform-neutral APIs.\n\nQuestion: How to get the constant \\(e\\) to an arbitary precision?\nThe constant is only represented by a given double precision.\n\nimport math\nprint(\"%0.20f\" % math.e)\nprint(\"%0.80f\" % math.e)\n\n2.71828182845904509080\n2.71828182845904509079559829842764884233474731445312500000000000000000000000000000\n\n\nNow use package decimal to export with an arbitary precision.\n\nimport decimal  # for what?\n\n## set the required number digits to 150\ndecimal.getcontext().prec = 150\ndecimal.Decimal(1).exp().to_eng_string()\ndecimal.Decimal(1).exp().to_eng_string()[2:]\n\n'71828182845904523536028747135266249775724709369995957496696762772407663035354759457138217852516642742746639193200305992181741359662904357290033429526'",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "python.html#important-libraries",
    "href": "python.html#important-libraries",
    "title": "4  Python Refreshment",
    "section": "4.3 Important Libraries",
    "text": "4.3 Important Libraries\n\nNumPy\npandas\nmatplotlib\nIPython/Jupyter\nSciPy\nscikit-learn\nstatsmodels\n\nQuestion: how to draw a random sample from a normal distribution and evaluate the density and distributions at these points?\n\nfrom scipy.stats import norm\n\nmu, sigma = 2, 4\nmean, var, skew, kurt = norm.stats(mu, sigma, moments='mvsk')\nprint(mean, var, skew, kurt)\nx = norm.rvs(loc = mu, scale = sigma, size = 10)\nx\n\n2.0 16.0 0.0 0.0\n\n\narray([-7.61542223,  4.04410857, -0.66018039, -3.22051506,  3.46732088,\n        8.19518069, -0.249417  , -1.29855302,  4.58570583,  4.19481766])\n\n\nThe pdf and cdf can be evaluated:\n\nnorm.pdf(x, loc = mu, scale = sigma)\n\narray([0.00554702, 0.08752706, 0.07994828, 0.04255689, 0.09324591,\n       0.03005831, 0.08514888, 0.07098782, 0.08093034, 0.08579701])",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "python.html#writing-a-function",
    "href": "python.html#writing-a-function",
    "title": "4  Python Refreshment",
    "section": "4.4 Writing a Function",
    "text": "4.4 Writing a Function\nConsider the Fibonacci Sequence \\(1, 1, 2, 3, 5, 8, 13, 21, 34, ...\\). The next number is found by adding up the two numbers before it. We are going to use 3 ways to solve the problems.\nThe first is a recursive solution.\n\ndef fib_rs(n):\n    if (n==1 or n==2):\n        return 1\n    else:\n        return fib_rs(n - 1) + fib_rs(n - 2)\n\n%timeit fib_rs(10)\n\n8.83 μs ± 51 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n\n\nThe second uses dynamic programming memoization.\n\ndef fib_dm_helper(n, mem):\n    if mem[n] is not None:\n        return mem[n]\n    elif (n == 1 or n == 2):\n        result = 1\n    else:\n        result = fib_dm_helper(n - 1, mem) + fib_dm_helper(n - 2, mem)\n    mem[n] = result\n    return result\n\ndef fib_dm(n):\n    mem = [None] * (n + 1)\n    return fib_dm_helper(n, mem)\n\n%timeit fib_dm(10)\n\n1.93 μs ± 9.57 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)\n\n\nThe third is still dynamic programming but bottom-up.\n\ndef fib_dbu(n):\n    mem = [None] * (n + 1)\n    mem[1] = 1;\n    mem[2] = 1;\n    for i in range(3, n + 1):\n        mem[i] = mem[i - 1] + mem[i - 2]\n    return mem[n]\n\n\n%timeit fib_dbu(500)\n\n66.4 μs ± 918 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n\n\nApparently, the three solutions have very different performance for larger n.\n\n4.4.1 Monty Hall\nHere is a function that performs the Monty Hall experiments. In this version, the host opens only one empty door.\n\nimport numpy as np\n\ndef montyhall(ndoors, ntrials):\n    doors = np.arange(1, ndoors + 1) / 10\n    prize = np.random.choice(doors, size=ntrials)\n    player = np.random.choice(doors, size=ntrials)\n    host = np.array([np.random.choice([d for d in doors\n                                       if d not in [player[x], prize[x]]])\n                     for x in range(ntrials)])\n    player2 = np.array([np.random.choice([d for d in doors\n                                          if d not in [player[x], host[x]]])\n                        for x in range(ntrials)])\n    return {'noswitch': np.sum(prize == player), 'switch': np.sum(prize == player2)}\n\nTest it out:\n\nmontyhall(3, 1000)\nmontyhall(4, 1000)\n\n{'noswitch': np.int64(242), 'switch': np.int64(381)}\n\n\nThe true value for the two strategies with \\(n\\) doors are, respectively, \\(1 / n\\) and \\(\\frac{n - 1}{n (n - 2)}\\).\nIn the homework exercise, the host opens \\(m\\) doors that are empty. An argument nempty could be added to the function.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "python.html#variables-versus-objects",
    "href": "python.html#variables-versus-objects",
    "title": "4  Python Refreshment",
    "section": "4.5 Variables versus Objects",
    "text": "4.5 Variables versus Objects\nIn Python, variables and the objects they point to actually live in two different places in the computer memory. Think of variables as pointers to the objects they’re associated with, rather than being those objects. This matters when multiple variables point to the same object.\n\nx = [1, 2, 3]  # create a list; x points to the list\ny = x          # y also points to the same list in the memory\ny.append(4)    # append to y\nx              # x changed!\n\n[1, 2, 3, 4]\n\n\nNow check their addresses\n\nprint(id(x))   # address of x\nprint(id(y))   # address of y\n\n4696292544\n4696292544\n\n\nNonetheless, some data types in Python are “immutable”, meaning that their values cannot be changed in place. One such example is strings.\n\nx = \"abc\"\ny = x\ny = \"xyz\"\nx\n\n'abc'\n\n\nNow check their addresses\n\nprint(id(x))   # address of x\nprint(id(y))   # address of y\n\n4526902032\n4610332432\n\n\nQuestion: What’s mutable and what’s immutable?\nAnything that is a collection of other objects is mutable, except tuples.\nNot all manipulations of mutable objects change the object rather than create a new object. Sometimes when you do something to a mutable object, you get back a new object. Manipulations that change an existing object, rather than create a new one, are referred to as “in-place mutations” or just “mutations.” So:\n\nAll manipulations of immutable types create new objects.\nSome manipulations of mutable types create new objects.\n\nDifferent variables may all be pointing at the same object is preserved through function calls (a behavior known as “pass by object-reference”). So if you pass a list to a function, and that function manipulates that list using an in-place mutation, that change will affect any variable that was pointing to that same object outside the function.\n\nx = [1, 2, 3]\ny = x\n\ndef append_42(input_list):\n    input_list.append(42)\n    return input_list\n\nappend_42(x)\n\n[1, 2, 3, 42]\n\n\nNote that both x and y have been appended by \\(42\\).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "python.html#number-representation",
    "href": "python.html#number-representation",
    "title": "4  Python Refreshment",
    "section": "4.6 Number Representation",
    "text": "4.6 Number Representation\nNumers in a computer’s memory are represented by binary styles (on and off of bits).\n\n4.6.1 Integers\nIf not careful, It is easy to be bitten by overflow with integers when using Numpy and Pandas in Python.\n\nimport numpy as np\n\nx = np.array(2 ** 63 - 1 , dtype = 'int')\nx\n# This should be the largest number numpy can display, with\n# the default int8 type (64 bits)\n\narray(9223372036854775807)\n\n\nNote: on Windows and other platforms, dtype = 'int' may have to be changed to dtype = np.int64 for the code to execute. Source: Stackoverflow\nWhat if we increment it by 1?\n\ny = np.array(x + 1, dtype = 'int')\ny\n# Because of the overflow, it becomes negative!\n\narray(-9223372036854775808)\n\n\nFor vanilla Python, the overflow errors are checked and more digits are allocated when needed, at the cost of being slow.\n\n2 ** 63 * 1000\n\n9223372036854775808000\n\n\nThis number is 1000 times larger than the prior number, but still displayed perfectly without any overflows\n\n\n4.6.2 Floating Number\nStandard double-precision floating point number uses 64 bits. Among them, 1 is for sign, 11 is for exponent, and 52 are fraction significand, See https://en.wikipedia.org/wiki/Double-precision_floating-point_format. The bottom line is that, of course, not every real number is exactly representable.\nIf you have played the Game 24, here is a tricky one:\n\n8 / (3 - 8 / 3) == 24\n\nFalse\n\n\nSurprise?\nThere are more.\n\n0.1 + 0.1 + 0.1 == 0.3\n\nFalse\n\n\n\n0.3 - 0.2 == 0.1\n\nFalse\n\n\nWhat is really going on?\n\nimport decimal\ndecimal.Decimal(0.1)\n\nDecimal('0.1000000000000000055511151231257827021181583404541015625')\n\n\n\ndecimal.Decimal(8 / (3 - 8 / 3))\n\nDecimal('23.999999999999989341858963598497211933135986328125')\n\n\nBecause the mantissa bits are limited, it can not represent a floating point that’s both very big and very precise. Most computers can represent all integers up to \\(2^{53}\\), after that it starts skipping numbers.\n\n2.1 ** 53 + 1 == 2.1 ** 53\n\n# Find a number larger than 2 to the 53rd\n\nTrue\n\n\n\nx = 2.1 ** 53\nfor i in range(1000000):\n    x = x + 1\nx == 2.1 ** 53\n\nTrue\n\n\nWe add 1 to x by 1000000 times, but it still equal to its initial value, 2.1 ** 53. This is because this number is too big that computer can’t handle it with precision like add 1.\nMachine epsilon is the smallest positive floating-point number x such that 1 + x != 1.\n\nprint(np.finfo(float).eps)\nprint(np.finfo(np.float32).eps)\n\n2.220446049250313e-16\n1.1920929e-07",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "python.html#sec-python-venv",
    "href": "python.html#sec-python-venv",
    "title": "4  Python Refreshment",
    "section": "4.7 Virtual Environment",
    "text": "4.7 Virtual Environment\nVirtual environments in Python are essential tools for managing dependencies and ensuring consistency across projects. They allow you to create isolated environments for each project, with its own set of installed packages, separate from the global Python installation. This isolation prevents conflicts between project dependencies and versions, making your projects more reliable and easier to manage. It’s particularly useful when working on multiple projects with differing requirements, or when collaborating with others who may have different setups.\nTo set up a virtual environment, you first need to ensure that Python is installed on your system. Most modern Python installations come with the venv module, which is used to create virtual environments. Here’s how to set one up:\n\nOpen your command line interface.\nNavigate to your project directory.\nRun python3 -m venv myenv, where myenv is the name of the virtual environment to be created. Choose an informative name.\n\nThis command creates a new directory named myenv (or your chosen name) in your project directory, containing the virtual environment.\nTo start using this environment, you need to activate it. The activation command varies depending on your operating system:\n\nOn Windows, run myenv\\Scripts\\activate.\nOn Linux or MacOS, use source myenv/bin/activate or . myenv/bin/activate.\n\nOnce activated, your command line will typically show the name of the virtual environment, and you can then install and use packages within this isolated environment without affecting your global Python setup.\nTo exit the virtual environment, simply type deactivate in your command line. This will return you to your system’s global Python environment.\nAs an example, let’s install a package, like numpy, in this newly created virtual environment:\n\nEnsure your virtual environment is activated.\nRun pip install numpy.\n\nThis command installs the requests library in your virtual environment. You can verify the installation by running pip list, which should show requests along with its version.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "python.html#numpy",
    "href": "python.html#numpy",
    "title": "4  Python Refreshment",
    "section": "4.8 Numpy",
    "text": "4.8 Numpy",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "manipulation.html",
    "href": "manipulation.html",
    "title": "5  Data Manipulation",
    "section": "",
    "text": "5.1 Introduction\nData manipulation is crucial for transforming raw data into a more analyzable format, essential for uncovering patterns and ensuring accurate analysis. This chapter introduces the core techniques for data manipulation in Python, utilizing the Pandas library, a cornerstone for data handling within Python’s data science toolkit.\nPython’s ecosystem is rich with libraries that facilitate not just data manipulation but comprehensive data analysis. Pandas, in particular, provides extensive functionality for data manipulation tasks including reading, cleaning, transforming, and summarizing data. Using real-world datasets, we will explore how to leverage Python for practical data manipulation tasks.\nBy the end of this chapter, you will learn to:",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Manipulation</span>"
    ]
  },
  {
    "objectID": "manipulation.html#introduction",
    "href": "manipulation.html#introduction",
    "title": "5  Data Manipulation",
    "section": "",
    "text": "Import/export data from/to diverse sources.\nClean and preprocess data efficiently.\nTransform and aggregate data to derive insights.\nMerge and concatenate datasets from various origins.\nAnalyze real-world datasets using these techniques.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Manipulation</span>"
    ]
  },
  {
    "objectID": "manipulation.html#data-manipulation-with-pandas",
    "href": "manipulation.html#data-manipulation-with-pandas",
    "title": "5  Data Manipulation",
    "section": "5.2 Data Manipulation with Pandas",
    "text": "5.2 Data Manipulation with Pandas\nThis section is prepared by Lang Lang. I am a senior student double majoring in\ndata science and economics in University of Connecticut.\n\n5.2.1 Introduction\nIn this section, I will introduce about data manipulation using Pandas,\nwhich is a powerful Python library for working with data. I’ll walk through\nsome basic operations like filtering, merging, and summarizing data using a real\ndata set of NYC motor vehicle collisions.\nPandas is a powerful Python library for data manipulation and analysis. It\nprovides two key data structures:\n\nSeries: A one-dimensional labeled array.\nData Frame: A two-dimensional labeled table with rows and columns.\n\n\n5.2.1.1 Why Use Pandas?\n\nEfficiently handles large data sets.\nProvides flexible data manipulation functions.\nWorks well with NumPy and visualization libraries like Matplotlib.\n\n\n\n\n5.2.2 Loading Data\n\n5.2.2.1 Reading NYC Crash Data\nWe’ll work with the NYC Motor Vehicle Collisions data set in the class notes\nrepository.\n\nUsing the following code to import the Pandas library in Python\n\n\nimport pandas as pd\n\n\nUsing the following code to load the data set\n\n\ndf = pd.read_csv(\"data/nyccrashes_2024w0630_by20250212.csv\")\n\n\n\n5.2.2.2 Renaming The Columns\nUsing the following codes to rename the columns.\n\ndf.columns = [col.strip().lower().replace(\" \", \"_\") for col in df.columns]\ndf.columns\n\nIndex(['crash_date', 'crash_time', 'borough', 'zip_code', 'latitude',\n       'longitude', 'location', 'on_street_name', 'cross_street_name',\n       'off_street_name', 'number_of_persons_injured',\n       'number_of_persons_killed', 'number_of_pedestrians_injured',\n       'number_of_pedestrians_killed', 'number_of_cyclist_injured',\n       'number_of_cyclist_killed', 'number_of_motorist_injured',\n       'number_of_motorist_killed', 'contributing_factor_vehicle_1',\n       'contributing_factor_vehicle_2', 'contributing_factor_vehicle_3',\n       'contributing_factor_vehicle_4', 'contributing_factor_vehicle_5',\n       'collision_id', 'vehicle_type_code_1', 'vehicle_type_code_2',\n       'vehicle_type_code_3', 'vehicle_type_code_4', 'vehicle_type_code_5'],\n      dtype='object')\n\n\n\n\n5.2.2.3 Viewing the First Few Rows\nThe head function in Pandas is used to display the first few rows of a DataFrame.\n\ndf.head() # Default value of n is 5\n\n\n\n\n\n\n\n\ncrash_date\ncrash_time\nborough\nzip_code\nlatitude\nlongitude\nlocation\non_street_name\ncross_street_name\noff_street_name\n...\ncontributing_factor_vehicle_2\ncontributing_factor_vehicle_3\ncontributing_factor_vehicle_4\ncontributing_factor_vehicle_5\ncollision_id\nvehicle_type_code_1\nvehicle_type_code_2\nvehicle_type_code_3\nvehicle_type_code_4\nvehicle_type_code_5\n\n\n\n\n0\n06/30/2024\n23:17\nBRONX\n10460.0\n40.838844\n-73.87817\n(40.838844, -73.87817)\nEAST 177 STREET\nDEVOE AVENUE\nNaN\n...\nUnspecified\nNaN\nNaN\nNaN\n4737486\nSedan\nPick-up Truck\nNaN\nNaN\nNaN\n\n\n1\n06/30/2024\n8:30\nBRONX\n10468.0\n40.862732\n-73.90333\n(40.862732, -73.90333)\nWEST FORDHAM ROAD\nGRAND AVENUE\nNaN\n...\nUnspecified\nNaN\nNaN\nNaN\n4737502\nSedan\nNaN\nNaN\nNaN\nNaN\n\n\n2\n06/30/2024\n20:47\nNaN\nNaN\n40.763630\n-73.95330\n(40.76363, -73.9533)\nFDR DRIVE\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\n4737510\nSedan\nNaN\nNaN\nNaN\nNaN\n\n\n3\n06/30/2024\n13:10\nBROOKLYN\n11234.0\n40.617030\n-73.91989\n(40.61703, -73.91989)\nEAST 57 STREET\nAVENUE O\nNaN\n...\nDriver Inattention/Distraction\nNaN\nNaN\nNaN\n4737499\nSedan\nSedan\nNaN\nNaN\nNaN\n\n\n4\n06/30/2024\n16:42\nNaN\nNaN\nNaN\nNaN\nNaN\n33 STREET\nASTORIA BOULEVARD\nNaN\n...\nUnspecified\nNaN\nNaN\nNaN\n4736925\nSedan\nStation Wagon/Sport Utility Vehicle\nNaN\nNaN\nNaN\n\n\n\n\n5 rows × 29 columns\n\n\n\n\n\n5.2.2.4 Checking Dataset Structure\nThe info function in Pandas provides a summary of a DataFrame, including:\n\nNumber of rows and columns\nColumn names and data types\nNumber of non-null values per column\nMemory usage\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1876 entries, 0 to 1875\nData columns (total 29 columns):\n #   Column                         Non-Null Count  Dtype  \n---  ------                         --------------  -----  \n 0   crash_date                     1876 non-null   object \n 1   crash_time                     1876 non-null   object \n 2   borough                        1334 non-null   object \n 3   zip_code                       1334 non-null   float64\n 4   latitude                       1745 non-null   float64\n 5   longitude                      1745 non-null   float64\n 6   location                       1745 non-null   object \n 7   on_street_name                 1330 non-null   object \n 8   cross_street_name              944 non-null    object \n 9   off_street_name                546 non-null    object \n 10  number_of_persons_injured      1876 non-null   int64  \n 11  number_of_persons_killed       1876 non-null   int64  \n 12  number_of_pedestrians_injured  1876 non-null   int64  \n 13  number_of_pedestrians_killed   1876 non-null   int64  \n 14  number_of_cyclist_injured      1876 non-null   int64  \n 15  number_of_cyclist_killed       1876 non-null   int64  \n 16  number_of_motorist_injured     1876 non-null   int64  \n 17  number_of_motorist_killed      1876 non-null   int64  \n 18  contributing_factor_vehicle_1  1865 non-null   object \n 19  contributing_factor_vehicle_2  1426 non-null   object \n 20  contributing_factor_vehicle_3  174 non-null    object \n 21  contributing_factor_vehicle_4  52 non-null     object \n 22  contributing_factor_vehicle_5  14 non-null     object \n 23  collision_id                   1876 non-null   int64  \n 24  vehicle_type_code_1            1843 non-null   object \n 25  vehicle_type_code_2            1231 non-null   object \n 26  vehicle_type_code_3            162 non-null    object \n 27  vehicle_type_code_4            48 non-null     object \n 28  vehicle_type_code_5            14 non-null     object \ndtypes: float64(3), int64(9), object(17)\nmemory usage: 425.2+ KB\n\n\nThis tells us:\n\nThe dataset has 1876 rows and 29 columns.\nData types include float64(3), int64(9), object(17).\nThere are no missing values in any column.\nThe dataset consumes 425.2+ KB of memory.\n\n\n\n5.2.2.5 Descriptive Statistics\nThe discribe function provides summary statistics for numerical columns in a\nPandas DataFrame.\n\ndf.describe()\n\n\n\n\n\n\n\n\nzip_code\nlatitude\nlongitude\nnumber_of_persons_injured\nnumber_of_persons_killed\nnumber_of_pedestrians_injured\nnumber_of_pedestrians_killed\nnumber_of_cyclist_injured\nnumber_of_cyclist_killed\nnumber_of_motorist_injured\nnumber_of_motorist_killed\ncollision_id\n\n\n\n\ncount\n1334.000000\n1745.000000\n1745.000000\n1876.000000\n1876.000000\n1876.000000\n1876.000000\n1876.000000\n1876.0\n1876.000000\n1876.000000\n1.876000e+03\n\n\nmean\n10902.185157\n40.649264\n-73.792754\n0.616738\n0.004264\n0.093284\n0.002665\n0.065032\n0.0\n0.432836\n0.001599\n4.738602e+06\n\n\nstd\n526.984169\n1.689337\n3.064378\n0.915477\n0.103191\n0.338369\n0.095182\n0.246648\n0.0\n0.891003\n0.039968\n1.772834e+03\n\n\nmin\n10001.000000\n0.000000\n-74.237366\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.0\n0.000000\n0.000000\n4.736561e+06\n\n\n25%\n10457.000000\n40.661804\n-73.968540\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.0\n0.000000\n0.000000\n4.737667e+06\n\n\n50%\n11208.500000\n40.712696\n-73.922960\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.0\n0.000000\n0.000000\n4.738258e+06\n\n\n75%\n11239.000000\n40.767690\n-73.869180\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.0\n1.000000\n0.000000\n4.738886e+06\n\n\nmax\n11694.000000\n40.907246\n0.000000\n11.000000\n4.000000\n7.000000\n4.000000\n1.000000\n0.0\n11.000000\n1.000000\n4.765601e+06\n\n\n\n\n\n\n\nThis provides insights such as:\n\nTotal count of entries\nMean, min, and max values\nStandard deviation\n\n\n\n\n5.2.3 Selecting and Filtering Data\n\n5.2.3.1 Selecting Specific Columns\nSometimes, we only need certain columns.\nIn this case, you can use select function.\n\ndf_selected = df[['crash_date', 'crash_time', 'borough', \n                  'number_of_persons_injured']]\ndf_selected.head() \n\n\n\n\n\n\n\n\ncrash_date\ncrash_time\nborough\nnumber_of_persons_injured\n\n\n\n\n0\n06/30/2024\n23:17\nBRONX\n0\n\n\n1\n06/30/2024\n8:30\nBRONX\n0\n\n\n2\n06/30/2024\n20:47\nNaN\n0\n\n\n3\n06/30/2024\n13:10\nBROOKLYN\n1\n\n\n4\n06/30/2024\n16:42\nNaN\n1\n\n\n\n\n\n\n\n\n\n5.2.3.2 Filtering Data\nwhen you would like to filter certain specific data (e.g., Crashes in 2024-06-30),\nyou can using the following data to define:\n\n# Convert crash date to datetime format\ndf['crash_date'] = pd.to_datetime(df['crash_date'])\n\njune30_df = df[df['crash_date'] == '2024-06-30']\njune30_df.head()\n\n\n\n\n\n\n\n\ncrash_date\ncrash_time\nborough\nzip_code\nlatitude\nlongitude\nlocation\non_street_name\ncross_street_name\noff_street_name\n...\ncontributing_factor_vehicle_2\ncontributing_factor_vehicle_3\ncontributing_factor_vehicle_4\ncontributing_factor_vehicle_5\ncollision_id\nvehicle_type_code_1\nvehicle_type_code_2\nvehicle_type_code_3\nvehicle_type_code_4\nvehicle_type_code_5\n\n\n\n\n0\n2024-06-30\n23:17\nBRONX\n10460.0\n40.838844\n-73.87817\n(40.838844, -73.87817)\nEAST 177 STREET\nDEVOE AVENUE\nNaN\n...\nUnspecified\nNaN\nNaN\nNaN\n4737486\nSedan\nPick-up Truck\nNaN\nNaN\nNaN\n\n\n1\n2024-06-30\n8:30\nBRONX\n10468.0\n40.862732\n-73.90333\n(40.862732, -73.90333)\nWEST FORDHAM ROAD\nGRAND AVENUE\nNaN\n...\nUnspecified\nNaN\nNaN\nNaN\n4737502\nSedan\nNaN\nNaN\nNaN\nNaN\n\n\n2\n2024-06-30\n20:47\nNaN\nNaN\n40.763630\n-73.95330\n(40.76363, -73.9533)\nFDR DRIVE\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\n4737510\nSedan\nNaN\nNaN\nNaN\nNaN\n\n\n3\n2024-06-30\n13:10\nBROOKLYN\n11234.0\n40.617030\n-73.91989\n(40.61703, -73.91989)\nEAST 57 STREET\nAVENUE O\nNaN\n...\nDriver Inattention/Distraction\nNaN\nNaN\nNaN\n4737499\nSedan\nSedan\nNaN\nNaN\nNaN\n\n\n4\n2024-06-30\n16:42\nNaN\nNaN\nNaN\nNaN\nNaN\n33 STREET\nASTORIA BOULEVARD\nNaN\n...\nUnspecified\nNaN\nNaN\nNaN\n4736925\nSedan\nStation Wagon/Sport Utility Vehicle\nNaN\nNaN\nNaN\n\n\n\n\n5 rows × 29 columns\n\n\n\n\n\n5.2.3.3 Filter A DataFrame\nThe query function in Pandas is used to filter a DataFrame using a more\nreadable, SQL-like syntax. For example, now I would like to find the crashes with\nthe number of persons injured more than 2. We can using the following code:\n\ndf_filtered = df.query(\"`number_of_persons_injured` &gt; 2\")\ndf_filtered.head()\n\n\n\n\n\n\n\n\ncrash_date\ncrash_time\nborough\nzip_code\nlatitude\nlongitude\nlocation\non_street_name\ncross_street_name\noff_street_name\n...\ncontributing_factor_vehicle_2\ncontributing_factor_vehicle_3\ncontributing_factor_vehicle_4\ncontributing_factor_vehicle_5\ncollision_id\nvehicle_type_code_1\nvehicle_type_code_2\nvehicle_type_code_3\nvehicle_type_code_4\nvehicle_type_code_5\n\n\n\n\n20\n2024-06-30\n13:05\nNaN\nNaN\n40.797447\n-73.946710\n(40.797447, -73.94671)\nMADISON AVENUE\nNaN\nNaN\n...\nUnspecified\nNaN\nNaN\nNaN\n4736750\nSedan\nStation Wagon/Sport Utility Vehicle\nNaN\nNaN\nNaN\n\n\n33\n2024-06-30\n15:30\nBROOKLYN\n11229.0\n40.610653\n-73.953550\n(40.610653, -73.95355)\nKINGS HIGHWAY\nOCEAN AVENUE\nNaN\n...\nNaN\nNaN\nNaN\nNaN\n4737460\nSedan\nNaN\nNaN\nNaN\nNaN\n\n\n35\n2024-06-30\n16:28\nSTATEN ISLAND\n10304.0\n40.589180\n-74.098946\n(40.58918, -74.098946)\nJEFFERSON STREET\nLIBERTY AVENUE\nNaN\n...\nUnspecified\nNaN\nNaN\nNaN\n4736985\nSedan\nSedan\nNaN\nNaN\nNaN\n\n\n37\n2024-06-30\n20:29\nMANHATTAN\n10027.0\n40.807667\n-73.949290\n(40.807667, -73.94929)\n7 AVENUE\nWEST 123 STREET\nNaN\n...\nUnspecified\nNaN\nNaN\nNaN\n4737257\nStation Wagon/Sport Utility Vehicle\nSedan\nNaN\nNaN\nNaN\n\n\n45\n2024-06-30\n19:56\nBROOKLYN\n11237.0\n40.709003\n-73.922340\n(40.709003, -73.92234)\nSCOTT AVENUE\nFLUSHING AVENUE\nNaN\n...\nUnspecified\nUnspecified\nNaN\nNaN\n4736995\nSedan\nStation Wagon/Sport Utility Vehicle\nMoped\nNaN\nNaN\n\n\n\n\n5 rows × 29 columns\n\n\n\n\n\n\n5.2.4 Merging DataFrames\nIn Pandas, the merge function is used to combine multiple DataFrames based\non a common column.\nThis is useful when working with multiple datasets that need to be joined for\nanalysis.\n\n5.2.4.1 Example: Merging Crash Data with Total Injuries per Borough\nSuppose we have a dataset containing crash details, and we want to analyze how\nthe total number of injuries in each borough relates to individual crashes.\nWe can achieve this by aggregating the total injuries per borough and merging it\nwith the crash dataset.\nThe following code:\n\nAggregates the total injuries per borough using .groupby().\nSelects relevant columns from the main dataset (collision_id, borough,\nnumber_of_persons_injured).\nMerges the aggregated injury data with the crash dataset using merge function on\nthe borough column.\n\n\n# Aggregate total injuries per borough\ndf_borough_info = df.groupby(\n          'borough', as_index=False\n          )['number_of_persons_injured'].sum()\ndf_borough_info.rename(\n          columns={'number_of_persons_injured': 'total_injuries'}, \n                  inplace=True)\n\n# Select relevant columns from the main dataset\ndf_crashes = df[['collision_id', 'borough', 'number_of_persons_injured']]\n\n# Merge crash data with total injuries per borough\ndf_merged = pd.merge(df_crashes, df_borough_info, on='borough', how='left')\n\n# Display first few rows of the merged dataset\ndf_merged.head()\n\n\n\n\n\n\n\n\ncollision_id\nborough\nnumber_of_persons_injured\ntotal_injuries\n\n\n\n\n0\n4737486\nBRONX\n0\n126.0\n\n\n1\n4737502\nBRONX\n0\n126.0\n\n\n2\n4737510\nNaN\n0\nNaN\n\n\n3\n4737499\nBROOKLYN\n1\n295.0\n\n\n4\n4736925\nNaN\n1\nNaN\n\n\n\n\n\n\n\nThe merged dataset now includes:\n\ncollision_id: Unique identifier for each crash.\nborough: The borough where the crash occurred.\nnumber_of_persons_injured: Number of injuries in a specific crash.\ntotal_injuries: The total number of injuries reported in that borough.\n\nThis merged dataset allows us to compare individual crash injuries with the\noverall injury trend in each borough, helping in further data analysis and\nvisualization.\n\n\n\n5.2.5 Data Visualization\nVisualizing data is crucial to understanding patterns and relationships within\nthe dataset. In this section, we will create one-variable tables (frequency\ntable), two-variable tables (contingency table).\n\n5.2.5.1 One-Variable Table\nA one-variable table (also called a frequency table) shows the distribution of\nvalues for a single categorical variable.\nFor example, we can count the number of crashes per borough:\n\nborough_counts = df['borough'].value_counts()\nborough_counts\n\nborough\nBROOKLYN         462\nQUEENS           381\nMANHATTAN        228\nBRONX            213\nSTATEN ISLAND     50\nName: count, dtype: int64\n\n\nThis table displays the number of accidents recorded in each borough of NYC. It\nhelps identify which borough has the highest accident frequency.\n\n\n5.2.5.2 Two-Variable Table\nA two-variable table (also called a contingency table) shows the relationship\nbetween two categorical variables.\nFor example, we can analyze the number of crashes per borough per day:\n\n# make pivot table\nborough_day_table = df.pivot_table(\n    index='crash_date',\n    columns='borough',\n    values='collision_id',\n    aggfunc='count'\n)\nborough_day_table\n\n\n\n\n\n\n\nborough\nBRONX\nBROOKLYN\nMANHATTAN\nQUEENS\nSTATEN ISLAND\n\n\ncrash_date\n\n\n\n\n\n\n\n\n\n2024-06-30\n24\n69\n40\n40\n8\n\n\n2024-07-01\n27\n62\n35\n45\n3\n\n\n2024-07-02\n19\n53\n37\n54\n8\n\n\n2024-07-03\n33\n59\n25\n58\n3\n\n\n2024-07-04\n27\n47\n31\n44\n8\n\n\n2024-07-05\n32\n64\n28\n58\n9\n\n\n2024-07-06\n27\n62\n16\n37\n6\n\n\n2024-07-07\n24\n46\n16\n45\n5\n\n\n\n\n\n\n\nThis table shows the number of accidents per borough for each day in the dataset.\n\n\n\n5.2.6 Data Cleaning and Transformation\nThis part is from the textbook “Python for Data Analysis: Data Wrangling with\nPandas, NumPy, and IPython.” Chapter 5, Third Edition by Wes McK- inney, O’Reilly\nMedia, 2022. https://wesmckinney.com/book/.\n\n5.2.6.1 Changing Data Types\nThe following functions in Pandas are used to convert data types. This is\nimportant when working with dates and categorical data to ensure proper analysis.\nFor example, we want to:\n\n# Convert crash date to datetime format\ndf['crash_date'] = pd.to_datetime(df['crash_date'])\ndf['crash_date']\n  \n# Convert ZIP code to string\ndf['zip_code'] = df['zip_code'].astype(str)\ndf['zip_code']\n\n0       10460.0\n1       10468.0\n2           nan\n3       11234.0\n4           nan\n         ...   \n1871        nan\n1872        nan\n1873    10468.0\n1874        nan\n1875        nan\nName: zip_code, Length: 1876, dtype: object\n\n\n\n\n5.2.6.2 Sorting Data\nWe can sort data by one or more columns:\n\n# Sort by date and time\ndf_sorted = df.sort_values(\n    by=['crash_date', 'crash_time'], \n    ascending=[True, True]\n)\ndf_sorted.head()\n\n\n\n\n\n\n\n\ncrash_date\ncrash_time\nborough\nzip_code\nlatitude\nlongitude\nlocation\non_street_name\ncross_street_name\noff_street_name\n...\ncontributing_factor_vehicle_2\ncontributing_factor_vehicle_3\ncontributing_factor_vehicle_4\ncontributing_factor_vehicle_5\ncollision_id\nvehicle_type_code_1\nvehicle_type_code_2\nvehicle_type_code_3\nvehicle_type_code_4\nvehicle_type_code_5\n\n\n\n\n90\n2024-06-30\n0:00\nNaN\nnan\n40.638268\n-74.07880\n(40.638268, -74.0788)\nVICTORY BOULEVARD\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\n4737312\nPick-up Truck\nNaN\nNaN\nNaN\nNaN\n\n\n180\n2024-06-30\n0:00\nMANHATTAN\n10004.0\n40.704834\n-74.01658\n(40.704834, -74.01658)\nNaN\nNaN\n17 BATTERY PLACE\n...\nUnspecified\nNaN\nNaN\nNaN\n4737900\nTaxi\nSedan\nNaN\nNaN\nNaN\n\n\n188\n2024-06-30\n0:00\nBRONX\n10455.0\n40.815754\n-73.89529\n(40.815754, -73.89529)\nLONGWOOD AVENUE\nBRUCKNER BOULEVARD\nNaN\n...\nUnspecified\nNaN\nNaN\nNaN\n4737875\nSedan\nSedan\nNaN\nNaN\nNaN\n\n\n248\n2024-06-30\n0:04\nNaN\nnan\n40.712490\n-73.96854\n(40.71249, -73.96854)\nWILLIAMSBURG BRIDGE\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\n4737093\nBike\nNaN\nNaN\nNaN\nNaN\n\n\n137\n2024-06-30\n0:05\nNaN\nnan\n40.793980\n-73.97229\n(40.79398, -73.97229)\nBROADWAY\nNaN\nNaN\n...\nUnspecified\nNaN\nNaN\nNaN\n4736952\nSedan\nBike\nNaN\nNaN\nNaN\n\n\n\n\n5 rows × 29 columns\n\n\n\nNow you can see the data is sorted by crash time.\n\n\n5.2.6.3 Handling Missing Data\nPandas provides methods for handling missing values.\nFor example, you can using the following codes to fix the missing data.\n\n# Check for missing values\ndf.isna().sum()\n\ncrash_date                          0\ncrash_time                          0\nborough                           542\nzip_code                            0\nlatitude                          131\nlongitude                         131\nlocation                          131\non_street_name                    546\ncross_street_name                 932\noff_street_name                  1330\nnumber_of_persons_injured           0\nnumber_of_persons_killed            0\nnumber_of_pedestrians_injured       0\nnumber_of_pedestrians_killed        0\nnumber_of_cyclist_injured           0\nnumber_of_cyclist_killed            0\nnumber_of_motorist_injured          0\nnumber_of_motorist_killed           0\ncontributing_factor_vehicle_1      11\ncontributing_factor_vehicle_2     450\ncontributing_factor_vehicle_3    1702\ncontributing_factor_vehicle_4    1824\ncontributing_factor_vehicle_5    1862\ncollision_id                        0\nvehicle_type_code_1                33\nvehicle_type_code_2               645\nvehicle_type_code_3              1714\nvehicle_type_code_4              1828\nvehicle_type_code_5              1862\ndtype: int64\n\n\n\n# Fill missing values with NaN\ndf.fillna(float('nan'), inplace=True)\ndf.fillna\n\n&lt;bound method NDFrame.fillna of      crash_date crash_time   borough zip_code   latitude  longitude  \\\n0    2024-06-30      23:17     BRONX  10460.0  40.838844 -73.878170   \n1    2024-06-30       8:30     BRONX  10468.0  40.862732 -73.903330   \n2    2024-06-30      20:47       NaN      nan  40.763630 -73.953300   \n3    2024-06-30      13:10  BROOKLYN  11234.0  40.617030 -73.919890   \n4    2024-06-30      16:42       NaN      nan        NaN        NaN   \n...         ...        ...       ...      ...        ...        ...   \n1871 2024-07-07      20:15       NaN      nan  40.677982 -73.791214   \n1872 2024-07-07      14:45       NaN      nan  40.843822 -73.927500   \n1873 2024-07-07      14:12     BRONX  10468.0  40.861084 -73.911490   \n1874 2024-07-07       1:40       NaN      nan  40.813114 -73.931470   \n1875 2024-07-07      19:00       NaN      nan  40.680960 -73.773575   \n\n                     location           on_street_name  cross_street_name  \\\n0      (40.838844, -73.87817)          EAST 177 STREET       DEVOE AVENUE   \n1      (40.862732, -73.90333)        WEST FORDHAM ROAD       GRAND AVENUE   \n2        (40.76363, -73.9533)                FDR DRIVE                NaN   \n3       (40.61703, -73.91989)           EAST 57 STREET           AVENUE O   \n4                         NaN                33 STREET  ASTORIA BOULEVARD   \n...                       ...                      ...                ...   \n1871  (40.677982, -73.791214)        SUTPHIN BOULEVARD         120 AVENUE   \n1872    (40.843822, -73.9275)  MAJOR DEEGAN EXPRESSWAY                NaN   \n1873   (40.861084, -73.91149)                      NaN                NaN   \n1874   (40.813114, -73.93147)  MAJOR DEEGAN EXPRESSWAY                NaN   \n1875   (40.68096, -73.773575)           MARSDEN STREET                NaN   \n\n              off_street_name  ...   contributing_factor_vehicle_2  \\\n0                         NaN  ...                     Unspecified   \n1                         NaN  ...                     Unspecified   \n2                         NaN  ...                             NaN   \n3                         NaN  ...  Driver Inattention/Distraction   \n4                         NaN  ...                     Unspecified   \n...                       ...  ...                             ...   \n1871                      NaN  ...                     Unspecified   \n1872                      NaN  ...                     Unspecified   \n1873  2258      HAMPDEN PLACE  ...                             NaN   \n1874                      NaN  ...                     Unspecified   \n1875                      NaN  ...                             NaN   \n\n      contributing_factor_vehicle_3  contributing_factor_vehicle_4  \\\n0                               NaN                            NaN   \n1                               NaN                            NaN   \n2                               NaN                            NaN   \n3                               NaN                            NaN   \n4                               NaN                            NaN   \n...                             ...                            ...   \n1871                            NaN                            NaN   \n1872                            NaN                            NaN   \n1873                            NaN                            NaN   \n1874                            NaN                            NaN   \n1875                            NaN                            NaN   \n\n      contributing_factor_vehicle_5  collision_id  vehicle_type_code_1  \\\n0                               NaN       4737486                Sedan   \n1                               NaN       4737502                Sedan   \n2                               NaN       4737510                Sedan   \n3                               NaN       4737499                Sedan   \n4                               NaN       4736925                Sedan   \n...                             ...           ...                  ...   \n1871                            NaN       4745391                Sedan   \n1872                            NaN       4746540                Sedan   \n1873                            NaN       4746320                Sedan   \n1874                            NaN       4747076                Sedan   \n1875                            NaN       4749679                Sedan   \n\n                      vehicle_type_code_2  vehicle_type_code_3  \\\n0                           Pick-up Truck                  NaN   \n1                                     NaN                  NaN   \n2                                     NaN                  NaN   \n3                                   Sedan                  NaN   \n4     Station Wagon/Sport Utility Vehicle                  NaN   \n...                                   ...                  ...   \n1871                                Sedan                  NaN   \n1872                                Sedan                  NaN   \n1873                                  NaN                  NaN   \n1874  Station Wagon/Sport Utility Vehicle                  NaN   \n1875                                  NaN                  NaN   \n\n     vehicle_type_code_4 vehicle_type_code_5  \n0                    NaN                 NaN  \n1                    NaN                 NaN  \n2                    NaN                 NaN  \n3                    NaN                 NaN  \n4                    NaN                 NaN  \n...                  ...                 ...  \n1871                 NaN                 NaN  \n1872                 NaN                 NaN  \n1873                 NaN                 NaN  \n1874                 NaN                 NaN  \n1875                 NaN                 NaN  \n\n[1876 rows x 29 columns]&gt;\n\n\n\n\n\n5.2.7 Conclusion\nPandas is a powerful tool for data analysis. Learning how to use Pandas will\nallow you to perform more advanced analytics and become more proficent in using\npython.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Manipulation</span>"
    ]
  },
  {
    "objectID": "manipulation.html#example-nyc-crash-data",
    "href": "manipulation.html#example-nyc-crash-data",
    "title": "5  Data Manipulation",
    "section": "5.3 Example: NYC Crash Data",
    "text": "5.3 Example: NYC Crash Data\nConsider a subset of the NYC Crash Data, which contains all NYC motor vehicle collisions data with documentation from NYC Open Data. We downloaded the crash data for the week of June 30, 2024, on February 12, 2025, in CSC format.\n\nimport numpy as np\nimport pandas as pd\n\n# Load the dataset\nfile_path = 'data/nyccrashes_2024w0630_by20250212.csv'\ndf = pd.read_csv(file_path,\n                 dtype={'LATITUDE': np.float32,\n                        'LONGITUDE': np.float32,\n                        'ZIP CODE': str})\n\n# Replace column names: convert to lowercase and replace spaces with underscores\ndf.columns = df.columns.str.lower().str.replace(' ', '_')\n\n# Check for missing values\ndf.isnull().sum()\n\ncrash_date                          0\ncrash_time                          0\nborough                           542\nzip_code                          542\nlatitude                          131\nlongitude                         131\nlocation                          131\non_street_name                    546\ncross_street_name                 932\noff_street_name                  1330\nnumber_of_persons_injured           0\nnumber_of_persons_killed            0\nnumber_of_pedestrians_injured       0\nnumber_of_pedestrians_killed        0\nnumber_of_cyclist_injured           0\nnumber_of_cyclist_killed            0\nnumber_of_motorist_injured          0\nnumber_of_motorist_killed           0\ncontributing_factor_vehicle_1      11\ncontributing_factor_vehicle_2     450\ncontributing_factor_vehicle_3    1702\ncontributing_factor_vehicle_4    1824\ncontributing_factor_vehicle_5    1862\ncollision_id                        0\nvehicle_type_code_1                33\nvehicle_type_code_2               645\nvehicle_type_code_3              1714\nvehicle_type_code_4              1828\nvehicle_type_code_5              1862\ndtype: int64\n\n\nTake a peek at the first five rows:\n\ndf.head()\n\n\n\n\n\n\n\n\ncrash_date\ncrash_time\nborough\nzip_code\nlatitude\nlongitude\nlocation\non_street_name\ncross_street_name\noff_street_name\n...\ncontributing_factor_vehicle_2\ncontributing_factor_vehicle_3\ncontributing_factor_vehicle_4\ncontributing_factor_vehicle_5\ncollision_id\nvehicle_type_code_1\nvehicle_type_code_2\nvehicle_type_code_3\nvehicle_type_code_4\nvehicle_type_code_5\n\n\n\n\n0\n06/30/2024\n23:17\nBRONX\n10460\n40.838844\n-73.878166\n(40.838844, -73.87817)\nEAST 177 STREET\nDEVOE AVENUE\nNaN\n...\nUnspecified\nNaN\nNaN\nNaN\n4737486\nSedan\nPick-up Truck\nNaN\nNaN\nNaN\n\n\n1\n06/30/2024\n8:30\nBRONX\n10468\n40.862732\n-73.903328\n(40.862732, -73.90333)\nWEST FORDHAM ROAD\nGRAND AVENUE\nNaN\n...\nUnspecified\nNaN\nNaN\nNaN\n4737502\nSedan\nNaN\nNaN\nNaN\nNaN\n\n\n2\n06/30/2024\n20:47\nNaN\nNaN\n40.763630\n-73.953300\n(40.76363, -73.9533)\nFDR DRIVE\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\n4737510\nSedan\nNaN\nNaN\nNaN\nNaN\n\n\n3\n06/30/2024\n13:10\nBROOKLYN\n11234\n40.617031\n-73.919891\n(40.61703, -73.91989)\nEAST 57 STREET\nAVENUE O\nNaN\n...\nDriver Inattention/Distraction\nNaN\nNaN\nNaN\n4737499\nSedan\nSedan\nNaN\nNaN\nNaN\n\n\n4\n06/30/2024\n16:42\nNaN\nNaN\nNaN\nNaN\nNaN\n33 STREET\nASTORIA BOULEVARD\nNaN\n...\nUnspecified\nNaN\nNaN\nNaN\n4736925\nSedan\nStation Wagon/Sport Utility Vehicle\nNaN\nNaN\nNaN\n\n\n\n\n5 rows × 29 columns\n\n\n\nA quick summary of the data types of the columns:\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1876 entries, 0 to 1875\nData columns (total 29 columns):\n #   Column                         Non-Null Count  Dtype  \n---  ------                         --------------  -----  \n 0   crash_date                     1876 non-null   object \n 1   crash_time                     1876 non-null   object \n 2   borough                        1334 non-null   object \n 3   zip_code                       1334 non-null   object \n 4   latitude                       1745 non-null   float32\n 5   longitude                      1745 non-null   float32\n 6   location                       1745 non-null   object \n 7   on_street_name                 1330 non-null   object \n 8   cross_street_name              944 non-null    object \n 9   off_street_name                546 non-null    object \n 10  number_of_persons_injured      1876 non-null   int64  \n 11  number_of_persons_killed       1876 non-null   int64  \n 12  number_of_pedestrians_injured  1876 non-null   int64  \n 13  number_of_pedestrians_killed   1876 non-null   int64  \n 14  number_of_cyclist_injured      1876 non-null   int64  \n 15  number_of_cyclist_killed       1876 non-null   int64  \n 16  number_of_motorist_injured     1876 non-null   int64  \n 17  number_of_motorist_killed      1876 non-null   int64  \n 18  contributing_factor_vehicle_1  1865 non-null   object \n 19  contributing_factor_vehicle_2  1426 non-null   object \n 20  contributing_factor_vehicle_3  174 non-null    object \n 21  contributing_factor_vehicle_4  52 non-null     object \n 22  contributing_factor_vehicle_5  14 non-null     object \n 23  collision_id                   1876 non-null   int64  \n 24  vehicle_type_code_1            1843 non-null   object \n 25  vehicle_type_code_2            1231 non-null   object \n 26  vehicle_type_code_3            162 non-null    object \n 27  vehicle_type_code_4            48 non-null     object \n 28  vehicle_type_code_5            14 non-null     object \ndtypes: float32(2), int64(9), object(18)\nmemory usage: 410.5+ KB\n\n\nNow we can do some cleaning after a quick browse.\n\n# Replace invalid coordinates (latitude=0, longitude=0 or NaN) with NaN\ndf.loc[(df['latitude'] == 0) & (df['longitude'] == 0), \n       ['latitude', 'longitude']] = pd.NA\ndf['latitude'] = df['latitude'].replace(0, pd.NA)\ndf['longitude'] = df['longitude'].replace(0, pd.NA)\n\n# Drop the redundant `latitute` and `longitude` columns\ndf = df.drop(columns=['location'])\n\n# Converting 'crash_date' and 'crash_time' columns into a single datetime column\ndf['crash_datetime'] = pd.to_datetime(df['crash_date'] + ' ' \n                       + df['crash_time'], format='%m/%d/%Y %H:%M', errors='coerce')\n\n# Drop the original 'crash_date' and 'crash_time' columns\ndf = df.drop(columns=['crash_date', 'crash_time'])\n\nLet’s get some basic frequency tables of borough and zip_code, whose values could be used to check their validity against the legitmate values.\n\n# Frequency table for 'borough' without filling missing values\nborough_freq = df['borough'].value_counts(dropna=False).reset_index()\nborough_freq.columns = ['borough', 'count']\n\n# Frequency table for 'zip_code' without filling missing values\nzip_code_freq = df['zip_code'].value_counts(dropna=False).reset_index()\nzip_code_freq.columns = ['zip_code', 'count']\nzip_code_freq\n\n\n\n\n\n\n\n\nzip_code\ncount\n\n\n\n\n0\nNaN\n542\n\n\n1\n11207\n31\n\n\n2\n11208\n28\n\n\n3\n11236\n28\n\n\n4\n11101\n23\n\n\n...\n...\n...\n\n\n164\n10470\n1\n\n\n165\n11040\n1\n\n\n166\n11693\n1\n\n\n167\n11415\n1\n\n\n168\n10025\n1\n\n\n\n\n169 rows × 2 columns\n\n\n\nA comprehensive list of ZIP codes by borough can be obtained, for example, from the New York City Department of Health’s UHF Codes. We can use this list to check the validity of the zip codes in the data.\n\n# List of valid NYC ZIP codes compiled from UHF codes\n# Define all_valid_zips based on the earlier extracted ZIP codes\nall_valid_zips = {\n    10463, 10471, 10466, 10469, 10470, 10475, 10458, 10467, 10468,\n    10461, 10462, 10464, 10465, 10472, 10473, 10453, 10457, 10460,\n    10451, 10452, 10456, 10454, 10455, 10459, 10474, 11211, 11222,\n    11201, 11205, 11215, 11217, 11231, 11213, 11212, 11216, 11233,\n    11238, 11207, 11208, 11220, 11232, 11204, 11218, 11219, 11230,\n    11203, 11210, 11225, 11226, 11234, 11236, 11239, 11209, 11214,\n    11228, 11223, 11224, 11229, 11235, 11206, 11221, 11237, 10031,\n    10032, 10033, 10034, 10040, 10026, 10027, 10030, 10037, 10039,\n    10029, 10035, 10023, 10024, 10025, 10021, 10028, 10044, 10128,\n    10001, 10011, 10018, 10019, 10020, 10036, 10010, 10016, 10017,\n    10022, 10012, 10013, 10014, 10002, 10003, 10009, 10004, 10005,\n    10006, 10007, 10038, 10280, 11101, 11102, 11103, 11104, 11105,\n    11106, 11368, 11369, 11370, 11372, 11373, 11377, 11378, 11354,\n    11355, 11356, 11357, 11358, 11359, 11360, 11361, 11362, 11363,\n    11364, 11374, 11375, 11379, 11385, 11365, 11366, 11367, 11414,\n    11415, 11416, 11417, 11418, 11419, 11420, 11421, 11412, 11423,\n    11432, 11433, 11434, 11435, 11436, 11004, 11005, 11411, 11413,\n    11422, 11426, 11427, 11428, 11429, 11691, 11692, 11693, 11694,\n    11695, 11697, 10302, 10303, 10310, 10301, 10304, 10305, 10314,\n    10306, 10307, 10308, 10309, 10312\n}\n\n    \n# Convert set to list of strings\nall_valid_zips = list(map(str, all_valid_zips))\n\n# Identify invalid ZIP codes (including NaN)\ninvalid_zips = df[\n    df['zip_code'].isna() | ~df['zip_code'].isin(all_valid_zips)\n    ]['zip_code']\n\n# Calculate frequency of invalid ZIP codes\ninvalid_zip_freq = invalid_zips.value_counts(dropna=False).reset_index()\ninvalid_zip_freq.columns = ['zip_code', 'frequency']\n\ninvalid_zip_freq\n\n\n\n\n\n\n\n\nzip_code\nfrequency\n\n\n\n\n0\nNaN\n542\n\n\n1\n10065\n7\n\n\n2\n11249\n4\n\n\n3\n10112\n1\n\n\n4\n11040\n1\n\n\n\n\n\n\n\nAs it turns out, the collection of valid NYC zip codes differ from different sources. From United States Zip Codes, 10065 appears to be a valid NYC zip code. Under this circumstance, it might be safer to not remove any zip code from the data.\nTo be safe, let’s concatenate valid and invalid zips.\n\n# Convert invalid ZIP codes to a set of strings\ninvalid_zips_set = set(invalid_zip_freq['zip_code'].dropna().astype(str))\n\n# Convert all_valid_zips to a set of strings (if not already)\nvalid_zips_set = set(map(str, all_valid_zips))\n\n# Merge both sets\nmerged_zips = invalid_zips_set | valid_zips_set  # Union of both sets\n\nAre missing in zip code and borough always co-occur?\n\n# Check if missing values in 'zip_code' and 'borough' always co-occur\n# Count rows where both are missing\nmissing_cooccur = df[['zip_code', 'borough']].isnull().all(axis=1).sum()\n# Count total missing in 'zip_code' and 'borough', respectively\ntotal_missing_zip_code = df['zip_code'].isnull().sum()\ntotal_missing_borough = df['borough'].isnull().sum()\n\n# If missing in both columns always co-occur, the number of missing\n# co-occurrences should be equal to the total missing in either column\nnp.array([missing_cooccur, total_missing_zip_code, total_missing_borough])\n\narray([542, 542, 542])\n\n\nAre there cases where zip_code and borough are missing but the geo codes are not missing? If so, fill in zip_code and borough using the geo codes by reverse geocoding.\nFirst make sure geopy is installed.\npip install geopy\nNow we use model Nominatim in package geopy to reverse geocode.\n\nfrom geopy.geocoders import Nominatim\nimport time\n\n# Initialize the geocoder; the `user_agent` is your identifier \n# when using the service. Be mindful not to crash the server\n# by unlimited number of queries, especially invalid code.\ngeolocator = Nominatim(user_agent=\"jyGeopyTry\")\n\nWe write a function to do the reverse geocoding given lattitude and longitude.\n\n# Function to fill missing zip_code\ndef get_zip_code(latitude, longitude):\n    try:\n        location = geolocator.reverse((latitude, longitude), timeout=10)\n        if location:\n            address = location.raw['address']\n            zip_code = address.get('postcode', None)\n            return zip_code\n        else:\n            return None\n    except Exception as e:\n        print(f\"Error: {e} for coordinates {latitude}, {longitude}\")\n        return None\n    finally:\n        time.sleep(1)  # Delay to avoid overwhelming the service\n\nLet’s try it out:\n\n# Example usage\nlatitude = 40.730610\nlongitude = -73.935242\nget_zip_code(latitude, longitude)\n\n'11101'\n\n\nThe function get_zip_code can then be applied to rows where zip code is missing but geocodes are not to fill the missing zip code.\nOnce zip code is known, figuring out burough is simple because valid zip codes from each borough are known.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Manipulation</span>"
    ]
  },
  {
    "objectID": "manipulation.html#accessing-census-data",
    "href": "manipulation.html#accessing-census-data",
    "title": "5  Data Manipulation",
    "section": "5.4 Accessing Census Data",
    "text": "5.4 Accessing Census Data\nThe U.S. Census Bureau provides extensive demographic, economic, and social data through multiple surveys, including the decennial Census, the American Community Survey (ACS), and the Economic Census. These datasets offer valuable insights into population trends, economic conditions, and community characteristics at multiple geographic levels.\nThere are several ways to access Census data:\n\nCensus API: The Census API allows programmatic access to various datasets. It supports queries for different geographic levels and time periods.\ndata.census.gov: The official web interface for searching and downloading Census data.\nIPUMS USA: Provides harmonized microdata for longitudinal research. Available at IPUMS USA.\nNHGIS: Offers historical Census data with geographic information. Visit NHGIS.\n\nIn addition, Python tools simplify API access and data retrieval.\n\n5.4.1 Python Tools for Accessing Census Data\nSeveral Python libraries facilitate Census data retrieval:\n\ncensusapi: The official API wrapper for direct access to Census datasets.\ncensus: A high-level interface to the Census API, supporting ACS and decennial Census queries. See census on PyPI.\ncensusdata: A package for downloading and processing Census data directly in Python. Available at censusdata documentation.\nuszipcode: A library for retrieving Census and geographic information by ZIP code. See uszipcode on PyPI.\n\n\n\n5.4.2 Zip-Code Level for NYC Crash Data\nNow that we have NYC crash data, we might want to analyze patterns at the zip-code level to understand whether certain demographic or economic factors correlate with traffic incidents. While the crash dataset provides details about individual accidents, such as location, time, and severity, it does not contain contextual information about the neighborhoods where these crashes occur.\nTo perform meaningful zip-code-level analysis, we need additional data sources that provide relevant demographic, economic, and geographic variables. For example, understanding whether high-income areas experience fewer accidents, or whether population density influences crash frequency, requires integrating Census data. Key variables such as population size, median household income, employment rate, and population density can provide valuable context for interpreting crash trends across different zip codes.\nSince the Census Bureau provides detailed estimates for these variables at the zip-code level, we can use the Census API or other tools to retrieve relevant data and merge it with the NYC crash dataset. To access the Census API, you need an API key, which is free and easy to obtain. Visit the Census API Request page and submit your email address to receive a key. Once you have the key, you must include it in your API requests to access Census data. The following demonstration assumes that you have registered, obtained your API key, and saved it in a file called censusAPIkey.txt.\n\n# Import modules\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport geopandas as gpd\nfrom census import Census\nfrom us import states\nimport os\nimport io\n\napi_key = open(\"censusAPIkey.txt\").read().strip()\nc = Census(api_key)\n\nSuppose that we want to get some basic info from ACS data of the year of 2023 for all the NYC zip codes. The variable names can be found in the ACS variable documentation.\n\nACS_YEAR = 2023\nACS_DATASET = \"acs/acs5\"\n\n# Important ACS variables (including land area for density calculation)\nACS_VARIABLES = {\n    \"B01003_001E\": \"Total Population\",\n    \"B19013_001E\": \"Median Household Income\",\n    \"B02001_002E\": \"White Population\",\n    \"B02001_003E\": \"Black Population\",\n    \"B02001_005E\": \"Asian Population\",\n    \"B15003_022E\": \"Bachelor’s Degree Holders\",\n    \"B15003_025E\": \"Graduate Degree Holders\",\n    \"B23025_002E\": \"Labor Force\",\n    \"B23025_005E\": \"Unemployed\",\n    \"B25077_001E\": \"Median Home Value\"\n}\n\n# Convert set to list of strings\nmerged_zips = list(map(str, merged_zips))\n\nLet’s set up the query to request the ACS data, and process the returned data.\n\nacs_data = c.acs5.get(\n    list(ACS_VARIABLES.keys()), \n    {'for': f'zip code tabulation area:{\",\".join(merged_zips)}'}\n    )\n\n# Convert to DataFrame\ndf_acs = pd.DataFrame(acs_data)\n\n# Rename columns\ndf_acs.rename(columns=ACS_VARIABLES, inplace=True)\ndf_acs.rename(columns={\"zip code tabulation area\": \"ZIP Code\"}, inplace=True)\n\nWe could save the ACS data df_acs in feather format (see next Section).\n#| eval: false\ndf_acs.to_feather(\"data/acs2023.feather\")\nThe population density could be an important factor for crash likelihood. To obtain the population densities, we need the areas of the zip codes. The shape files can be obtained from NYC Open Data.\n\nimport requests\nimport zipfile\nimport geopandas as gpd\n\n# Define the NYC MODZCTA shapefile URL and extraction directory\nshapefile_url = \"https://data.cityofnewyork.us/api/geospatial/pri4-ifjk?method=export&format=Shapefile\"\nextract_dir = \"MODZCTA_Shapefile\"\n\n# Create the directory if it doesn't exist\nos.makedirs(extract_dir, exist_ok=True)\n\n# Step 1: Download and extract the shapefile\nprint(\"Downloading MODZCTA shapefile...\")\nresponse = requests.get(shapefile_url)\nwith zipfile.ZipFile(io.BytesIO(response.content), \"r\") as z:\n    z.extractall(extract_dir)\n\nprint(f\"Shapefile extracted to: {extract_dir}\")\n\nDownloading MODZCTA shapefile...\nShapefile extracted to: MODZCTA_Shapefile\n\n\nNow we process the shape file to calculate the areas of the polygons.\n\n# Step 2: Automatically detect the correct .shp file\nshapefile_path = None\nfor file in os.listdir(extract_dir):\n    if file.endswith(\".shp\"):\n        shapefile_path = os.path.join(extract_dir, file)\n        break  # Use the first .shp file found\n\nif not shapefile_path:\n    raise FileNotFoundError(\"No .shp file found in extracted directory.\")\n\nprint(f\"Using shapefile: {shapefile_path}\")\n\n# Step 3: Load the shapefile into GeoPandas\ngdf = gpd.read_file(shapefile_path)\n\n# Step 4: Convert to CRS with meters for accurate area calculation\ngdf = gdf.to_crs(epsg=3857)\n\n# Step 5: Compute land area in square miles\ngdf['land_area_sq_miles'] = gdf['geometry'].area / 2_589_988.11\n# 1 square mile = 2,589,988.11 square meters\n\nprint(gdf[['modzcta', 'land_area_sq_miles']].head())\n\nUsing shapefile: MODZCTA_Shapefile/geo_export_1daca795-0288-4cf1-be6c-e69d6ffefeee.shp\n  modzcta  land_area_sq_miles\n0   10001            1.153516\n1   10002            1.534509\n2   10003            1.008318\n3   10026            0.581848\n4   10004            0.256876\n\n\nLet’s export this data frame for future usage in feather format (see next Section).\n\ngdf[['modzcta', 'land_area_sq_miles']].to_feather('data/nyc_zip_areas.feather')\n\nNow we are ready to merge the two data frames.\n\n# Merge ACS data (`df_acs`) directly with MODZCTA land area (`gdf`)\ngdf = gdf.merge(df_acs, left_on='modzcta', right_on='ZIP Code', how='left')\n\n# Calculate Population Density (people per square mile)\ngdf['popdensity_per_sq_mile'] = (\n    gdf['Total Population'] / gdf['land_area_sq_miles']\n    )\n\n# Display first few rows\nprint(gdf[['modzcta', 'Total Population', 'land_area_sq_miles',\n    'popdensity_per_sq_mile']].head())\n\n  modzcta  Total Population  land_area_sq_miles  popdensity_per_sq_mile\n0   10001           27004.0            1.153516            23410.171200\n1   10002           76518.0            1.534509            49864.797219\n2   10003           53877.0            1.008318            53432.563117\n3   10026           38265.0            0.581848            65764.650082\n4   10004            4579.0            0.256876            17825.700993\n\n\nSome visualization of population density.\n\nimport matplotlib.pyplot as plt\nimport geopandas as gpd\n\n# Set up figure and axis\nfig, ax = plt.subplots(figsize=(10, 12))\n\n# Plot the choropleth map\ngdf.plot(column='popdensity_per_sq_mile', \n         cmap='viridis',  # Use a visually appealing color map\n         linewidth=0.8, \n         edgecolor='black',\n         legend=True,\n         legend_kwds={'label': \"Population Density (per sq mile)\",\n             'orientation': \"horizontal\"},\n         ax=ax)\n\n# Add a title\nax.set_title(\"Population Density by ZCTA in NYC\", fontsize=14)\n\n# Remove axes\nax.set_xticks([])\nax.set_yticks([])\nax.set_frame_on(False)\n\n# Show the plot\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Manipulation</span>"
    ]
  },
  {
    "objectID": "manipulation.html#cross-platform-data-format-arrow",
    "href": "manipulation.html#cross-platform-data-format-arrow",
    "title": "5  Data Manipulation",
    "section": "5.5 Cross-platform Data Format Arrow",
    "text": "5.5 Cross-platform Data Format Arrow\nThe CSV format (and related formats like TSV - tab-separated values) for data tables is ubiquitous, convenient, and can be read or written by many different data analysis environments, including spreadsheets. An advantage of the textual representation of the data in a CSV file is that the entire data table, or portions of it, can be previewed in a text editor. However, the textual representation can be ambiguous and inconsistent. The format of a particular column: Boolean, integer, floating-point, text, factor, etc. must be inferred from text representation, often at the expense of reading the entire file before these inferences can be made. Experienced data scientists are aware that a substantial part of an analysis or report generation is often the “data cleaning” involved in preparing the data for analysis. This can be an open-ended task — it required numerous trial-and-error iterations to create the list of different missing data representations we use for the sample CSV file and even now we are not sure we have them all.\nTo read and export data efficiently, leveraging the Apache Arrow library can significantly improve performance and storage efficiency, especially with large datasets. The IPC (Inter-Process Communication) file format in the context of Apache Arrow is a key component for efficiently sharing data between different processes, potentially written in different programming languages. Arrow’s IPC mechanism is designed around two main file formats:\n\nStream Format: For sending an arbitrary length sequence of Arrow record batches (tables). The stream format is useful for real-time data exchange where the size of the data is not known upfront and can grow indefinitely.\nFile (or Feather) Format: Optimized for storage and memory-mapped access, allowing for fast random access to different sections of the data. This format is ideal for scenarios where the entire dataset is available upfront and can be stored in a file system for repeated reads and writes.\n\nApache Arrow provides a columnar memory format for flat and hierarchical data, optimized for efficient data analytics. It can be used in Python through the pyarrow package. Here’s how you can use Arrow to read, manipulate, and export data, including a demonstration of storage savings.\nFirst, ensure you have pyarrow installed on your computer (and preferrably, in your current virtual environment):\npip install pyarrow\nFeather is a fast, lightweight, and easy-to-use binary file format for storing data frames, optimized for speed and efficiency, particularly for IPC and data sharing between Python and R or Julia.\nThe following code processes the cleaned data in CSV format from Mohammad Mundiwala and write out in Arrow format.\n\nimport pandas as pd\n\n# Read CSV, ensuring 'zip_code' is string and 'crash_datetime' is parsed as datetime\ndf = pd.read_csv('data/nyc_crashes_cleaned_mm.csv',\n                 dtype={'zip_code': str},\n                 parse_dates=['crash_datetime'])\n\n# Drop the 'date' and 'time' columns\ndf = df.drop(columns=['crash_date', 'crash_time'])\n\n# Move 'crash_datetime' to the first column\ndf = df[['crash_datetime'] + df.drop(columns=['crash_datetime']).columns.tolist()]\n\ndf['zip_code'] = df['zip_code'].astype(str).str.rstrip('.0')\n\ndf = df.sort_values(by='crash_datetime')\n\ndf.to_feather('nyccrashes_cleaned.feather')\n\nLet’s compare the file sizes of the feather format and the CSV format.\n\nimport os\n\n# File paths\ncsv_file = 'data/nyccrashes_2024w0630_by20250212.csv'\nfeather_file = 'data/nyccrashes_cleaned.feather'\n\n# Get file sizes in bytes\ncsv_size = os.path.getsize(csv_file)\nfeather_size = os.path.getsize(feather_file)\n\n# Convert bytes to a more readable format (e.g., MB)\ncsv_size_mb = csv_size / (1024 * 1024)\nfeather_size_mb = feather_size / (1024 * 1024)\n\n# Print the file sizes\nprint(f\"CSV file size: {csv_size_mb:.2f} MB\")\nprint(f\"Feather file size: {feather_size_mb:.2f} MB\")\n\nCSV file size: 0.34 MB\nFeather file size: 0.19 MB\n\n\nRead the feather file back in:\n#| eval: false\ndff = pd.read_feather(\"data/nyccrashes_cleaned.feather\")\ndff.shape",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Manipulation</span>"
    ]
  },
  {
    "objectID": "manipulation.html#database-with-sql",
    "href": "manipulation.html#database-with-sql",
    "title": "5  Data Manipulation",
    "section": "5.6 Database with SQL",
    "text": "5.6 Database with SQL\n\nThis section was prepared by Alyssa Horn, a junior Applied Data Analysis major with a domain in Public Policy/Management.\nThis section explores database operations using SQL in Python, focusing on the contributing_factor_vehicle_1 column of the NYC crash dataset. We’ll use Python’s sqlite3 and pandas libraries for database interaction.\n\n\n5.6.1 What is a Database?\n\nA collection of related data.\nOrganized in a way that allows computers to efficiently store, retrieve, and manipulate information.\n“Filing system” for large amounts of data that can be easily accessed and analyzed\n\n\n5.6.1.1 Non-relational Databases\n\nIdeal for handling large volumes of unstructured or semi-structured data.\nDoes not store data in a traditional tabular format with rows and columns like a relational database.\nAllows for more flexible data structures like documents, key-value pairs, graphs, or columns.\n\n\n\n5.6.1.2 Relational Databases\n\nStores data in tables with rows (records) and columns (attributes).\nEach table has a primary key to uniquely identify records.\nAllows you to easily access and understand how different pieces of data are connected to each other.\nExample: In a phone book, each contact has a unique ID, name, phone number, and address.\n\n\n\n\n5.6.2 What is SQL?\n\nStructured Query Language for managing and querying relational databases.\nHelps you store, retrieve, update, and delete data easily using simple commands in Python.\nUsing sqlite3, you can run SQL queries directly from your Python code to interact with your database seamlessly.\n\n\n5.6.2.1 CRUD Model\n\nThe four most basic operations that can be performed with most traditional database systems and they are the backbone for interacting with any database.\n\nCreate: Insert new records.\nRead: Retrieve data.\nUpdate: Modify existing records.\nDelete: Remove records.\n\n\n\n\n5.6.2.2 What can SQL do?\n\nExecute queries against a database\nRetrieve data from a database\nInsert records in a database\nUpdate records in a database\nDelete records from a database\nCreate new databases\nCreate new tables in a database\nCreate stored procedures in a database\nCreate views in a database\nSet permissions on tables, procedures, and views\n\n\n\n5.6.2.3 Key Statements\n\nCreate a cursor object to interact with the database\ncursor.execute executes a single SQL statement\nconn.commit saves all changes made\nquery = requests specific information from database\n\n\n\n\n5.6.3 Setting up the Database\n\n5.6.3.1 Read in the datatset and store dataframe as SQL table\nWe start by importing necessary packages and reading in the cleaned nyccrashes feather.\n\nUse data.to_sql to store dataframe as an SQL table.\n\n\nimport sqlite3\nimport pandas as pd\n\n# Create a database and load the NYC crash data\ndb_path = 'nyc_crash.db'\nconn = sqlite3.connect(db_path)\n#The conn object acts as a bridge between Python and the database,\n#allowing you to execute SQL queries and manage data.\n\n# Load Feather\ndata = pd.read_feather(\"data/nyccrashes_cleaned.feather\")\n\n# create crash_date and crash_time columns\ndata[\"crash_date\"] = pd.to_datetime(data[\"crash_datetime\"]).dt.date\ndata[\"crash_time\"] = pd.to_datetime(data[\"crash_datetime\"]).dt.strftime(\"%H:%M:%S\")\n\n# Drop the original datetime column (optional)\ndata.drop(columns=[\"crash_datetime\"], inplace=True)\n\n# Store DataFrame as a SQL table\ndata.to_sql('nyc_crashes', conn, if_exists='replace', index=False)\n\n1875\n\n\n\n\n5.6.3.2 Display the Table\nWe can display the table by querying all (or some) of the data and using the .head() command\n\n# Query to select all data (or limit rows to avoid overload)\nquery = \"SELECT * FROM nyc_crashes LIMIT 10;\"\n\n# Load the data into a pandas DataFrame\nnyc_crashes_data = pd.read_sql_query(query, conn)\n\n# Display the DataFrame\nnyc_crashes_data.head(5)\n\n\n\n\n\n\n\n\nborough\nzip_code\nlatitude\nlongitude\nlocation\non_street_name\ncross_street_name\noff_street_name\nnumber_of_persons_injured\nnumber_of_persons_killed\n...\ncontributing_factor_vehicle_4\ncontributing_factor_vehicle_5\ncollision_id\nvehicle_type_code_1\nvehicle_type_code_2\nvehicle_type_code_3\nvehicle_type_code_4\nvehicle_type_code_5\ncrash_date\ncrash_time\n\n\n\n\n0\nNone\nNaN\nNaN\nNaN\n(0.0, 0.0)\nNone\nNone\nGOLD STREET\n0\n0\n...\nNone\nNone\n4736746\nSedan\nSedan\nNone\nNone\nNone\n2024-06-30\n17:30:00\n\n\n1\nNone\nNaN\nNaN\nNaN\nNone\nBELT PARKWAY RAMP\nNone\nNone\n0\n0\n...\nNone\nNone\n4736768\nStation Wagon/Sport Utility Vehicle\nStation Wagon/Sport Utility Vehicle\nNone\nNone\nNone\n2024-06-30\n00:32:00\n\n\n2\nBROOKLYN\n11235.0\n40.58106\n-73.96744\n(40.58106, -73.96744)\nNone\nNone\n2797 OCEAN PARKWAY\n0\n0\n...\nNone\nNone\n4737060\nStation Wagon/Sport Utility Vehicle\nNone\nNone\nNone\nNone\n2024-06-30\n07:05:00\n\n\n3\nMANHATTAN\n10021.0\n40.76363\n-73.95330\n(40.76363, -73.9533)\nFDR DRIVE\nNone\nNone\n0\n0\n...\nNone\nNone\n4737510\nSedan\nNone\nNone\nNone\nNone\n2024-06-30\n20:47:00\n\n\n4\nBROOKLYN\n11222.0\n40.73046\n-73.95149\n(40.73046, -73.95149)\nGREENPOINT AVENUE\nMC GUINNESS BOULEVARD\nNone\n0\n0\n...\nNone\nNone\n4736759\nBus\nBox Truck\nNone\nNone\nNone\n2024-06-30\n10:14:00\n\n\n\n\n5 rows × 29 columns\n\n\n\n\n\n\n5.6.4 Normalizing the Database with a Lookup Table\n\n5.6.4.1 Create the lookup table\nCreate the lookup table using the create table command with the corresponding column names.\n\n# Connect to the SQLite database\ncursor = conn.cursor()\n\ncursor.execute('''\nCREATE TABLE IF NOT EXISTS contributing_factor_lookup (\n    factor_id INTEGER PRIMARY KEY AUTOINCREMENT,\n    factor_description TEXT UNIQUE\n)\n''')\nprint(\"Lookup table created successfully.\")\n\nLookup table created successfully.\n\n\n\n\n5.6.4.2 Populate the Lookup Table with Distinct Values\nPopulate the lookup table with the values contained in contributing_factor_vehicle_1.\n\ncursor.execute('''\nINSERT OR IGNORE INTO contributing_factor_lookup (factor_description)\nSELECT DISTINCT contributing_factor_vehicle_1\nFROM nyc_crashes\nWHERE contributing_factor_vehicle_1 IS NOT NULL;\n''')\nprint(\"Lookup table populated with distinct contributing factors.\")\n\nLookup table populated with distinct contributing factors.\n\n\n\n\n5.6.4.3 Update the Original Table to Include factor_id\nUse ALTER TABLE to add factor_id column into original table.\n\ncursor.execute('''\nALTER TABLE nyc_crashes ADD COLUMN factor_id INTEGER;\n''')\nprint(\"Added 'factor_id' column to nyc_crashes table.\")\n\nAdded 'factor_id' column to nyc_crashes table.\n\n\n\n\n5.6.4.4 Update the Original Table with Corresponding Codes\nUse UPDATE command to update table with factor descriptions.\n\ncursor.execute('''\nUPDATE nyc_crashes\nSET factor_id = (\n    SELECT factor_id \n    FROM contributing_factor_lookup \n    WHERE contributing_factor_vehicle_1 = factor_description\n)\nWHERE contributing_factor_vehicle_1 IS NOT NULL;\n''')\nprint(\"Updated nyc_crashes with corresponding factor IDs.\")\n\nUpdated nyc_crashes with corresponding factor IDs.\n\n\n\n\n5.6.4.5 Query with a Join to Retrieve Full Descriptions\nUse JOIN command to recieve contributing factor descriptions from factor_id.\n\nquery = '''\nSELECT n.*, l.factor_description\nFROM nyc_crashes n\nJOIN contributing_factor_lookup l ON n.factor_id = l.factor_id\nLIMIT 10;\n'''\n\n# Load the data into a pandas DataFrame\nresult_df = pd.read_sql_query(query, conn)\n\n# Commit changes\nconn.commit()\n\nresult_df.head()\n\n\n\n\n\n\n\n\nborough\nzip_code\nlatitude\nlongitude\nlocation\non_street_name\ncross_street_name\noff_street_name\nnumber_of_persons_injured\nnumber_of_persons_killed\n...\ncollision_id\nvehicle_type_code_1\nvehicle_type_code_2\nvehicle_type_code_3\nvehicle_type_code_4\nvehicle_type_code_5\ncrash_date\ncrash_time\nfactor_id\nfactor_description\n\n\n\n\n0\nNone\nNaN\nNaN\nNaN\n(0.0, 0.0)\nNone\nNone\nGOLD STREET\n0\n0\n...\n4736746\nSedan\nSedan\nNone\nNone\nNone\n2024-06-30\n17:30:00\n1\nPassing Too Closely\n\n\n1\nNone\nNaN\nNaN\nNaN\nNone\nBELT PARKWAY RAMP\nNone\nNone\n0\n0\n...\n4736768\nStation Wagon/Sport Utility Vehicle\nStation Wagon/Sport Utility Vehicle\nNone\nNone\nNone\n2024-06-30\n00:32:00\n2\nUnspecified\n\n\n2\nBROOKLYN\n11235.0\n40.58106\n-73.96744\n(40.58106, -73.96744)\nNone\nNone\n2797 OCEAN PARKWAY\n0\n0\n...\n4737060\nStation Wagon/Sport Utility Vehicle\nNone\nNone\nNone\nNone\n2024-06-30\n07:05:00\n2\nUnspecified\n\n\n3\nMANHATTAN\n10021.0\n40.76363\n-73.95330\n(40.76363, -73.9533)\nFDR DRIVE\nNone\nNone\n0\n0\n...\n4737510\nSedan\nNone\nNone\nNone\nNone\n2024-06-30\n20:47:00\n2\nUnspecified\n\n\n4\nBROOKLYN\n11222.0\n40.73046\n-73.95149\n(40.73046, -73.95149)\nGREENPOINT AVENUE\nMC GUINNESS BOULEVARD\nNone\n0\n0\n...\n4736759\nBus\nBox Truck\nNone\nNone\nNone\n2024-06-30\n10:14:00\n1\nPassing Too Closely\n\n\n\n\n5 rows × 31 columns\n\n\n\n\n\n5.6.4.6 Display Table With factor.id\nSince we added factor_id column to dataframe, we can now display the table including the factor_id column using a query.\n\n# Query to select all data (or limit rows to avoid overload)\nquery = \"SELECT * FROM nyc_crashes LIMIT 10;\"\n\n# Load the data into a pandas DataFrame\nnyc_crashes_data = pd.read_sql_query(query, conn)\n\n# Display the DataFrame\nnyc_crashes_data.head()\n\n\n\n\n\n\n\n\nborough\nzip_code\nlatitude\nlongitude\nlocation\non_street_name\ncross_street_name\noff_street_name\nnumber_of_persons_injured\nnumber_of_persons_killed\n...\ncontributing_factor_vehicle_5\ncollision_id\nvehicle_type_code_1\nvehicle_type_code_2\nvehicle_type_code_3\nvehicle_type_code_4\nvehicle_type_code_5\ncrash_date\ncrash_time\nfactor_id\n\n\n\n\n0\nNone\nNaN\nNaN\nNaN\n(0.0, 0.0)\nNone\nNone\nGOLD STREET\n0\n0\n...\nNone\n4736746\nSedan\nSedan\nNone\nNone\nNone\n2024-06-30\n17:30:00\n1\n\n\n1\nNone\nNaN\nNaN\nNaN\nNone\nBELT PARKWAY RAMP\nNone\nNone\n0\n0\n...\nNone\n4736768\nStation Wagon/Sport Utility Vehicle\nStation Wagon/Sport Utility Vehicle\nNone\nNone\nNone\n2024-06-30\n00:32:00\n2\n\n\n2\nBROOKLYN\n11235.0\n40.58106\n-73.96744\n(40.58106, -73.96744)\nNone\nNone\n2797 OCEAN PARKWAY\n0\n0\n...\nNone\n4737060\nStation Wagon/Sport Utility Vehicle\nNone\nNone\nNone\nNone\n2024-06-30\n07:05:00\n2\n\n\n3\nMANHATTAN\n10021.0\n40.76363\n-73.95330\n(40.76363, -73.9533)\nFDR DRIVE\nNone\nNone\n0\n0\n...\nNone\n4737510\nSedan\nNone\nNone\nNone\nNone\n2024-06-30\n20:47:00\n2\n\n\n4\nBROOKLYN\n11222.0\n40.73046\n-73.95149\n(40.73046, -73.95149)\nGREENPOINT AVENUE\nMC GUINNESS BOULEVARD\nNone\n0\n0\n...\nNone\n4736759\nBus\nBox Truck\nNone\nNone\nNone\n2024-06-30\n10:14:00\n1\n\n\n\n\n5 rows × 30 columns\n\n\n\n\n\n\n5.6.5 Inserting Data\nNew records (rows) can be added into a database table. The INSERT INTO statement is used to accomplish this task. When you insert data, you provide values for one or more columns in the table.\n\nINSERT INTO table_name (columns) VALUES (values\n\nInsert a new crash record into the nyc_crashes table with the date 06/30/2024, time 10:15, location BROOKLYN, and contributing factor “Driver Inattention/Distraction”.\n\ncursor = conn.cursor()\n\n# Adds a crash on 06/30/2024 at 10:15 in\n# Brooklyn due to Inattention/Distraction\ncursor.execute(\"\"\"\nINSERT INTO nyc_crashes (crash_date, crash_time, \nborough, contributing_factor_vehicle_1)\nVALUES ('2024-06-30', '10:15:00', 'BROOKLYN',\n'Driver Inattention/Distraction')\n\"\"\")\n\nconn.commit()\n\n\n5.6.5.1 Verify the record exists\nWe can use a query for a specific data point to verify if addition was successful.\n\nquery_before = \"\"\"\nSELECT * FROM nyc_crashes \nWHERE crash_date = '2024-06-30' \nAND crash_time = '10:15:00' \nAND borough = 'BROOKLYN';\n\"\"\"\n\nbefore_deletion = pd.read_sql_query(query_before, conn)\nprint(\"Before Deletion:\")\nbefore_deletion\n\nBefore Deletion:\n\n\n\n\n\n\n\n\n\nborough\nzip_code\nlatitude\nlongitude\nlocation\non_street_name\ncross_street_name\noff_street_name\nnumber_of_persons_injured\nnumber_of_persons_killed\n...\ncontributing_factor_vehicle_5\ncollision_id\nvehicle_type_code_1\nvehicle_type_code_2\nvehicle_type_code_3\nvehicle_type_code_4\nvehicle_type_code_5\ncrash_date\ncrash_time\nfactor_id\n\n\n\n\n0\nBROOKLYN\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\n...\nNone\nNone\nNone\nNone\nNone\nNone\nNone\n2024-06-30\n10:15:00\nNone\n\n\n\n\n1 rows × 30 columns\n\n\n\n\n\n\n5.6.6 Deleting Data\nUse DELETE FROM statement to delete data\n\ndelete_query = \"\"\"\nDELETE FROM nyc_crashes\nWHERE crash_date = '2024-06-30' \nAND crash_time = '10:15:00' \nAND borough = 'BROOKLYN'\nAND contributing_factor_vehicle_1 = 'Driver Inattention/Distraction';\n\"\"\"\n\ncursor.execute(delete_query)\nconn.commit()\n\n\n5.6.6.1 Verify the deletion\nWe can use a query for a specific data point to verify if deletion was successful.\n\nquery_after = \"\"\"\nSELECT * FROM nyc_crashes \nWHERE crash_date = '2024-06-30' \nAND crash_time = '10:15:00' \nAND borough = 'BROOKLYN';\n\"\"\"\n\nafter_deletion = pd.read_sql_query(query_after, conn)\nprint(\"After Deletion:\")\nafter_deletion\n\nAfter Deletion:\n\n\n\n\n\n\n\n\n\nborough\nzip_code\nlatitude\nlongitude\nlocation\non_street_name\ncross_street_name\noff_street_name\nnumber_of_persons_injured\nnumber_of_persons_killed\n...\ncontributing_factor_vehicle_5\ncollision_id\nvehicle_type_code_1\nvehicle_type_code_2\nvehicle_type_code_3\nvehicle_type_code_4\nvehicle_type_code_5\ncrash_date\ncrash_time\nfactor_id\n\n\n\n\n\n\n0 rows × 30 columns\n\n\n\n\n\n\n5.6.7 Querying the data\nQuerying the data means requesting specific information from a database. In SQL, queries are written as commands to retrieve, filter, group, or sort data based on certain conditions. The goal of querying is to extract meaningful insights or specific subsets of data from a larger dataset.\n\nSELECT DISTINCT retrieves unique values from a column\npd.read_sql_query() executes the SQL query and returns the result as a DataFrame\n\n\n\n5.6.7.1 Query to find distinct contributing factors\nThis query selects the distinct contributing factors from the contributing_factor_vehicle_1 column.\n\nquery = \"SELECT DISTINCT contributing_factor_vehicle_1 FROM nyc_crashes;\"\n\nfactors = pd.read_sql_query(query, conn)\n\nfactors.head(5)\n\n\n\n\n\n\n\n\ncontributing_factor_vehicle_1\n\n\n\n\n0\nPassing Too Closely\n\n\n1\nUnspecified\n\n\n2\nDriver Inattention/Distraction\n\n\n3\nFailure to Yield Right-of-Way\n\n\n4\nOther Vehicular\n\n\n\n\n\n\n\n\n\n\n5.6.7.2 Can query using factor.id\nThis query selects the distinct contributing factors using the factor_id column.\n\nquery = \"\"\"\nSELECT DISTINCT n.factor_id, l.factor_description \nFROM nyc_crashes n\nJOIN contributing_factor_lookup l ON n.factor_id = l.factor_id\nWHERE n.factor_id IS NOT NULL;\n\"\"\"\n\nfactors = pd.read_sql_query(query, conn)\nfactors.head(5)\n\n\n\n\n\n\n\n\nfactor_id\nfactor_description\n\n\n\n\n0\n1\nPassing Too Closely\n\n\n1\n2\nUnspecified\n\n\n2\n3\nDriver Inattention/Distraction\n\n\n3\n4\nFailure to Yield Right-of-Way\n\n\n4\n5\nOther Vehicular\n\n\n\n\n\n\n\n\n\n\n5.6.8 Analyzing contributing_factor_vehicle_1\n\nSELECT Choose columns to retrieve\nCOUNT Count rows for each group\nGROUP BY Group rows that have the same values in specific columns\nORDER BY Sort results by the count in descending order\n\n\n5.6.8.1 Insights into contributing_factor_vehicle_1\n\nIdentify the most common contributing factors.\nUnderstand trends related to vehicle crash causes.\n\n\nfactor_count = pd.read_sql_query(\"\"\"\nSELECT contributing_factor_vehicle_1, COUNT(*) AS count \nFROM nyc_crashes \nGROUP BY contributing_factor_vehicle_1\nORDER BY count DESC;\n\"\"\", conn)\n\nfactor_count.head(10)\n\n\n\n\n\n\n\n\ncontributing_factor_vehicle_1\ncount\n\n\n\n\n0\nUnspecified\n473\n\n\n1\nDriver Inattention/Distraction\n447\n\n\n2\nFailure to Yield Right-of-Way\n116\n\n\n3\nFollowing Too Closely\n104\n\n\n4\nUnsafe Speed\n82\n\n\n5\nPassing or Lane Usage Improper\n74\n\n\n6\nTraffic Control Disregarded\n66\n\n\n7\nOther Vehicular\n63\n\n\n8\nPassing Too Closely\n57\n\n\n9\nAlcohol Involvement\n57\n\n\n\n\n\n\n\n\n\n\n5.6.9 Visualizing Analysis\nCan use Plotnine to visualize our Analysis for contributing_factor_vehicle_1 in a chart.\n\nfrom plotnine import ggplot, aes, geom_bar, theme_minimal, coord_flip, labs\n\ndb_path = 'nyc_crash.db'\nconn = sqlite3.connect(db_path)\n\n# Query to get the contributing factor counts\nfactor_count = pd.read_sql_query(\"\"\"\nSELECT contributing_factor_vehicle_1, COUNT(*) AS count\nFROM nyc_crashes\nGROUP BY contributing_factor_vehicle_1\nORDER BY count DESC;\n\"\"\", conn)\n\n# Create a bar chart using plotnine\nchart = (\n    ggplot(factor_count, aes(x='reorder(contributing_factor_vehicle_1, count)'\n                             , y='count')) +\n    geom_bar(stat='identity', fill='steelblue') +\n    coord_flip() +  # Flip for better readability\n    theme_minimal() +\n    labs(title='Top Contributing Factors to NYC Crashes',\n         x='Contributing Factor',\n         y='Number of Incidents')\n)\n\nchart\n\n\n\n\n\n\n\n\n\n\n5.6.10 Conclusion\n\nSQL in Python is powerful for handling structured data.\nsqlite3 and pandas simplify database interactions.\nAnalyzing crash data helps understand key contributing factors for traffic incidents.\n\n\n\n\n5.6.11 Further Readings:\n\n[How to use SQL in Python] (Przybyla (2024))\n[Python MySQL] (W3Schools (2025))\n[SQL using Python] (Bansal (2024))\n[Master Using SQL with Python - Using SQL with Pandas] (Cafferky (2019))\n\n\n\n\n\n\nBansal, R. (2024). SQL using python.\n\n\nCafferky, B. (2019). Master using SQL with python: Lesson 1 - using SQL with pandas.\n\n\nPrzybyla, M. (2024). How to use SQL in python.\n\n\nW3Schools. (2025). Python MySQL.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Manipulation</span>"
    ]
  },
  {
    "objectID": "visualization.html",
    "href": "visualization.html",
    "title": "6  Data Visualization",
    "section": "",
    "text": "6.1 Handling Spatial Data with GeoPandas and gmplot\nThe following section was written by Thomas Schittina, a senior majoring in statistics and minoring in mathematics at the University of Connecticut.\nThis section focuses on how to manipulate and visualize spatial data in Python, with a particular focus on the packages GeoPandas and gmplot. We’ll start with GeoPandas and do the following:\nFor gmplot we will:",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "visualization.html#handling-spatial-data-with-geopandas-and-gmplot",
    "href": "visualization.html#handling-spatial-data-with-geopandas-and-gmplot",
    "title": "6  Data Visualization",
    "section": "",
    "text": "Cover the core concepts and functionalities\nWalkthrough an example using NYC shape data\n\n\n\nTalk about why you’ll need a Google Maps API key\nSee some of the different plotting functionalities\nWalkthrough an example using NYC shape data\n\n\n6.1.1 GeoPandas\n\n6.1.1.1 Introducing GeoPandas\nFounded in 2013, GeoPandas is an open-source extension of Pandas that adds support for geospatial data. GeoPandas is built around the GeoSeries and GeoDataFrame objects. Both are subclasses of the corresponding Pandas objects, so they should feel familiar to those who have used Pandas before.\n\n\n6.1.1.2 A Remark about Shapely\nThe package Shapely is a core dependency of GeoPandas that handles geometric operations. Each geometry (point, polygon, etc.) stored in a GeoDataFrame is a Shapely object, and GeoPandas internally calls Shapely methods to perform spatial analysis. You won’t often need to interact directly with Shapely when using GeoPandas. Still, you may want to familiarize yourself with its basic concepts.\nShapely Documentation can be found here.\n\n\n6.1.1.3 GeoSeries and GeoDataFrame\nGeoSeries:\n\nSimilar to Series, but should exclusively contain geometries\nGeoSeries.crs stores the Coordinate Reference System information\n\nGeoDataFrame:\n\nMay consist of both Series and GeoSeries\nMay contain several GeoSeries, but only one active geometry column\n\nGeometric operations will only apply to the active column\nAccessed and manipulated with GeoDataFrame.geometry\n\nOtherwise similar to a normal DataFrame\n\n\n\n\n6.1.2 Example with NYC MODZCTA Shapefile\nGiven a file containing geospatial data, geopandas.read_file() will detect the filetype and create a GeoDataFrame.\n\nimport geopandas as gpd\nimport os\n\n# get .shp from MODZCTA_Shapefile folder\nshapefile_path = None\nfor file in os.listdir('MODZCTA_Shapefile'):\n    if file.endswith(\".shp\"):\n        shapefile_path = os.path.join('MODZCTA_Shapefile', file)\n        break  # Use the first .shp file found\n\n# read in data\ngdf = gpd.read_file(shapefile_path)\n\ngdf.drop(columns=['label', 'zcta'], inplace=True)\n\ngdf.head()\n\n\n\n\n\n\n\n\nmodzcta\npop_est\ngeometry\n\n\n\n\n0\n10001\n23072.0\nPOLYGON ((-73.98774 40.74407, -73.98819 40.743...\n\n\n1\n10002\n74993.0\nPOLYGON ((-73.9975 40.71407, -73.99709 40.7146...\n\n\n2\n10003\n54682.0\nPOLYGON ((-73.98864 40.72293, -73.98876 40.722...\n\n\n3\n10026\n39363.0\nMULTIPOLYGON (((-73.96201 40.80551, -73.96007 ...\n\n\n4\n10004\n3028.0\nMULTIPOLYGON (((-74.00827 40.70772, -74.00937 ...\n\n\n\n\n\n\n\nIt’s very important to know which CRS your geospatial data is in. Operations involving distance or area require a projected CRS (using feet, meters, etc.). If a geographic CRS is used (degrees), the calculations will likely be wrong.\n\nprint(gdf.crs)\n\n# convert to projected CRS\ngdf = gdf.to_crs(epsg=3857)\n\nprint(gdf.crs)\n\nEPSG:4326\nEPSG:3857\n\n\nOriginally, the geometries were in EPSG 4326, which is measured by latitude and longitude. In order to work with the shape data, the CRS was converted to EPSG 3857, which uses meters.\nNow we can start working with the spatial data. First, let’s compute the area of each zip code and store it as a new column.\n\n# create column of areas\ngdf['area'] = gdf.area\ngdf.head(3)\n\n\n\n\n\n\n\n\nmodzcta\npop_est\ngeometry\narea\n\n\n\n\n0\n10001\n23072.0\nPOLYGON ((-8236278.03 4974664.364, -8236327.85...\n2.987592e+06\n\n\n1\n10002\n74993.0\nPOLYGON ((-8237364.444 4970258.308, -8237318.6...\n3.974361e+06\n\n\n2\n10003\n54682.0\nPOLYGON ((-8236377.258 4971559.548, -8236390.9...\n2.611531e+06\n\n\n\n\n\n\n\nOur active geometry column is the shape data for each zip code, so gdf.area() only acts on that column and ignores the others.\nLet’s also find the boundary of each zip code, as well as its geographic center.\n\n# create columns for boundary and centorid info\ngdf['boundary'] = gdf.boundary\ngdf['centroid'] = gdf.centroid\n\ngdf[['modzcta', 'boundary', 'centroid']].head(3)\n\n\n\n\n\n\n\n\nmodzcta\nboundary\ncentroid\n\n\n\n\n0\n10001\nLINESTRING (-8236278.03 4974664.364, -8236327....\nPOINT (-8237323.727 4975637.524)\n\n\n1\n10002\nLINESTRING (-8237364.444 4970258.308, -8237318...\nPOINT (-8236103.249 4970509.323)\n\n\n2\n10003\nLINESTRING (-8236377.258 4971559.548, -8236390...\nPOINT (-8236435.551 4972866.281)\n\n\n\n\n\n\n\nSuppose we want to find the distance between two centroids. The current active geometry column is the shape data. Run gdf.geometry = gdf['centroid'] to switch the active geometry.\n\n# switch active geometry to centroid info\ngdf.geometry = gdf['centroid']\n\nThen we can calculate the distance between the first two centroids with distance().\n\n# find distance between first two centroids\ngdf.geometry[0].distance(gdf.geometry[1])\n\n5271.432980923517\n\n\n\n6.1.2.1 Plotting with GeoPandas\nGeoPandas also includes some basic plotting functionality. Similar to Pandas, plot() will generate visuals using matplotlib.\n\n# plot NYC zip codes with color mapping by area\ngdf.geometry = gdf['geometry'] # must switch active geometry back first\ngdf.plot('area', legend=True)\n\n\n\n\n\n\n\n\nInteractive maps can also be generated using explore, but you will need to install optional dependencies. An alternative approach is the package gmplot, which we’ll discuss next. First though, here is a list of common GeoPandas methods we’ve not yet covered.\n\nto_file(): save GeoDataFrame to a geospatial file (.shp, .GEOjson, etc.)\nlength(): calculate the length of a geometry, useful for linestrings\ninstersects(): check if one geometry intersects with another\ncontains(): check if one geometry contains another\nbuffer(): create a buffer of specified size around a geometry\nequals(): check if the CRS of two objects is the same\nis_valid(): check for invalid geometries\n\n\n\n\n6.1.3 gmplot\n\n6.1.3.1 Google Maps API\nAn API key is not necessary to create visuals with gmplot, but it is highly recommended. Without a key, any generated output will be dimmed and have a watermark.\n\n\n\nExample with no API key\n\n\nThe process to create an API key is very simple. Go here and click on Get Started. It requires some credit card information, but you start on a free trial with $300 of credit. You will not be charged unless you select activate full account.\nThere are some configuration options you can set for your key. Google has many different APIs, but gmplot only requires the Maps Javascript API.\n\n\n6.1.3.2 Creating Plots with gmplot\ngmplot is designed to mimic matplotlib, so the syntax should feel similar. The class GoogleMapPlotter provides the core functionality of the package.\n\nimport gmplot\n\napikey = open('gmapKey.txt').read().strip() # read in API key\n\n# plot map centered at NYC with zoom = 11\ngmap = gmplot.GoogleMapPlotter(40.5665, -74.1697, 11, apikey=apikey)\n\nNote: To render the classnotes on your computer, you will need to create the text file gmapKey.txt and store your Google Maps API key there.\nThe arguments include:\n\nThe latitude and longitude of NYC\nThe level of zoom\nAPI key (even if it’s not used directly)\nmore optional arguments for further customization\n\n\n\n\n6.1.4 Making Maps with NYC Zip Code Data\nLet’s display the largest zip code by area in NYC.\n\ngdf = gdf.to_crs(epsg=4326) # convert CRS to plot by latitude and longitude\nlargest_zip = gdf['geometry'][gdf['area'].idxmax()] # returns Shapely POLYGON\n\ncoords = list(largest_zip.exterior.coords) # unpack boundary coordinates\nlats = [lat for lon, lat in coords]\nlons = [lon for lon, lat in coords]\n\n# plot shape of zip code\ngmap.polygon(lats, lons, face_color='green', edge_color='blue', edge_width=3)\n\n# gmap.draw('largest_zip.html')\n\nAfter creating the plot, gmap.draw('filename') saves it as an HTML file in the current working directory, unless another location is specified. In the classnotes, all outputs will be shown as a PNG image.\n\n\n\nLargest NYC Zip Code by area\n\n\nLet’s also plot the centriod of this zip code, and include a link to gmplot’s documentation (in the classnotes this link won’t work because the PNG is used).\n\ngdf.geometry = gdf['centroid'] # now working with new geometry column\ngdf = gdf.to_crs(epsg=4326) # convert CRS to plot by latitude and longitude\n\ncentroid = gdf['centroid'][gdf['area'].idxmax()] # returns Shapely POINT\n\n# plot the point with info window\ngmap.marker(centroid.y, centroid.x, title='Center of Zip Code',\n            info_window=\"&lt;a href='https://github.com/gmplot/gmplot/wiki'&gt;gmplot docs&lt;/a&gt;\")\n\n# plot the polygon\ngmap.polygon(lats, lons, face_color='green', edge_color='blue', edge_width=3)\n\n# gmap.draw('zip_w_marker.html')\n\nHere’s the output:\n\n\n\nCenter of largest NYC Zip Code\n\n\n\n6.1.4.1 Other Features of gmplot\n\ndirections(): draw directions from one point to another\nscatter(): plot a collection of points\nheatmap(): plot a heatmap\nenable_marker_dropping(): click on map to create/remove markers\nfrom_geocode(): use name of location instead of coordinates\nsee docs for more\n\nYou can also change the map type when you create an instance of GoogleMapPlotter.\n\n# create hybrid type map\ngmap = gmplot.GoogleMapPlotter(40.776676, -73.971321, 11.5, apikey=apikey,\n                               map_type='hybrid')\n\n# gmap.draw('nyc_hybrid.html')\n\n\n\n\nHybrid map of NYC\n\n\n\n\n\n6.1.5 Summary\nGeopandas is a powerful tool for handling spatial data and operations. It builds on regular Pandas by introducing two new data structures, the GeoSeries and GeoDataFrame. Under the hood, Shapely handles geometric operations.\nThe package gmplot is a simple yet dynamic tool that overlays spatial data onto interactive Google maps. It does so through the class GoogleMapPlotter, which offers an alternative to Geopandas’ built in graphing methods for simple plots.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "stats.html",
    "href": "stats.html",
    "title": "7  Statistical Tests and Models",
    "section": "",
    "text": "7.1 Tests for Exploratory Data Analysis\nA collection of functions are available from scipy.stats.\nSince R has a richer collections of statistical functions, we can call R function from Python with rpy2. See, for example, a blog on this subject.\nFor example, fisher_exact can only handle 2x2 contingency tables. For contingency tables larger than 2x2, we can call fisher.test() from R through rpy2. See this StackOverflow post. Note that the . in function names and arguments are replaced with _.\nimport pandas as pd\nimport numpy as np\nimport rpy2.robjects.numpy2ri\nfrom rpy2.robjects.packages import importr\nrpy2.robjects.numpy2ri.activate()\n\nstats = importr('stats')\n\nw0630 = pd.read_feather(\"data/nyccrashes_cleaned.feather\")\nw0630[\"injury\"] = np.where(w0630[\"number_of_persons_injured\"] &gt; 0, 1, 0)\nm = pd.crosstab(w0630[\"injury\"], w0630[\"borough\"])\nprint(m)\n\nres = stats.fisher_test(m.to_numpy(), simulate_p_value = True)\nprint(res)\n\nLoading custom .Rprofileborough  BRONX  BROOKLYN  MANHATTAN  QUEENS  STATEN ISLAND\ninjury                                                    \n0          149       345        164     249             65\n1          129       266        127     227             28\n\n    Fisher's Exact Test for Count Data with simulated p-value (based on\n    2000 replicates)\n\ndata:  structure(c(149L, 129L, 345L, 266L, 164L, 127L, 249L, 227L, 65L, 28L), dim = c(2L, 5L))\np-value = 0.03148\nalternative hypothesis: two.sided",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Statistical Tests and Models</span>"
    ]
  },
  {
    "objectID": "stats.html#tests-for-exploratory-data-analysis",
    "href": "stats.html#tests-for-exploratory-data-analysis",
    "title": "7  Statistical Tests and Models",
    "section": "",
    "text": "Comparing the locations of two samples\n\nttest_ind: t-test for two independent samples\nttest_rel: t-test for paired samples\nranksums: Wilcoxon rank-sum test for two independent samples\nwilcoxon: Wilcoxon signed-rank test for paired samples\n\nComparing the locations of multiple samples\n\nf_oneway: one-way ANOVA\nkruskal: Kruskal-Wallis H-test\n\nTests for associations in contigency tables\n\nchi2_contingency: Chi-square test of independence of variables\nfisher_exact: Fisher exact test on a 2x2 contingency table\n\nGoodness of fit\n\ngoodness_of_fit: distribution could contain unspecified parameters\nanderson: Anderson-Darling test\nkstest: Kolmogorov-Smirnov test\nchisquare: one-way chi-square test\nnormaltest: test for normality",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Statistical Tests and Models</span>"
    ]
  },
  {
    "objectID": "stats.html#statistical-modeling",
    "href": "stats.html#statistical-modeling",
    "title": "7  Statistical Tests and Models",
    "section": "7.2 Statistical Modeling",
    "text": "7.2 Statistical Modeling\nStatistical modeling is a cornerstone of data science, offering tools to understand complex relationships within data and to make predictions. Python, with its rich ecosystem for data analysis, features the statsmodels package— a comprehensive library designed for statistical modeling, tests, and data exploration. statsmodels stands out for its focus on classical statistical models and compatibility with the Python scientific stack (numpy, scipy, pandas).\n\n7.2.1 Installation of statsmodels\nTo start with statistical modeling, ensure statsmodels is installed:\nUsing pip:\npip install statsmodels\n\n\n7.2.2 Linear Model\nLet’s simulate some data for illustrations.\n\nimport numpy as np\n\nnobs = 200\nncov = 5\nnp.random.seed(123)\nx = np.random.random((nobs, ncov)) # Uniform over [0, 1)\nbeta = np.repeat(1, ncov)\ny = 2 + np.dot(x, beta) + np.random.normal(size = nobs)\n\nCheck the shape of y:\n\ny.shape\n\n(200,)\n\n\nCheck the shape of x:\n\nx.shape\n\n(200, 5)\n\n\nThat is, the true linear regression model is \\[\ny = 2 + x_1 + x_2 + x_3 + x_4 + x_5 + \\epsilon.\n\\]\nA regression model for the observed data can be fitted as\n\nimport statsmodels.api as sma\nxmat = sma.add_constant(x)\nmymod = sma.OLS(y, xmat)\nmyfit = mymod.fit()\nmyfit.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\ny\nR-squared:\n0.309\n\n\nModel:\nOLS\nAdj. R-squared:\n0.292\n\n\nMethod:\nLeast Squares\nF-statistic:\n17.38\n\n\nDate:\nWed, 02 Apr 2025\nProb (F-statistic):\n3.31e-14\n\n\nTime:\n19:47:39\nLog-Likelihood:\n-272.91\n\n\nNo. Observations:\n200\nAIC:\n557.8\n\n\nDf Residuals:\n194\nBIC:\n577.6\n\n\nDf Model:\n5\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nconst\n1.8754\n0.282\n6.656\n0.000\n1.320\n2.431\n\n\nx1\n1.1703\n0.248\n4.723\n0.000\n0.682\n1.659\n\n\nx2\n0.8988\n0.235\n3.825\n0.000\n0.435\n1.362\n\n\nx3\n0.9784\n0.238\n4.114\n0.000\n0.509\n1.448\n\n\nx4\n1.3418\n0.250\n5.367\n0.000\n0.849\n1.835\n\n\nx5\n0.6027\n0.239\n2.519\n0.013\n0.131\n1.075\n\n\n\n\n\n\n\n\nOmnibus:\n0.810\nDurbin-Watson:\n1.978\n\n\nProb(Omnibus):\n0.667\nJarque-Bera (JB):\n0.903\n\n\nSkew:\n-0.144\nProb(JB):\n0.637\n\n\nKurtosis:\n2.839\nCond. No.\n8.31\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nQuestions to review:\n\nHow are the regression coefficients interpreted? Intercept?\nWhy does it make sense to center the covariates?\n\nNow we form a data frame with the variables\n\nimport pandas as pd\ndf = np.concatenate((y.reshape((nobs, 1)), x), axis = 1)\ndf = pd.DataFrame(data = df,\n                  columns = [\"y\"] + [\"x\" + str(i) for i in range(1,\n                  ncov + 1)])\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 200 entries, 0 to 199\nData columns (total 6 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   y       200 non-null    float64\n 1   x1      200 non-null    float64\n 2   x2      200 non-null    float64\n 3   x3      200 non-null    float64\n 4   x4      200 non-null    float64\n 5   x5      200 non-null    float64\ndtypes: float64(6)\nmemory usage: 9.5 KB\n\n\nLet’s use a formula to specify the regression model as in R, and fit a robust linear model (rlm) instead of OLS. Note that the model specification and the function interface is similar to R.\n\nimport statsmodels.formula.api as smf\nmymod = smf.rlm(formula = \"y ~ x1 + x2 + x3 + x4 + x5\", data = df)\nmyfit = mymod.fit()\nmyfit.summary()\n\n\nRobust linear Model Regression Results\n\n\nDep. Variable:\ny\nNo. Observations:\n200\n\n\nModel:\nRLM\nDf Residuals:\n194\n\n\nMethod:\nIRLS\nDf Model:\n5\n\n\nNorm:\nHuberT\n\n\n\n\nScale Est.:\nmad\n\n\n\n\nCov Type:\nH1\n\n\n\n\nDate:\nWed, 02 Apr 2025\n\n\n\n\nTime:\n19:47:40\n\n\n\n\nNo. Iterations:\n16\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n1.8353\n0.294\n6.246\n0.000\n1.259\n2.411\n\n\nx1\n1.1254\n0.258\n4.355\n0.000\n0.619\n1.632\n\n\nx2\n0.9664\n0.245\n3.944\n0.000\n0.486\n1.447\n\n\nx3\n0.9995\n0.248\n4.029\n0.000\n0.513\n1.486\n\n\nx4\n1.3275\n0.261\n5.091\n0.000\n0.816\n1.839\n\n\nx5\n0.6768\n0.250\n2.712\n0.007\n0.188\n1.166\n\n\n\nIf the model instance has been used for another fit with different fit parameters, then the fit options might not be the correct ones anymore .\n\n\nFor model diagnostics, one can check residual plots.\n\nimport matplotlib.pyplot as plt\n\nmyOlsFit = smf.ols(formula = \"y ~ x1 + x2 + x3 + x4 + x5\", data = df).fit()\nfig = plt.figure(figsize = (6, 6))\n## residual versus x1; can do the same for other covariates\nfig = sma.graphics.plot_regress_exog(myOlsFit, 'x1', fig=fig)\n\n\n\n\n\n\n\n\nSee more on residual diagnostics and specification tests.\n\n\n7.2.3 Generalized Linear Regression\nA linear regression model cannot be applied to presence/absence or count data. Generalized Linear Models (GLM) extend the classical linear regression to accommodate such response variables, that follow distributions other than the normal distribution. GLMs consist of three main components:\n\nRandom Component: This specifies the distribution of the response variable \\(Y\\). It is assumed to be from the exponential family of distributions, such as Binomial for binary data and Poisson for count data.\nSystematic Component: This consists of the linear predictor, a linear combination of unknown parameters and explanatory variables. It is denoted as \\(\\eta = X\\beta\\), where \\(X\\) represents the explanatory variables, and \\(\\beta\\) represents the coefficients.\nLink Function: The link function, \\(g\\), provides the relationship between the linear predictor and the mean of the distribution function. For a GLM, the mean of \\(Y\\) is related to the linear predictor through the link function as \\(\\mu = g^{-1}(\\eta)\\).\n\nGLMs adapt to various data types through the selection of appropriate link functions and probability distributions. Here, we outline four special cases of GLM: normal regression, logistic regression, Poisson regression, and gamma regression.\n\nNormal Regression (Linear Regression). In normal regression, the response variable has a normal distribution. The identity link function is typically used, making this case equivalent to classical linear regression.\n\nUse Case: Modeling continuous data where residuals are normally distributed.\nLink Function: Identity, \\(g(\\mu) = \\mu\\).\nDistribution: Normal.\n\nLogistic Regression. Logistic regression is used for binary response variables. It employs the logit link function to model the probability that an observation falls into one of two categories.\n\nUse Case: Binary outcomes (e.g., success/failure).\nLink Function: Logit, \\(g(\\mu) = \\log\\frac{\\mu}{1-\\mu}\\).\nDistribution: Binomial.\n\nPoisson Regression. Poisson regression models count data using the Poisson distribution. It’s ideal for modeling the rate at which events occur.\n\nUse Case: Count data, such as the number of occurrences of an event.\nLink Function: Log, \\(g(\\mu) = \\log(\\mu)\\)\nDistribution: Poisson.\n\nGamma Regression. Gamma regression is suited for modeling positive continuous variables, especially when data are skewed and variance increases with the mean.\n\nUse Case: Positive continuous outcomes with non-constant variance.\nLink Function: Inverse \\(g(\\mu) = \\frac{1}{\\mu}\\).\nDistribution: Gamma.\n\n\nEach GLM variant addresses specific types of data and research questions, enabling precise modeling and inference based on the underlying data distribution. Prediction will need the inverse link function which transforms the linear predictor to the expectation of the outcome.\nTo demonstrate the validation of logistic regression models, we first create a simulated dataset with binary outcomes. This setup involves generating logistic probabilities and then drawing binary outcomes based on these probabilities.\n\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\n\n# Set seed for reproducibility\nnp.random.seed(42)\n\n# Create a DataFrame with random features named `simdat`\nsimdat = pd.DataFrame(np.random.randn(1000, 5), columns=['x1', 'x2', 'x3', 'x4', 'x5'])\n\n# Calculating the linear combination of inputs plus an intercept\neta = simdat.dot([2, 2, 2, 0, 0]) - 5\n\n# Applying the logistic function to get probabilities using statsmodels' logit link\np = sm.families.links.Logit().inverse(eta)\n\n# Generating binary outcomes based on these probabilities and adding them to `simdat`\nsimdat['yb'] = np.random.binomial(1, p, p.size)\n\n# Display the first few rows of the dataframe\nprint(simdat.head())\n\n         x1        x2        x3        x4        x5  yb\n0  0.496714 -0.138264  0.647689  1.523030 -0.234153   0\n1 -0.234137  1.579213  0.767435 -0.469474  0.542560   0\n2 -0.463418 -0.465730  0.241962 -1.913280 -1.724918   0\n3 -0.562288 -1.012831  0.314247 -0.908024 -1.412304   0\n4  1.465649 -0.225776  0.067528 -1.424748 -0.544383   0\n\n\nFit a logistic regression for y1b with the formula interface.\n\nimport statsmodels.formula.api as smf\n\n# Specify the model formula\nformula = 'yb ~ x1 + x2 + x3 + x4 + x5'\n\n# Fit the logistic regression model using glm and a formula\nfit = smf.glm(formula=formula, data=simdat, family=sm.families.Binomial()).fit()\n\n# Print the summary of the model\nprint(fit.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                     yb   No. Observations:                 1000\nModel:                            GLM   Df Residuals:                      994\nModel Family:                Binomial   Df Model:                            5\nLink Function:                  Logit   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -136.44\nDate:                Wed, 02 Apr 2025   Deviance:                       272.89\nTime:                        19:47:43   Pearson chi2:                 1.09e+03\nNo. Iterations:                     8   Pseudo R-squ. (CS):             0.2793\nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     -5.4564      0.453    -12.049      0.000      -6.344      -4.569\nx1             2.1544      0.244      8.822      0.000       1.676       2.633\nx2             2.0781      0.225      9.234      0.000       1.637       2.519\nx3             1.9260      0.237      8.125      0.000       1.461       2.391\nx4            -0.1085      0.166     -0.652      0.514      -0.434       0.217\nx5             0.2672      0.158      1.695      0.090      -0.042       0.576\n==============================================================================",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Statistical Tests and Models</span>"
    ]
  },
  {
    "objectID": "stats.html#logistic-regression",
    "href": "stats.html#logistic-regression",
    "title": "7  Statistical Tests and Models",
    "section": "7.3 Logistic Regression",
    "text": "7.3 Logistic Regression\nOnce a logistic regression model is fitted, interpreting its results is crucial for understanding how predictor variables influence the probability of the outcome. Logistic regression models the log-odds of the response variable as a linear function of the predictor variables. To ease the intrepretation, consider a logistic model with a single binary predictor (e.g., treatment indicator):\n\\[\n\\log\\left(\\frac{\\mu}{1 - \\mu}\\right) = \\beta_0 + \\beta_1 X\n\\]\nwhere \\(\\mu = E(Y \\mid X)\\) represents the probability of the positive class, and \\(\\beta_1\\) is the estimated coefficient for the binary predictor \\(X\\).\n\n7.3.1 Interpreting Coefficients\nIf \\(X\\) is a binary variable (e.g., 0 for “No” and 1 for “Yes”), \\(\\beta_1\\) represents the difference in log-odds between the two groups. Exponentiating \\(\\beta_1\\) gives the odds ratio:\n\\[\n\\text{Odds Ratio} = \\frac{\\exp(\\beta_0 + \\beta_1)}{\\exp(\\beta_0)} = e^{\\beta_1}.\n\\]\n\nIf \\(e^{\\beta_1} &gt; 1\\), the outcome is more likely when \\(X = 1\\) than when \\(X = 0\\).\nIf \\(e^{\\beta_1} &lt; 1\\), the outcome is less likely when \\(X = 1\\).\nIf \\(e^{\\beta_1} = 1\\), there is no effect of \\(X\\) on the odds of the outcome.\n\nEquivalently, \\(\\beta_1\\) is the log odds ratio between the two groups.\nWhen there are multiple predictors, the intrepretation needs to state that all the other predictors are unchanged.\nHow would you intreprete the coefficient of a continuous predictor?\n\n\n7.3.2 Probabilistic Interpretation\nWe can transform the linear predictor into a probability estimate using the inverse logit function:\n\\[\n\\Pr(Y=1 | X) = \\frac{e^{\\beta_0 + \\beta_1 X}}{1 + e^{\\beta_0 + \\beta_1 X}}.\n\\]\nThis allows for a direct interpretation of how being in one category of \\(X\\) influences the predicted probability of the outcome. By construction, this value is always in \\((0, 1)\\).\n\n\n7.3.3 Evaluating Statistical Significance\nThe significance of \\(\\beta_1\\) is assessed using standard errors and p-values:\n\nA small p-value (e.g., &lt; 0.05) suggests that \\(X\\) has a statistically significant effect on the outcome.\nConfidence intervals for \\(e^{\\beta_1}\\) help understand the precision of odds ratio estimates.\n\n\n\n7.3.4 Confusion Matrix\nValidating the performance of logistic regression models is crucial to assess their effectiveness and reliability. This section explores key metrics used to evaluate the performance of logistic regression models, starting with the confusion matrix, then moving on to accuracy, precision, recall, F1 score, and the area under the ROC curve (AUC). Using simulated data, we will demonstrate how to calculate and interpret these metrics using Python.\nThe confusion matrix is a fundamental tool used for calculating several other classification metrics. It is a table used to describe the performance of a classification model on a set of data for which the true values are known. The matrix displays the actual values against the predicted values, providing insight into the number of correct and incorrect predictions.\n\n\n\nActual\nPredicted Positive\nPredicted Negative\n\n\n\n\nActual Positive\nTrue Positive (TP)\nFalse Negative (FN)\n\n\nActual Negative\nFalse Positive (FP)\nTrue Negative (TN)\n\n\n\nFour entries in the confusion matrix:\n\nTrue Positive (TP): The cases in which the model correctly predicted the positive class.\nFalse Positive (FP): The cases in which the model incorrectly predicted the positive class (i.e., the model predicted positive, but the actual class was negative).\nTrue Negative (TN): The cases in which the model correctly predicted the negative class.\nFalse Negative (FN): The cases in which the model incorrectly predicted the negative class (i.e., the model predicted negative, but the actual class was positive).\n\nFour rates from the confusion matrix with actual (row) margins:\n\nTrue positive rate (TPR): TP / (TP + FN). Also known as sensitivity.\nFalse negative rate (FNR): FN / (TP + FN). Also known as miss rate.\nFalse positive rate (FPR): FP / (FP + TN). Also known as false alarm, fall-out.\nTrue negative rate (TNR): TN / (FP + TN). Also known as specificity.\n\nNote that TPR and FPR do not add up to one. Neither do FNR and FPR.\n\nPositive predictive value (PPV): TP / (TP + FP). Also known as precision.\nFalse discovery rate (FDR): FP / (TP + FP).\nFalse omission rate (FOR): FN / (FN + TN).\nNegative predictive value (NPV): TN / (FN + TN).\n\nNote that PPV and NP do not add up to one.\n\n\n7.3.5 Accuracy\nAccuracy measures the overall correctness of the model and is defined as the ratio of correct predictions (both positive and negative) to the total number of cases examined.\n  Accuracy = (TP + TN) / (TP + TN + FP + FN)\n\nImbalanced Classes: Accuracy can be misleading if there is a significant imbalance between the classes. For instance, in a dataset where 95% of the samples are of one class, a model that naively predicts the majority class for all instances will still achieve 95% accuracy, which does not reflect true predictive performance.\nMisleading Interpretations: High overall accuracy might hide the fact that the model is performing poorly on a smaller, yet important, segment of the data.\n\n\n\n7.3.6 Precision\nPrecision (or PPV) measures the accuracy of positive predictions. It quantifies the number of correct positive predictions made.\n  Precision = TP / (TP + FP)\n\nNeglect of False Negatives: Precision focuses solely on the positive class predictions. It does not take into account false negatives (instances where the actual class is positive but predicted as negative). This can be problematic in cases like disease screening where missing a positive case (disease present) could be dangerous.\nNot a Standalone Metric: High precision alone does not indicate good model performance, especially if recall is low. This situation could mean the model is too conservative in predicting positives, thus missing out on a significant number of true positive instances.\n\n\n\n7.3.7 Recall\nRecall (Sensitivity or TPR) measures the ability of a model to find all relevant cases (all actual positives).\n  Recall = TP / (TP + FN)\n\nNeglect of False Positives: Recall does not consider false positives (instances where the actual class is negative but predicted as positive). High recall can be achieved at the expense of precision, leading to a large number of false positives which can be costly or undesirable in certain contexts, such as in spam detection.\nTrade-off with Precision: Often, increasing recall decreases precision. This trade-off needs to be managed carefully, especially in contexts where both false positives and false negatives carry significant costs or risks.\n\n\n\n7.3.8 F-beta Score\nThe F-beta score is a weighted harmonic mean of precision and recall, taking into account a \\(\\beta\\) parameter such that recall is considered \\(\\beta\\) times as important as precision: \\[\n(1 + \\beta^2) \\frac{\\text{precision} \\cdot \\text{recall}}\n{\\beta^2 \\text{precision} + \\text{recall}}.\n\\]\nSee stackexchange post for the motivation of \\(\\beta^2\\) instead of just \\(\\beta\\).\nThe F-beta score reaches its best value at 1 (perfect precision and recall) and worst at 0.\nIf reducing false negatives is more important (as might be the case in medical diagnostics where missing a positive diagnosis could be critical), you might choose a beta value greater than 1. If reducing false positives is more important (as in spam detection, where incorrectly classifying an email as spam could be inconvenient), a beta value less than 1 might be appropriate.\nThe F1 Score is a specific case of the F-beta score where beta is 1, giving equal weight to precision and recall. It is the harmonic mean of Precision and Recall and is a useful measure when you seek a balance between Precision and Recall and there is an uneven class distribution (large number of actual negatives).\n\n\n7.3.9 Receiver Operating Characteristic (ROC) Curve\nThe Receiver Operating Characteristic (ROC) curve is a plot that illustrates the diagnostic ability of a binary classifier as its discrimination threshold is varied. It shows the trade-off between the TPR and FPR. The ROC plots TPR against FPR as the decision threshold is varied. It can be particularly useful in evaluating the performance of classifiers when the class distribution is imbalanced,\n\nIncreasing from \\((0, 0)\\) to \\((1, 1)\\).\nBest classification passes \\((0, 1)\\).\nClassification by random guess gives the 45-degree line.\nArea between the ROC and the 45-degree line is the Gini coefficient, a measure of inequality.\nArea under the curve (AUC) of ROC thus provides an important metric of classification results.\n\nThe Area Under the ROC Curve (AUC) is a scalar value that summarizes the performance of a classifier. It measures the total area underneath the ROC curve, providing a single metric to compare models. The value of AUC ranges from 0 to 1:\n\nAUC = 1: A perfect classifier, which perfectly separates positive and negative classes.\nAUC = 0.5: A classifier that performs no better than random chance.\nAUC &lt; 0.5: A classifier performing worse than random.\n\nThe AUC value provides insight into the model’s ability to discriminate between positive and negative classes across all possible threshold values.\n\n\n7.3.10 Demonstration\nLet’s apply these metrics to the simdat dataset to understand their practical implications. We will fit a logistic regression model, make predictions, and then compute accuracy, precision, and recall.\n\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, confusion_matrix,\n    f1_score, roc_curve, auc\n)\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_classification\n\n# Generate synthetic data\nX, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n\n# Fit the logistic regression model\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\n\n# Predict labels on the test set\ny_pred = model.predict(X_test)\n\n# Get predicted probabilities for ROC curve and AUC\ny_scores = model.predict_proba(X_test)[:, 1]  # Probability for the positive class\n\n# Compute confusion matrix\ncm = confusion_matrix(y_test, y_pred)\n\n# Calculate accuracy, precision, and recall\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\n\n# Print confusion matrix and metrics\nprint(\"Confusion Matrix:\\n\", cm)\nprint(f\"Accuracy: {accuracy:.2f}\")\nprint(f\"Precision: {precision:.2f}\")\nprint(f\"Recall: {recall:.2f}\")\n\nConfusion Matrix:\n [[104  11]\n [ 26 109]]\nAccuracy: 0.85\nPrecision: 0.91\nRecall: 0.81\n\n\nBy varying threshold, one can plot the whole ROC curve.\n\n# Compute ROC curve and AUC\nfpr, tpr, thresholds = roc_curve(y_test, y_scores)\nroc_auc = auc(fpr, tpr)\n\n# Print AUC\nprint(f\"AUC: {roc_auc:.2f}\")\n\n# Plot ROC curve\nplt.figure()\nplt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')  # Diagonal line (random classifier)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC) Curve')\nplt.legend(loc=\"lower right\")\nplt.show()\n\nAUC: 0.92\n\n\n\n\n\n\n\n\n\nWe could pick the best threshold that optmizes F1-score/\n\n# Compute F1 score for each threshold\nf1_scores = []\nfor thresh in thresholds:\n    y_pred_thresh = (y_scores &gt;= thresh).astype(int)  # Apply threshold to get binary predictions\n    f1 = f1_score(y_test, y_pred_thresh)\n    f1_scores.append(f1)\n\n# Find the best threshold (the one that maximizes F1 score)\nbest_thresh = thresholds[np.argmax(f1_scores)]\nbest_f1 = max(f1_scores)\n\n# Print the best threshold and corresponding F1 score\nprint(f\"Best threshold: {best_thresh:.4f}\")\nprint(f\"Best F1 score: {best_f1:.2f}\")\n\nBest threshold: 0.3960\nBest F1 score: 0.89",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Statistical Tests and Models</span>"
    ]
  },
  {
    "objectID": "stats.html#lasso-logistic-models",
    "href": "stats.html#lasso-logistic-models",
    "title": "7  Statistical Tests and Models",
    "section": "7.4 LASSO Logistic Models",
    "text": "7.4 LASSO Logistic Models\nThe Least Absolute Shrinkage and Selection Operator (LASSO) (Tibshirani, 1996), is a regression method that performs both variable selection and regularization. LASSO imposes an L1 penalty on the regression coefficients, which has the effect of shrinking some coefficients exactly to zero. This results in simpler, more interpretable models, especially in situations where the number of predictors exceeds the number of observations.\n\n7.4.1 Theoretical Formulation of the Problem\nThe objective function for LASSO logistic regression can be expressed as,\n\\[\n\\min_{\\beta}\n\\left\\{ -\\frac{1}{n} \\sum_{i=1}^n \\left[ y_i \\log(\\hat{p}_i) + (1 - y_i) \\log(1 - \\hat{p}_i) \\right] + \\lambda \\sum_{j=1}^p |\\beta_j| \\right\\}\n\\]\nwhere:\n\n\\(\\hat{p}_i = \\frac{1}{1 + e^{-X_i\\beta}}\\) is the predicted probability for the \\(i\\)-th sample.\n\\(y_i\\) represents the actual class label (binary: 0 or 1).\n\\(X_i\\) is the feature vector for the \\(i\\)-th observation.\n\\(\\beta\\) is the vector of model coefficients (including the intercept).\n\\(\\lambda\\) is the regularization parameter that controls the trade-off between model fit and sparsity (higher \\(\\lambda\\)) encourages sparsity by shrinking more coefficients to zero).\n\nThe lasso penalty encourages the sum of the absolute values of the coefficients to be small, effectively shrinking some coefficients to zero. This results in sparser solutions, simplifying the model and reducing variance without substantial increase in bias.\nPractical benefits of LASSO:\n\nDimensionality Reduction: LASSO is particularly useful when the number of features \\(p\\) is large, potentially even larger than the number of observations \\(n\\), as it automatically reduces the number of features.\nPreventing Overfitting: The L1 penalty helps prevent overfitting by constraining the model, especially when \\(p\\) is large or there is multicollinearity among features.\nInterpretability: By selecting only the most important features, LASSO makes the resulting model more interpretable, which is valuable in fields like bioinformatics, economics, and social sciences.\n\n\n\n7.4.2 Solution Path\nTo illustrate the effect of the lasso penalty in logistic regression, we can plot the solution path of the coefficients as a function of the regularization parameter \\(\\lambda\\). This demonstration will use a simulated dataset to show how increasing \\(\\lambda\\) leads to more coefficients being set to zero.\n\nimport numpy as np\nfrom sklearn.datasets import make_classification\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\n\n# Step 1: Generate a classification dataset\nX, y = make_classification(n_samples=100, n_features=20, n_informative=2,\n                               random_state=42)\n\n# Step 2: Get a lambda grid given length of lambda and min_ratio of lambda_max\ndef get_lambda_l1(xs: np.ndarray, y: np.ndarray, nlambda: int, min_ratio: float):\n    ybar = np.mean(y)\n    xbar = np.mean(xs, axis=0)\n    xs_centered = xs - xbar\n    xty = np.dot(xs_centered.T, (y - ybar))\n    lmax = np.max(np.abs(xty))\n    lambdas = np.logspace(np.log10(lmax), np.log10(min_ratio * lmax),\n                              num=nlambda)\n    return lambdas\n\n# Step 3: Calculate lambda values\nnlambda = 100\nmin_ratio = 0.01\nlambda_values = get_lambda_l1(X, y, nlambda, min_ratio)\n\n# Step 4: Standardize the features\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Step 5: Initialize arrays to store the coefficients for each lambda value\ncoefficients = []\n\n# Step 6: Fit logistic regression with L1 regularization (Lasso) for each lambda value\nfor lam in lambda_values:\n    model = LogisticRegression(penalty='l1', solver='liblinear', C=1/lam, max_iter=1000)\n    model.fit(X_scaled, y)\n    coefficients.append(model.coef_.flatten())\n\n# Convert coefficients list to a NumPy array for plotting\ncoefficients = np.array(coefficients)\n\n# Step 7: Plot the solution path for each feature\nplt.figure(figsize=(10, 6))\nfor i in range(coefficients.shape[1]):\n    plt.plot(lambda_values, coefficients[:, i], label=f'Feature {i + 1}')\n    \nplt.xscale('log')\nplt.xlabel('Lambda values (log scale)')\nplt.ylabel('Coefficient value')\nplt.title('Solution Path of Logistic Lasso Regression')\nplt.grid(True)\nplt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n7.4.3 Selection the Tuning Parameter\nIn logistic regression with LASSO regularization, selecting the optimal value of the regularization parameter \\(C\\) (the inverse of \\(\\lambda\\)) is crucial to balancing the model’s bias and variance. A small \\(C\\) value (large \\(\\lambda\\)) increases the regularization effect, shrinking more coefficients to zero and simplifying the model. Conversely, a large \\(C\\) (small \\(\\lambda\\)) allows the model to fit the data more closely.\nThe best way to select the optimal \\(C\\) is through cross-validation. In cross-validation, the dataset is split into several folds, and the model is trained on some folds while evaluated on the remaining fold. This process is repeated for each fold, and the results are averaged to ensure the model generalizes well to unseen data. The \\(C\\) value that results in the best performance is selected.\nThe performance metric used in cross-validation can vary based on the task. Common metrics include:\n\nLog-loss: Measures how well the predicted probabilities match the actual outcomes.\nAccuracy: Measures the proportion of correctly classified instances.\nF1-Score: Balances precision and recall, especially useful for imbalanced classes.\nAUC-ROC: Evaluates how well the model discriminates between the positive and negative classes.\n\nIn Python, the LogisticRegressionCV class from scikit-learn automates cross-validation for logistic regression. It evaluates the model’s performance for a range of \\(C\\) values and selects the best one.\n\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import make_classification\nfrom sklearn.metrics import accuracy_score\n\n# Generate synthetic data\nX, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n\n# Initialize LogisticRegressionCV with L1 penalty for Lasso and cross-validation\nlog_reg_cv = LogisticRegressionCV(\n    Cs=np.logspace(-4, 4, 20),  # Range of C values (inverse of lambda)\n    cv=5,                       # 5-fold cross-validation\n    penalty='l1',               # Lasso regularization (L1 penalty)\n    solver='liblinear',         # Solver for L1 regularization\n    scoring='accuracy',         # Optimize for accuracy\n    max_iter=10000              # Ensure convergence\n)\n\n# Train the model with cross-validation\nlog_reg_cv.fit(X_train, y_train)\n\n# Best C value (inverse of lambda)\nprint(f\"Best C value: {log_reg_cv.C_[0]}\")\n\n# Evaluate the model on the test set\ny_pred = log_reg_cv.predict(X_test)\ntest_accuracy = accuracy_score(y_test, y_pred)\nprint(f\"Test Accuracy: {test_accuracy:.2f}\")\n\n# Display the coefficients of the best model\nprint(\"Model Coefficients:\\n\", log_reg_cv.coef_)\n\nBest C value: 0.08858667904100823\nTest Accuracy: 0.86\nModel Coefficients:\n [[ 0.          0.          0.05552448  0.          0.          1.90889734\n   0.          0.          0.          0.          0.0096863   0.23541942\n   0.          0.         -0.0268928   0.          0.          0.\n   0.          0.        ]]\n\n\n\n\n7.4.4 Preparing for Logistic Regression Fitting\nThe LogisticRegression() function in scikit.learn takes the design matrix of the regression as input, which needs to be prepared with care from the covariates or features that we have.\n\n7.4.4.1 Continuous Variables\nFor continuous variables, it is often desirable to standardized them so that they have mean zero and standard deviation one. There are multiple advantages of doing so. It improves numerical stability in algorithms like logistic regression that rely on gradient descent, ensuring faster convergence and preventing features with large scales from dominating the optimization process. Standardization also enhances the interpretability of model coefficients by allowing for direct comparison of the effects of different features, as coefficients then represent the change in outcome for a one standard deviation increase in each variable. Additionally, it ensures that regularization techniques like Lasso and Ridge treat all features equally, allowing the model to select the most relevant ones without being biased by feature magnitude.\nMoreover, standardization is essential for distance-based models such as k-Nearest Neighbors (k-NN) and Support Vector Machines (SVMs), where differences in feature scale can distort the calculations. It also prevents models from being sensitive to arbitrary changes in the units of measurement, improving robustness and consistency. Finally, standardization facilitates better visualizations and diagnostics by putting all variables on a comparable scale, making patterns and residuals easier to interpret. Overall, it is a simple yet powerful preprocessing step that leads to better model performance and interpretability.\nWe have already seen this with StandardScaler.\n\n\n7.4.4.2 Categorical Variables\nCategorical variables can be classified into two types: nominal and ordinal. Nominal variables represent categories with no inherent order or ranking between them. Examples include variables like “gender” (male, female) or “color” (red, blue, green), where the categories are simply labels and one category does not carry more significance than another. Ordinal variables, on the other hand, represent categories with a meaningful order or ranking. For example, education levels such as “high school,” “bachelor,” “master,” and “PhD” have a clear hierarchy, where each level is ranked higher than the previous one. However, the differences between the ranks are not necessarily uniform or quantifiable, making ordinal variables distinct from numerical variables. Understanding the distinction between nominal and ordinal variables is important when deciding how to encode and interpret them in statistical models.\nCategorical variables needs to be coded into numerical values before further processing. In Python, nominal and ordinal variables are typically encoded differently to account for their unique properties. Nominal variables, which have no inherent order, are often encoded using One-Hot Encoding, where each category is transformed into a binary column (0 or 1). For example, the OneHotEncoder from scikit-learn can be used to convert a “color” variable with categories like “red,” “blue,” and “green” into separate columns color_red, color_blue, and color_green, with only one column being 1 for each observation. On the other hand, ordinal variables, which have a meaningful order, are best encoded using Ordinal Encoding. This method assigns an integer to each category based on their rank. For example, an “education” variable with categories “high school,” “bachelor,” “master,” and “PhD” can be encoded as 0, 1, 2, and 3, respectively. The OrdinalEncoder from scikit-learn can be used to implement this encoding, which ensures that the model respects the order of the categories during analysis.\n\n\n7.4.4.3 An Example\nHere is a demo with pipeline using a simulated dataset.\nFirst we generate data with sample size 1000 from a logistic model with both categorical and numerical covariates.\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nimport numpy as np\nfrom scipy.special import expit  # Sigmoid function\n\n# Generate a dataset with the specified size\ndataset_size = 1000\nnp.random.seed(20241014)\n\n# Simulate categorical and numerical features\ngender = np.random.choice(\n    ['male', 'female'], size=dataset_size)  # Nominal variable\neducation = np.random.choice(\n    ['high_school', 'bachelor', 'master', 'phd'], size=dataset_size)  # Ordinal variable\nage = np.random.randint(18, 65, size=dataset_size)\nincome = np.random.randint(30000, 120000, size=dataset_size)\n\n# Create a logistic relationship between the features and the outcome\ngender_num = np.where(gender == 'male', 0, 1)\n\n# Define the linear predictor with regression coefficients\nlinear_combination = (\n    0.3 * gender_num - 0.02 * age + 0.00002 * income\n)\n\n# Apply sigmoid function to get probabilities\nprobabilities = expit(linear_combination)\n\n# Generate binary outcome based on the probabilities\noutcome = np.random.binomial(1, probabilities)\n\n# Create a DataFrame\ndata = pd.DataFrame({\n    'gender': gender,\n    'education': education,\n    'age': age,\n    'income': income,\n    'outcome': outcome\n})\n\nNext we split the data into features and target and define transformers for each types of feature columns.\n\n# Split the dataset into features (X) and target (y)\nX = data[['gender', 'education', 'age', 'income']]\ny = data['outcome']\n\n# Define categorical and numerical columns\ncategorical_cols = ['gender', 'education']  \nnumerical_cols = ['age', 'income']\n\n# Define transformations for categorical variable\ncategorical_transformer = OneHotEncoder(\n    categories=[['male', 'female'], ['high_school', 'bachelor', 'master', 'phd']],\n    drop='first')\n\n# Define transformations for continuous variables\nnumerical_transformer = StandardScaler()\n\n# Use ColumnTransformer to transform the columns\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('cat', categorical_transformer, categorical_cols),\n        ('num', numerical_transformer, numerical_cols)\n    ]\n)\n\nDefine a pipeline, which preprocess the data and then fits a logistic model.\n\npipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('classifier', LogisticRegression(penalty='l1', solver='liblinear',\n    max_iter=1000))\n])\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=2024)\n\n# Fit the pipeline to the training data\npipeline.fit(X_train, y_train)\n\nPipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('cat',\n                                                  OneHotEncoder(categories=[['male',\n                                                                             'female'],\n                                                                            ['high_school',\n                                                                             'bachelor',\n                                                                             'master',\n                                                                             'phd']],\n                                                                drop='first'),\n                                                  ['gender', 'education']),\n                                                 ('num', StandardScaler(),\n                                                  ['age', 'income'])])),\n                ('classifier',\n                 LogisticRegression(max_iter=1000, penalty='l1',\n                                    solver='liblinear'))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFittedPipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('cat',\n                                                  OneHotEncoder(categories=[['male',\n                                                                             'female'],\n                                                                            ['high_school',\n                                                                             'bachelor',\n                                                                             'master',\n                                                                             'phd']],\n                                                                drop='first'),\n                                                  ['gender', 'education']),\n                                                 ('num', StandardScaler(),\n                                                  ['age', 'income'])])),\n                ('classifier',\n                 LogisticRegression(max_iter=1000, penalty='l1',\n                                    solver='liblinear'))]) preprocessor: ColumnTransformer?Documentation for preprocessor: ColumnTransformerColumnTransformer(transformers=[('cat',\n                                 OneHotEncoder(categories=[['male', 'female'],\n                                                           ['high_school',\n                                                            'bachelor',\n                                                            'master', 'phd']],\n                                               drop='first'),\n                                 ['gender', 'education']),\n                                ('num', StandardScaler(), ['age', 'income'])]) cat['gender', 'education'] OneHotEncoder?Documentation for OneHotEncoderOneHotEncoder(categories=[['male', 'female'],\n                          ['high_school', 'bachelor', 'master', 'phd']],\n              drop='first') num['age', 'income'] StandardScaler?Documentation for StandardScalerStandardScaler() LogisticRegression?Documentation for LogisticRegressionLogisticRegression(max_iter=1000, penalty='l1', solver='liblinear') \n\n\nCheck the coefficients of the fitted logistic regression model.\n\nmodel = pipeline.named_steps['classifier']\nintercept = model.intercept_\ncoefficients = model.coef_\n\n# Check the preprocessor's encoding\nencoded_columns = pipeline.named_steps['preprocessor']\\\n.transformers_[0][1].get_feature_names_out(categorical_cols)\n\n# Show intercept, coefficients, and encoded feature names\nintercept, coefficients, list(encoded_columns)\n\n(array([0.66748582]),\n array([[ 0.30568894,  0.10069842,  0.12087311,  0.22576774, -0.24749201,\n          0.55828424]]),\n ['gender_female', 'education_bachelor', 'education_master', 'education_phd'])\n\n\nNote that the encoded columns has one for gender and three for education, with male and high_school as reference levels, respectively. The reference level was determined when calling oneHotEncoder() with drop = 'first'. If categories were not specified, the first level in alphabetical order would be dropped. With the default drop = 'none', the estimated coefficients will have two columns that are not estimable and were set to zero. Obviously, if no level were dropped in forming the model matrix, the columns of the one hot encoding for each categorical variable would be perfectly linearly dependent because they would sum to one.\nThe regression coefficients returned by the logistic regression model in this case should be interpreted on the standardized scale of the numerical covariates (e.g., age and income). This is because we applied standardization to the numerical features using StandardScaler in the pipeline before fitting the model. For example, the coefficient for age would reflect the change in the log-odds of the outcome for a 1 standard deviation increase in age, rather than a 1-unit increase in years. The coefficients for the one-hot encoded categorical variables (gender and education) are on the original scale because one-hot encoding does not change the scale of the variables. For instance, the coefficient for gender_female tells us how much the log-odds of the outcome changes when the observation is male versus the reference category (male).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Statistical Tests and Models</span>"
    ]
  },
  {
    "objectID": "stats.html#count-data-modeling",
    "href": "stats.html#count-data-modeling",
    "title": "7  Statistical Tests and Models",
    "section": "7.5 Count Data Modeling",
    "text": "7.5 Count Data Modeling\nCount data consists of non-negative integers representing event occurrences over a fixed unit of time or space. These data often exhibit skewness, overdispersion, and a prevalence of zeros, requiring specialized statistical models. Count data is common in applied fields such as urban planning and environmental studies. This section introduces statistical models for count data, focusing on the Poisson and Negative Binomial (NB) distributions. Their probability mass functions (pmfs) are linked to Generalized Linear Model (GLM) parameters.\n\n7.5.1 Poisson Regression\nThe Poisson model is a member of the GLM. It assumes that the count variable \\(Y\\) follows a Poisson distribution:\n\\[\n\\Pr(Y = y) = \\frac{\\lambda^y e^{-\\lambda}}{y!}, \\quad y = 0,1,2,\\dots\n\\] where \\(\\lambda\\) is the expected count, linked to predictor variables through a log link function:\n\\[\n\\log \\lambda = X^{\\top} \\beta.\n\\]\nHere, $X represents the vector of covariates, and \\(\\beta\\) denotes the regression coefficients. The model assumes equidispersion (i.e., \\(E[Y] = \\text{Var}(Y)\\)), which is often violated in practice.\n\nThe coefficient \\(\\beta_j\\) represents the log change in the expected count per unit increase in \\(X_j\\).\nExponentiating \\(\\beta_j\\) provides the multiplicative effect on the mean count.\nIf \\(\\beta_j &gt; 0\\), increasing \\(X_j\\) leads to higher counts, while \\(\\beta_j &lt; 0\\) suggests a negative association\n\n\n\n7.5.2 Negative Binomial Regression\nWhen overdispersion (variance exceeding the mean) is present, the Negative Binomial (NB) regression provides a more flexible alternative. The NB model introduces an overdispersion parameter \\(\\theta\\), modifying the variance structure:\n\\[\n\\Pr(Y = y) = \\frac{\\Gamma(y + \\theta^{-1})}{y! \\Gamma(\\theta^{-1})}\n\\left( \\frac{\\theta \\lambda}{1 + \\theta \\lambda} \\right)^y\n\\left( \\frac{1}{1 + \\theta \\lambda} \\right)^{\\theta^{-1}},\n\\]\nwhere \\(\\theta\\) controls the degree of dispersion. The mean remains \\(\\lambda\\), but the variance expands to:\n\\[\n\\text{Var}(Y) = \\lambda + \\frac{\\lambda^2}{\\theta}.\n\\]\nThe log link function remains to be commonly used.\n\nCoefficients in NB regression are interpreted similarly to Poisson regression.\nThe dispersion parameter \\(\\theta\\) quantifies the degree of overdispersion; larger values suggest the Poisson model may still be appropriate.\n\n\n\n7.5.3 Model Diagnosis\nAssessing model fit is crucial in count data modeling. Common diagnostic methods include:\n\nOverdispersion Check: If the variance significantly exceeds the mean, NB regression is preferred.\nGoodness-of-Fit: Comparing Akaike Information Criterion (AIC) values for Poisson and NB models; lower AIC suggests a better fit.\nResidual Analysis: Examining Pearson and deviance residuals for systematic patterns.\nZero-Inflation Check: If excess zeros exist, zero-inflated models may be required.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Statistical Tests and Models</span>"
    ]
  },
  {
    "objectID": "stats.html#an-example-with-nyc-street-flood",
    "href": "stats.html#an-example-with-nyc-street-flood",
    "title": "7  Statistical Tests and Models",
    "section": "7.6 An Example with NYC Street Flood",
    "text": "7.6 An Example with NYC Street Flood\nWe use the NYC street flood data from the mid-term project. This analysis focuses on two sewer-related complaints in 2024: Street Flooding (SF) and Catch Basin (CB). SF complaints serve as a practical indicator of street flooding, while CB complaints provide insights into a key infrastructural factor—when catch basins fail to drain rainwater properly due to blockages or structural issues, water accumulates on the streets\nLet’s reformat the data to create count times series of SF and CB complaints by zip code.\n\nimport pandas as pd\n# Reload the dataset, ensuring 'Incident Zip' is read as a string from the start\ndf = pd.read_csv(\"data/nycflood2024.csv\",\n                 dtype={\"Incident Zip\": str}, parse_dates=[\"Created Date\"])\n\n# Filter for incidents in 2024, but also include 2023-12-31 for lag calculation\ndf_2024 = df[(df[\"Created Date\"] &gt;= \"2023-12-31\") &\n                           (df[\"Created Date\"] &lt;= \"2024-12-31\")].copy()\n\n# Extract date and ensure proper formatting\ndf_2024[\"Date\"] = df_2024[\"Created Date\"].dt.date\ndf_2024[\"Zipcode\"] = df_2024[\"Incident Zip\"].astype(str)\n\n# Identify complaint types\ndf_2024[\"SFcount\"] = df_2024[\"Descriptor\"].\\\nstr.contains(\"Street Flooding\", na=False).astype(int)\ndf_2024[\"CBcount\"] = df_2024[\"Descriptor\"].\\\nstr.contains(\"Catch Basin Clogged\", na=False).astype(int)\n\n# Aggregate counts by zip code and date\ndf_grouped = df_2024.groupby([\"Zipcode\", \"Date\"])[[\"SFcount\", \"CBcount\"]].sum().reset_index()\n\n# Generate a full range of dates including 2023-12-31 for lag calculation\nall_dates = pd.date_range(start=\"2023-12-31\", end=\"2024-12-31\")\nall_zipcodes = df_grouped[\"Zipcode\"].unique()\n\n# Create a complete grid of all zip codes and dates\nmulti_index = pd.MultiIndex.from_product([all_zipcodes, all_dates], names=[\"Zipcode\", \"Date\"])\nfull_df = pd.DataFrame(index=multi_index).reset_index()\n\n# Ensure 'Date' is in datetime format\nfull_df[\"Date\"] = pd.to_datetime(full_df[\"Date\"])\ndf_grouped[\"Date\"] = pd.to_datetime(df_grouped[\"Date\"])\n\n# Merge to include all combinations and fill missing values with 0\ndf_final = full_df.merge(df_grouped, on=[\"Zipcode\", \"Date\"], how=\"left\").fillna(0)\n\n# Convert counts to integers\ndf_final[\"SFcount\"] = df_final[\"SFcount\"].astype(int)\ndf_final[\"CBcount\"] = df_final[\"CBcount\"].astype(int)\n\n# Add lag-1 variable for CBcount\ndf_final[\"CBcount_Lag1\"] = df_final.groupby(\"Zipcode\")[\"CBcount\"].shift(1)\n\ndf_final.head()\n\n/var/folders/cq/5ysgnwfn7c3g0h46xyzvpj800000gn/T/ipykernel_20799/2628519755.py:3: UserWarning:\n\nCould not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n\n\n\n\n\n\n\n\n\n\nZipcode\nDate\nSFcount\nCBcount\nCBcount_Lag1\n\n\n\n\n0\n10001\n2023-12-31\n0\n0\nNaN\n\n\n1\n10001\n2024-01-01\n0\n0\n0.0\n\n\n2\n10001\n2024-01-02\n0\n0\n0.0\n\n\n3\n10001\n2024-01-03\n0\n0\n0.0\n\n\n4\n10001\n2024-01-04\n0\n0\n0.0\n\n\n\n\n\n\n\nNow let’s fit a Poisson model.\n\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\n# Filter out 2023-12-31 since it has missing values for lagged CBcount\ndf_model = df_final[df_final[\"Date\"] &gt;= \"2024-01-01\"].copy()\n\n# Fit Poisson regression\npoisson_model = smf.glm(\"SFcount ~ CBcount_Lag1\", \n                         data=df_model, \n                         family=sm.families.Poisson()).fit()\n\npoisson_model.summary()\n\n\nGeneralized Linear Model Regression Results\n\n\nDep. Variable:\nSFcount\nNo. Observations:\n67344\n\n\nModel:\nGLM\nDf Residuals:\n67342\n\n\nModel Family:\nPoisson\nDf Model:\n1\n\n\nLink Function:\nLog\nScale:\n1.0000\n\n\nMethod:\nIRLS\nLog-Likelihood:\n-13032.\n\n\nDate:\nWed, 02 Apr 2025\nDeviance:\n21120.\n\n\nTime:\n19:47:49\nPearson chi2:\n1.50e+05\n\n\nNo. Iterations:\n21\nPseudo R-squ. (CS):\n0.003481\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n-3.1437\n0.018\n-170.303\n0.000\n-3.180\n-3.107\n\n\nCBcount_Lag1\n0.2232\n0.009\n26.008\n0.000\n0.206\n0.240\n\n\n\n\n\nThen we fit a NB model.\n\n# Fit Negative Binomial regression\nnb_model = smf.negativebinomial(\"SFcount ~ CBcount_Lag1\", \n                    data=df_model).fit()\n\nnb_model.summary()\n\nOptimization terminated successfully.\n         Current function value: 0.193513\n         Iterations: 1\n         Function evaluations: 7\n         Gradient evaluations: 7\n\n\n/Users/junyan/work/teaching/ids-s25/ids-s25/.ids-s25/lib/python3.13/site-packages/statsmodels/base/model.py:595: HessianInversionWarning:\n\nInverting hessian failed, no bse or cov_params available\n\n\n\n\nNegativeBinomial Regression Results\n\n\nDep. Variable:\nSFcount\nNo. Observations:\n67344\n\n\nModel:\nNegativeBinomial\nDf Residuals:\n67342\n\n\nMethod:\nMLE\nDf Model:\n1\n\n\nDate:\nWed, 02 Apr 2025\nPseudo R-squ.:\n-0.1198\n\n\nTime:\n19:47:51\nLog-Likelihood:\n-13032.\n\n\nconverged:\nTrue\nLL-Null:\n-11638.\n\n\nCovariance Type:\nnonrobust\nLLR p-value:\n1.000\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n-3.1437\nnan\nnan\nnan\nnan\nnan\n\n\nCBcount_Lag1\n0.2232\nnan\nnan\nnan\nnan\nnan\n\n\nalpha\n2.252e-10\nnan\nnan\nnan\nnan\nnan\n\n\n\n\n\nNote that smf.glm does not allow the dispersion parameter to be estimated; instead, it is fixed at 1.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTibshirani, R. (1996). Regression shrinkage and selection via the LASSO. Journal of the Royal Statistical Society: Series B (Methodological), 58(1), 267–288.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Statistical Tests and Models</span>"
    ]
  },
  {
    "objectID": "machinelearning.html",
    "href": "machinelearning.html",
    "title": "8  Machine Learning: Overview",
    "section": "",
    "text": "8.1 Introduction\nMachine Learning (ML) is a branch of artificial intelligence that enables systems to learn from data and improve their performance over time without being explicitly programmed. At its core, machine learning algorithms aim to identify patterns in data and use those patterns to make decisions or predictions.\nMachine learning can be categorized into three main types: supervised learning, unsupervised learning, and reinforcement learning. Each type differs in the data it uses and the learning tasks it performs, addressing addresses different tasks and problems. Supervised learning aims to predict outcomes based on labeled data, unsupervised learning focuses on discovering hidden patterns within the data, and reinforcement learning centers around learning optimal actions through interaction with an environment.\nLet’s define some notations to introduce them:",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Machine Learning: Overview</span>"
    ]
  },
  {
    "objectID": "machinelearning.html#introduction",
    "href": "machinelearning.html#introduction",
    "title": "8  Machine Learning: Overview",
    "section": "",
    "text": "\\(X\\): A set of feature vectors representing the input data. Each element \\(X_i\\) corresponds to a set of features or attributes that describe an instance of data.\n\\(Y\\): A set of labels or rewards associated with outcomes. In supervised learning, \\(Y\\) is used to evaluate the correctness of the model’s predictions. In reinforcement learning, \\(Y\\) represents the rewards that guide the learning process.\n\\(A\\): A set of possible actions in a given context. In reinforcement learning, actions \\(A\\) represent choices that can be made in response to a given situation, with the goal of maximizing a reward.\n\n\n8.1.1 Supervised Learning\nSupervised learning is the most widely used type of machine learning. In supervised learning, we have both feature vectors \\(X\\) and their corresponding labels \\(Y\\). The objective is to train a model that can predict \\(Y\\) based on \\(X\\). This model is trained on labeled examples, where the correct outcome is known, and it adjusts its internal parameters to minimize the error in its predictions, which occurs as part of the cross-validation process.\nKey tasks in supervised learning include:\n\nClassification: Assigning data points to predefined categories or classes.\nRegression: Predicting a continuous value based on input data.\n\nIn supervised learning, the data consists of both feature vectors \\(X\\) and labels \\(Y\\), namely, \\((X, Y)\\).\n\n\n8.1.2 Unsupervised Learning\nUnsupervised learning involves learning patterns from data without any associated labels or outcomes. The objective is to explore and identify hidden structures in the feature vectors \\(X\\). Since there are no ground-truth labels \\(Y\\) to guide the learning process, the algorithm must discover patterns on its own. This is particularly useful when subject matter experts are unsure of common properties within a data set.\nCommon tasks in unsupervised learning include:\n\nClustering: Grouping similar data points together based on certain features.\nDimension Reduction: Simplifying the input data by reducing the number of features while preserving essential patterns.\n\nIn unsupervised learning, the data consists solely of feature vectors \\(X\\).\n\n\n8.1.3 Reinforcement Learning\nReinforcement learning involves learning how to make a sequence of decisions to maximize a cumulative reward. Unlike supervised learning, where the model learns from a static dataset of labeled examples, reinforcement learning involves an agent that interacts with an environment by taking actions \\(A\\), receiving feedback in the form of rewards \\(Y\\), and learning over time which actions lead to the highest cumulative reward.\nThe process in reinforcement learning involves:\n\nStates: The context or environment the agent is in, represented by feature vectors \\(X\\).\nActions: The set of possible choices the agent can make in response to the current state, denoted as \\(A\\).\nRewards: Feedback the agent receives after taking an action, which guides the learning process.\n\nIn reinforcement learning, the data consists of feature vectors \\(X\\), actions \\(A\\), and rewards \\(Y\\), namely, \\((X, A, Y)\\).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Machine Learning: Overview</span>"
    ]
  },
  {
    "objectID": "machinelearning.html#bias-variance-tradeoff",
    "href": "machinelearning.html#bias-variance-tradeoff",
    "title": "8  Machine Learning: Overview",
    "section": "8.2 Bias-Variance Tradeoff",
    "text": "8.2 Bias-Variance Tradeoff\nThe variance-bias trade-off is a core concept in machine learning that explains the relationship between the complexity of a model, its performance on training data, and its ability to generalize to unseen data. It applies to both supervised and unsupervised learning, though it manifests differently in each.\n\n8.2.1 Bias\nBias refers to the error introduced by approximating a complex real-world problem with a simplified model. A model with high bias makes strong assumptions about the data, leading to oversimplified patterns and poor performance on both the training data and new data. High bias results in underfitting, where the model fails to capture important trends in the data.\n\nExample (Supervised): In supervised learning, using a linear regression model to fit data that has a nonlinear relationship results in high bias because the model cannot capture the complexity of the data.\nExample (Unsupervised): In clustering (an unsupervised task), setting the number of clusters too low (e.g., forcing data into two clusters when more exist) leads to high bias, as the model oversimplifies the underlying structure.\n\n\n\n8.2.2 Variance\nVariance refers to the model’s sensitivity to small changes in the training data. A model with high variance will adapt closely to the training data, potentially capturing noise or fluctuations that are not representative of the general data distribution. High variance leads to overfitting, where the model performs well on training data but poorly on new, unseen data.\n\nExample (Supervised): A decision tree with many branches can exhibit high variance. The model perfectly fits the training data but may perform poorly on test data because it overfits to specific idiosyncrasies in the training set.\nExample (Unsupervised): In clustering, setting the number of clusters too high or fitting overly flexible cluster shapes (e.g., in Gaussian Mixture Models) can lead to overfitting, where the model captures noise and splits data unnecessarily.\n\n\n\n8.2.3 The Trade-Off\nThe bias-variance trade-off reflects the tension between bias and variance. As model complexity increases:\n\nBias decreases: The model becomes more flexible and can capture more details of the data.\nVariance increases: The model becomes more sensitive to the particular training data, potentially capturing noise.\n\nConversely, a simpler model will:\n\nHave high bias: It may not capture all relevant patterns in the data.\nHave low variance: It will be less sensitive to fluctuations in the data and is more likely to generalize well to unseen data.\n\n\n\n8.2.4 Bias-Variance in Supervised Learning\nIn supervised learning, the goal is to strike the right balance between bias and variance to minimize prediction error. This balance is critical for developing models that generalize well to new data. The total error of a model can be decomposed into:\n\\[\n\\text{Total Error} = \\text{Bias}^2 + \\text{Variance}\n+\\text{Irreducible Error}.\n\\]\n\nBias: The error from using a model that is too simple.\nVariance: The error from using a model that is too complex and overfits the training data.\nIrreducible Error: This is the noise inherent in the data itself, which cannot be eliminated no matter how well the model is tuned.\n\n\n\n8.2.5 Bias-Variance in Unsupervised Learning\nIn unsupervised learning, the bias-variance trade-off is less formally defined but still relevant. For example:\n\nIn clustering, choosing the wrong number of clusters can lead to either high bias (too few clusters, oversimplifying the data) or high variance (too many clusters, overfitting the data).\nIn dimensionality reduction, keeping too few components in Principal Component Analysis (PCA) increases bias by losing important information, while keeping too many components retains noise, increasing variance.\n\nIn unsupervised learning, balancing bias and variance typically involves tuning hyperparameters (e.g., number of clusters, number of components) to find the right complexity level.\n\n\n8.2.6 Striking the Right Balance\nTo strike the balance between bias and variance in both supervised and unsupervised learning, techniques such as regularization, early stopping, cross-validation, and hyperparameter tuning are essential. These techniques help ensure the model is complex enough to capture patterns in the data but not so complex that it overfits to noise or irrelevant details.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Machine Learning: Overview</span>"
    ]
  },
  {
    "objectID": "machinelearning.html#crossvalidation",
    "href": "machinelearning.html#crossvalidation",
    "title": "8  Machine Learning: Overview",
    "section": "8.3 Crossvalidation",
    "text": "8.3 Crossvalidation\nCross-validation is a technique used to evaluate machine learning models and tune hyperparameters by splitting the dataset into multiple subsets. This approach helps to avoid overfitting and provides a better estimate of the model’s performance on unseen data. Cross-validation is especially useful for managing the bias-variance trade-off by allowing you to test how well the model generalizes without relying on a single train-test split.\n\n8.3.1 K-Fold Cross-Validation\nThe most commonly used method is \\(k\\)-fold cross-validation:\n\nSplit the data: The dataset is divided into \\(k\\) equally-sized folds (subsets).\nTrain on \\(k - 1\\) folds: The model is trained on \\(k - 1\\) folds, leaving one fold as a test set.\nTest on the remaining fold: The model’s performance is evaluated on the fold that was left out.\nRepeat: This process is repeated \\(k\\) times, with each fold used once as the test set.\nAverage performance: The final cross-validation score is the average performance across all \\(k\\) iterations.\n\nBy averaging the results across multiple test sets, cross-validation provides a more robust estimate of model performance and helps avoid overfitting or underfitting to any particular training-test split.\nLeave-One-Out Cross-Validation (LOOCV) takes each observation as one fold. The dataset is split into \\(n\\) subsets (where \\(n\\) is the sample size), with each sample acting as a test set once. While this method provides the most exhaustive evaluation, it can be computationally expensive for large datasets.\n\n\n8.3.2 Benefits of Cross-Validation\n\nPrevents overfitting: By testing the model on multiple subsets of data, cross-validation helps to identify if the model is too complex and overfits to the training data.\nPrevents underfitting: If the model performs poorly across all folds, it may indicate that the model is too simple (high bias).\nBetter estimation: Cross-validation gives a better estimate of how the model will perform on unseen data compared to a single train-test split.\n\n\n\n8.3.3 Cross-Validation in Unsupervised Learning\nWhile cross-validation is most commonly used in supervised learning, it can also be applied to unsupervised learning through:\n\nStability testing: Running unsupervised algorithms (e.g., clustering) on different data splits and measuring the stability of the results (e.g., using the silhouette score).\nInternal validation metrics: In clustering, internal metrics like the silhouette score or Davies-Bouldin index can be used to evaluate the quality of clustering across different data splits.\n\nThe bias-variance trade-off is a universal problem in machine learning, affecting both supervised and unsupervised models. Cross-validation is a powerful tool for controlling this trade-off by providing a reliable estimate of model performance and helping to fine-tune model complexity. By balancing bias and variance through careful model selection, regularization, and cross-validation, you can develop models that generalize well to unseen data without overfitting or underfitting.\n\n\n8.3.4 A Curve-Fitting with Splines: An Example\nOverfitting occurs when a model becomes overly complex and starts to capture not just the underlying patterns in the data but also the noise or random fluctuations. This can lead to poor generalization to new, unseen data. A clear sign of overfitting is when a model performs very well on the training data but performs poorly on test data, as it fails to generalize beyond the data it was trained on.\nIn this example, we illustrate overfitting using cubic spline regression with different numbers of knots. Splines are a flexible tool that allow for piecewise polynomial regression, with knots defining where the pieces of the polynomial meet. The more knots we use, the more flexible the model becomes, which can potentially lead to overfitting.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import SplineTransformer\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import mean_squared_error\n\ndef true_function(X):\n    return np.sin(1.5 * X) + 0.5 * np.cos(0.5 * X) + np.sin(2 * X)\n\n# Generate synthetic data using the more complex true function\nX = np.sort(np.random.rand(200) * 10).reshape(-1, 1)\ny = true_function(X).ravel() + np.random.normal(0, 0.2, X.shape[0])\n\n# Split data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Function to explore overfitting by plotting both errors and fitted curves\ndef overfitting_example_with_fitted_curves(X_train, y_train, X_test, y_test, knots_list):\n    train_errors = []\n    test_errors = []\n    \n    # Generate fine grid for plotting the true curve and fitted models\n    X_line = np.linspace(0, 10, 1000).reshape(-1, 1)\n    y_true = true_function(X_line)\n    \n    # Plot the true function and observed data\n    plt.figure(figsize=(10, 6))\n    plt.scatter(X_train, y_train, color='blue', label='Training data', alpha=0.6)\n    plt.plot(X_line, y_true, label='True function', color='black', linestyle='--')\n    \n    for n_knots in knots_list:\n        # Create a spline model with fixed degree = 3 (cubic) and varying knots\n        spline = SplineTransformer(degree=3, n_knots=n_knots, include_bias=False)\n        model = make_pipeline(spline, LinearRegression())\n        \n        # Fit the model to training data\n        model.fit(X_train, y_train)\n        \n        # Predict on training and test data\n        y_pred_train = model.predict(X_train)\n        y_pred_test = model.predict(X_test)\n        y_pred = model.predict(X_line)\n        \n        # Calculate training and test errors (mean squared error)\n        train_errors.append(mean_squared_error(y_train, y_pred_train))\n        test_errors.append(mean_squared_error(y_test, y_pred_test))\n        \n        # Plot the fitted curve\n        plt.plot(X_line, y_pred, label=f'{n_knots} Knots (Fit)', alpha=0.7)\n\n    print(train_errors, test_errors)\n    \n    plt.title('Overfitting Example: Fitted Curves and Observed Data')\n    plt.xlabel('X')\n    plt.ylabel('y')\n    plt.legend()\n    plt.show()\n    \n    # Plot training and test error separately\n    plt.figure(figsize=(10, 6))\n    plt.plot(knots_list, train_errors, label='Training Error', marker='o', color='blue')\n    plt.plot(knots_list, test_errors, label='Test Error', marker='o', color='red')\n    plt.title('Training vs. Test Error with Varying Knots')\n    plt.xlabel('Number of Knots')\n    plt.ylabel('Mean Squared Error')\n    plt.legend()\n    plt.show()\n\n# Explore overfitting by varying the number of knots and overlaying the fitted curves\nknots_list = [6, 8, 10, 12, 14, 16]\noverfitting_example_with_fitted_curves(X_train, y_train, X_test, y_test, knots_list)\n\n[0.0941038034348758, 0.03647270930197563, 0.02687090498268356, 0.02675769996764363, 0.025503734093287652, 0.025143537696468987] [0.10692810563807131, 0.04663988297046514, 0.04752621151729337, 0.042686730415836274, 0.04142771803397926, 0.043598382263467385]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn the first plot, we fit cubic splines with 2, 4, 6, 8, 10, and 12 knots to the data. As the number of knots increases, the model becomes more flexible and better able to fit the training data. With only 2 knots, the model is quite smooth and underfits the data, capturing only broad trends but missing the detailed structure of the true underlying function. With 4 or 6 knots, the model begins to capture more of the data’s structure, balancing the bias-variance trade-off effectively. However, as we increase the number of knots to 10 and 12, the model becomes too flexible. It starts to fit the noise in the training data, producing a curve that adheres too closely to the data points. This is a classic case of overfitting: the model fits the training data very well, but it no longer generalizes to new data.\nIn the second plot, we observe the training error and test error as the number of knots increases. As expected, the training error consistently decreases as the number of knots increases, since a more complex model can fit the training data better. However, the test error tells a different story. Initially, the test error decreases as the model becomes more flexible, indicating that the model is learning meaningful patterns from the data. But after a certain point, the test error begins to increase, signaling overfitting.\nThis is a key insight into the bias-variance trade-off. While adding more complexity (in this case, more knots) reduces bias and improves fit on the training data, it also increases variance, making the model more sensitive to fluctuations and noise in the data. This results in worse performance on test data, as the model becomes too specialized to the training set.\nThe example clearly demonstrates how overfitting can occur when the model becomes too complex. In practice, it’s important to find a balance between underfitting (high bias) and overfitting (high variance). Techniques such as cross-validation, regularization, or limiting model complexity (e.g., setting a reasonable number of knots in spline regression) can help manage this trade-off and produce models that generalize well to unseen data.\nBy tuning the number of knots or other model parameters, we can achieve a model that strikes the right balance, capturing the true patterns in the data without fitting the noise.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Machine Learning: Overview</span>"
    ]
  },
  {
    "objectID": "supervised.html",
    "href": "supervised.html",
    "title": "9  Supervised Learning",
    "section": "",
    "text": "9.1 Decision Trees: Foundation\nDecision trees are widely used supervised learning models that predict the value of a target variable by iteratively splitting the dataset based on decision rules derived from input features. The model functions as a piecewise constant approximation of the target function, producing clear, interpretable rules that are easily visualized and analyzed (Breiman et al., 1984). Decision trees are fundamental in both classification and regression tasks, serving as the building blocks for more advanced ensemble models such as Random Forests and Gradient Boosting Machines.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Supervised Learning</span>"
    ]
  },
  {
    "objectID": "supervised.html#decision-trees-foundation",
    "href": "supervised.html#decision-trees-foundation",
    "title": "9  Supervised Learning",
    "section": "",
    "text": "9.1.1 Algorithm Formulation\nThe core mechanism of a decision tree algorithm is the identification of optimal splits that partition the data into subsets that are increasingly homogeneous with respect to the target variable. At any node \\(m\\), the data subset is denoted as \\(Q_m\\) with a sample size of \\(n_m\\). The objective is to find a candidate split \\(\\theta\\), defined as a threshold for a given feature, that minimizes an impurity or loss measure \\(H\\).\nWhen a split is made at node \\(m\\), the data is divided into two subsets: \\(Q_{m,l}\\) (left node) with sample size \\(n_{m,l}\\), and \\(Q_{m,r}\\) (right node) with sample size \\(n_{m,r}\\). The split quality, measured by \\(G(Q_m, \\theta)\\), is given by:\n\\[\nG(Q_m, \\theta) = \\frac{n_{m,l}}{n_m} H(Q_{m,l}(\\theta)) +\n\\frac{n_{m,r}}{n_m} H(Q_{m,r}(\\theta)).\n\\]\nThe algorithm aims to identify the split that minimizes the impurity:\n\\[\n\\theta^* = \\arg\\min_{\\theta} G(Q_m, \\theta).\n\\]\nThis process is applied recursively at each child node until a stopping condition is met.\n\nStopping Criteria: The algorithm stops when the maximum tree depth is reached or when the node sample size falls below a preset threshold.\nPruning: Reduce the complexity of the final tree by removing branches that add little predictive value. This reduces overfitting and improves the generalization accuracy of the model.\n\n\n\n9.1.2 Search Space for Possible Splits\nAt each node in the decision tree, the search space for possible splits comprises all features in the dataset and potential thresholds derived from the values of each feature. For a given feature, the algorithm considers each unique value in the current node’s subset as a possible split point. The potential thresholds are typically set as midpoints between consecutive unique values, ensuring the data is partitioned effectively.\nFormally, let the feature set be \\(\\{X_1, X_2, \\ldots, X_p\\}\\), where \\(p\\) is the total number of features, and let the unique values of feature \\(X_j\\) at node \\(m\\) be denoted by \\(\\{v_{j,1}, v_{j,2}, \\ldots, v_{j,k_j}\\}\\). The search space at node \\(m\\) includes:\n\nFeature candidates: \\(\\{X_1, X_2, \\ldots, X_p\\}\\).\nThreshold candidates for \\(X_j\\): \\[\n\\left\\{ \\frac{v_{j,i} + v_{j,i+1}}{2} \\mid 1 \\leq i &lt; k_j \\right\\}.\n\\]\n\nThe search space therefore encompasses all combinations of features and their respective thresholds. While the complexity of this search can be substantial, particularly for high-dimensional data or features with numerous unique values, efficient algorithms use sorting and single-pass scanning techniques to mitigate the computational cost.\n\n\n9.1.3 Metrics\n\n9.1.3.1 Classification\nIn decision tree classification, several criteria can be used to measure the quality of a split at each node. These criteria are based on how “pure” the resulting nodes are after the split. A pure node contains samples that predominantly belong to a single class. The goal is to minimize impurity, leading to nodes that are as homogeneous as possible.\n\nGini Index: The Gini index measures the impurity of a node by calculating the probability of randomly choosing two different classes. A perfect split (all instances belong to one class) has a Gini index of 0. At node \\(m\\), the Gini index is \\[\nH(Q_m) = \\sum_{k=1}^{K} p_{mk} (1 - p_{mk}),\n\\] where \\(p_{mk}\\) is the proportion of samples of class \\(k\\) at node \\(m\\); and\\(K\\) is the total number of classes The Gini index is often preferred for its speed and simplicity, and it’s used by default in many implementations of decision trees, including sklearn.\nEntropy (Information Gain): Entropy is another measure of impurity, derived from information theory. It quantifies the “disorder” of the data at a node. Lower entropy means higher purity. At node \\(m\\), it is defined as \\[\nH(Q_m) = - \\sum_{k=1}^{K} p_{mk} \\log p_{mk}\n\\] Entropy is commonly used in decision tree algorithms like ID3 and C4.5. The choice between Gini and entropy often depends on specific use cases, but both perform similarly in practice.\nMisclassification Error: Misclassification error focuses solely on the most frequent class in the node. It measures the proportion of samples that do not belong to the majority class. Although less sensitive than Gini and entropy, it can be useful for classification when simplicity is preferred. At node \\(m\\), it is defined as \\[\nH(Q_m) = 1 - \\max_k p_{mk},\n\\] where \\(\\max_k p_{mk}\\) is the largest proportion of samples belonging to any class \\(k\\).\n\n\n\n9.1.3.2 Regression Criteria\nIn decision tree regression, different criteria are used to assess the quality of a split. The goal is to minimize the spread or variance of the target variable within each node.\n\nMean Squared Error (MSE): Mean squared error is the most common criterion used in regression trees. It measures the average squared difference between the actual values and the predicted values (mean of the target in the node). The smaller the MSE, the better the fit. At node \\(m\\), it is \\[\nH(Q_m) = \\frac{1}{n_m} \\sum_{i=1}^{n_m} (y_i - \\bar{y}_m)^2,\n\\] where\n\n\\(y_i\\) is the actual value for sample \\(i\\);\n\\(\\bar{y}_m\\) is the mean value of the target at node \\(m\\);\n\\(n_m\\) is the number of samples at node \\(m\\).\n\nMSE works well when the target is continuous and normally distributed.\nHalf Poisson Deviance (for count targets): When dealing with count data, the Poisson deviance is used to model the variance in the number of occurrences of an event. It is well-suited for target variables representing counts (e.g., number of occurrences of an event). At node \\(m\\), it is \\[\nH(Q_m) = \\sum_{i=1}^{n_m} \\left( y_i \\log\\left(\\frac{y_i}{\\hat{y}_i}\\right) - (y_i - \\hat{y}_i) \\right),\n\\] where \\(\\hat{y}_i\\) is the predicted count. This criterion is especially useful when the target variable represents discrete counts, such as predicting the number of occurrences of an event.\nMean Absolute Error (MAE): Mean absolute error is another criterion that minimizes the absolute differences between actual and predicted values. While it is more robust to outliers than MSE, it is slower computationally due to the lack of a closed-form solution for minimization. At node \\(m\\), it is \\[\nH(Q_m) = \\frac{1}{n_m} \\sum_{i=1}^{n_m} |y_i - \\bar{y}_m|.\n\\] MAE is useful when you want to minimize large deviations and can be more robust in cases where outliers are present in the data.\n\n\n\n9.1.3.3 Summary\nIn decision trees, the choice of splitting criterion depends on the type of task (classification or regression) and the nature of the data. For classification tasks, the Gini index and entropy are the most commonly used, with Gini offering simplicity and speed, and entropy providing a more theoretically grounded approach. Misclassification error can be used for simpler cases. For regression tasks, MSE is the most popular choice, but Poisson deviance and MAE are useful for specific use cases such as count data and robust models, respectively.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Supervised Learning</span>"
    ]
  },
  {
    "objectID": "supervised.html#gradient-boosted-models",
    "href": "supervised.html#gradient-boosted-models",
    "title": "9  Supervised Learning",
    "section": "9.2 Gradient-Boosted Models",
    "text": "9.2 Gradient-Boosted Models\nGradient boosting is a powerful ensemble technique in machine learning that combines multiple weak learners into a strong predictive model. Unlike bagging methods, which train models independently, gradient boosting fits models sequentially, with each new model correcting errors made by the previous ensemble (Friedman, 2001). While decision trees are commonly used as weak learners, gradient boosting can be generalized to other base models. This iterative method optimizes a specified loss function by repeatedly adding models designed to reduce residual errors.\n\n9.2.1 Introduction\nGradient boosting builds on the general concept of boosting, aiming to construct a strong predictor from an ensemble of sequentially trained weak learners. The weak learners are often shallow decision trees (stumps), linear models, or generalized additive models (Hastie et al., 2009). Each iteration adds a new learner focusing primarily on the data points poorly predicted by the existing ensemble, thereby progressively enhancing predictive accuracy.\nGradient boosting’s effectiveness stems from:\n\nError Correction: Each iteration specifically targets previous errors, refining predictive accuracy.\nWeighted Learning: Iteratively focuses more heavily on difficult-to-predict data points.\nFlexibility: Capable of handling diverse loss functions and various types of predictive tasks.\n\nThe effectiveness of gradient-boosted models has made them popular across diverse tasks, including classification, regression, and ranking. Gradient boosting forms the foundation for algorithms such as XGBoost (Chen & Guestrin, 2016), LightGBM (Ke et al., 2017), and CatBoost (Prokhorenkova et al., 2018), known for their high performance and scalability.\n\n\n9.2.2 Gradient Boosting Process\nGradient boosting builds an ensemble by iteratively minimizing the residual errors from previous models. This iterative approach optimizes a loss function, \\(L(y, F(x))\\), where \\(y\\) represents the observed target variable and \\(F(x)\\) the model’s prediction for a given feature vector \\(x\\).\nKey concepts:\n\nLoss Function: Guides model optimization, such as squared error for regression or logistic loss for classification.\nLearning Rate: Controls incremental updates, balancing training speed and generalization.\nRegularization: Reduces overfitting through tree depth limitation, subsampling, and L1/L2 penalties.\n\n\n9.2.2.1 Model Iteration\nThe gradient boosting algorithm proceeds as follows:\n\nInitialization: Define a base model \\(F_0(x)\\), typically the mean of the target variable for regression or the log-odds for classification.\nIterative Boosting: At each iteration \\(m\\):\n\nCompute pseudo-residuals representing the negative gradient of the loss function at the current predictions. For each observation \\(i\\): \\[\nr_i^{(m)} = -\\left.\\frac{\\partial L(y_i, F(x_i))}{\\partial F(x_i)}\\right|_{F(x)=F_{m-1}(x)},\n\\] where \\(x_i\\) and \\(y_i\\) denote the feature vector and observed value for the \\(i\\)-th observation, respectively.\nFit a new weak learner \\(h_m(x)\\) to these residuals.\nUpdate the model: \\[\nF_m(x) = F_{m-1}(x) + \\eta \\, h_m(x),\n\\] where \\(\\eta\\) is a small positive learning rate (e.g., 0.01–0.1), controlling incremental improvement and reducing overfitting.\n\nFinal Model: After \\(M\\) iterations, the ensemble model is: \\[\n  F_M(x) = F_0(x) + \\sum_{m=1}^M \\eta \\, h_m(x).\n  \\]\n\nStochastic gradient boosting is a variant that enhances gradient boosting by introducing randomness through subsampling at each iteration, selecting a random fraction of data points (typically 50%–80%) to fit the model (Friedman, 2002). This randomness helps reduce correlation among trees, improve model robustness, and reduce the risk of overfitting.\n\n\n\n9.2.3 Demonstration\nHere’s a practical example using scikit-learn to demonstrate gradient boosting on the California housing dataset. First, import necessary libraries and load the data:\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\n# Load data\nhousing = fetch_california_housing(as_frame=True)\nX, y = housing.data, housing.target\n\nNext, split the dataset into training and testing sets:\n\n# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=20250407\n)\n\nThen, set up and train a stochastic gradient boosting model:\n\n# Gradient Boosting Model with stochastic subsampling\ngbm = GradientBoostingRegressor(\n    n_estimators=200,\n    learning_rate=0.1,\n    max_depth=3,\n    subsample=0.7,  # stochastic gradient boosting\n    random_state=20250408\n)\n\ngbm.fit(X_train, y_train)\n\nGradientBoostingRegressor(n_estimators=200, random_state=20250408,\n                          subsample=0.7)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GradientBoostingRegressor?Documentation for GradientBoostingRegressoriFittedGradientBoostingRegressor(n_estimators=200, random_state=20250408,\n                          subsample=0.7) \n\n\nFinally, make predictions and evaluate the model performance:\n\n# Predictions\ny_pred = gbm.predict(X_test)\n\n# Evaluate\nmse = mean_squared_error(y_test, y_pred)\nprint(f\"Test MSE: {mse:.4f}\")\n\nTest MSE: 0.2697\n\n\n\n\n9.2.4 XGBoost: Extreme Gradient Boosting\nXGBoost is a scalable and efficient implementation of gradient-boosted decision trees (Chen & Guestrin, 2016). It has become one of the most widely used machine learning methods for structured data due to its high predictive performance, regularization capabilities, and speed. XGBoost builds an ensemble of decision trees in a stage-wise fashion, minimizing a regularized objective that balances training loss and model complexity.\nThe core idea of XGBoost is to fit each new tree to the gradient of the loss function with respect to the model’s predictions. Unlike traditional boosting algorithms like AdaBoost, which use only first-order gradients, XGBoost optionally uses second-order derivatives (Hessians), enabling better convergence and stability (Friedman, 2001).\nXGBoost is widely used in data science competitions and real-world applications. It supports regularization (L1 and L2), handles missing values internally, and is designed for distributed computing.\nXGBoost builds upon the same foundational idea as gradient boosted machines—sequentially adding trees to improve the predictive model— but introduces a number of enhancements:\n\n\n\n\n\n\n\n\nAspect\nTraditional GBM\nXGBoost\n\n\n\n\nImplementation\nBasic gradient boosting\nOptimized, regularized boosting\n\n\nRegularization\nShrinkage only\nL1 and L2 regularization\n\n\nLoss Optimization\nFirst-order gradients\nFirst- and second-order\n\n\nMissing Data\nRequires manual imputation\nHandled automatically\n\n\nTree Construction\nDepth-wise\nLevel-wise (faster)\n\n\nParallelization\nLimited\nBuilt-in\n\n\nSparsity Handling\nNo\nYes\n\n\nObjective Functions\nFew options\nCustom supported\n\n\nCross-validation\nExternal via GridSearchCV\nBuilt-in xgb.cv\n\n\n\nXGBoost is therefore more suitable for large-scale problems and provides better generalization performance in many practical tasks.\n\n\n\n\n\n\nBreiman, L., Friedman, J. H., Olshen, R., & Stone, C. J. (1984). Classification and regression trees. Wadsworth.\n\n\nChen, T., & Guestrin, C. (2016). XGBoost: A scalable tree boosting system. Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 785–794. https://doi.org/10.1145/2939672.2939785\n\n\nFriedman, J. H. (2001). Greedy function approximation: A gradient boosting machine. The Annals of Statistics, 29(5), 1189–1232.\n\n\nFriedman, J. H. (2002). Stochastic gradient boosting. Computational Statistics & Data Analysis, 38(4), 367–378.\n\n\nHastie, T., Tibshirani, R., & Friedman, J. (2009). The elements of statistical learning: Data mining, inference, and prediction. Springer.\n\n\nKe, G., Meng, Q., Finley, T., Wang, T., Chen, W., Ma, W., Ye, Q., & Liu, T.-Y. (2017). LightGBM: A highly efficient gradient boosting decision tree. Advances in Neural Information Processing Systems, 3146–3154.\n\n\nProkhorenkova, L., Gusev, G., Vorobev, A., Dorogush, A. V., & Gulin, A. (2018). CatBoost: Unbiased boosting with categorical features. Advances in Neural Information Processing Systems, 6638–6648.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Supervised Learning</span>"
    ]
  },
  {
    "objectID": "unsupervised.html",
    "href": "unsupervised.html",
    "title": "10  Unsupervised Learning",
    "section": "",
    "text": "10.1 K-Means Clustering\nSo far, we have explored various supervised learning algorithms such as Decision Trees and Random Forests, which rely on labeled data with known outcomes. In contrast, unsupervised learning techniques analyze unlabeled data to identify patterns, making them particularly useful for clustering and association problems. Among these, K-means clustering stands out as one of the simplest and most widely used algorithms.\nK-means clustering aims to divide a dataset into non-overlapping groups based on similarity. Given a set of data points, each represented as a vector in a multi-dimensional space, the algorithm assigns each point to one of \\(k\\) clusters in a way that minimizes the variation within each cluster. This is done by reducing the sum of squared distances between each point and its assigned cluster center. Mathematically, we seek to minimize:\n\\[\\begin{equation*}\n\\sum_{i=1}^{k}\\sum_{\\boldsymbol{x}\\in S_i}\n\\left\\|\\boldsymbol{x}-\\boldsymbol{\\mu}_i\\right\\|^2\n\\end{equation*}\\]\nwhere \\(S_i\\) represents each cluster and \\(\\boldsymbol{\\mu}_i\\) is the mean of the points within that cluster.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Unsupervised Learning</span>"
    ]
  },
  {
    "objectID": "unsupervised.html#k-means-clustering",
    "href": "unsupervised.html#k-means-clustering",
    "title": "10  Unsupervised Learning",
    "section": "",
    "text": "10.1.1 Lloyd’s Algorithm\nK-means clustering is typically solved using Lloyd’s algorithm, which operates iteratively as follows:\n\nInitialization: Select \\(k\\) initial cluster centroids \\(\\boldsymbol{\\mu}_i\\) randomly.\nIteration:\n\nAssignment step: Assign each point \\(\\boldsymbol{x}\\) to the cluster whose centroid is closest based on the squared Euclidean distance.\nUpdate step: Recompute the centroids as the mean of all points assigned to each cluster:\n\\[\\begin{equation*}\n\\boldsymbol{\\mu}_i \\leftarrow \\frac{1}{|S_i|}\n\\sum_{\\boldsymbol{x}_j \\in S_i} \\boldsymbol{x}_j\n\\end{equation*}\\]\n\nTermination: The process stops when either the assignments no longer change or a predefined number of iterations is reached.\n\n\n\n10.1.2 Example: Iris Data\nK-means clustering can be implemented using the scikit-learn library. Below, we apply it to the Iris dataset.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\nfrom sklearn.cluster import KMeans\n\n\n# Load the Iris dataset\niris = datasets.load_iris()\nX = iris.data[:, :2]  # Using only two features\ny = iris.target\n\nWe visualize the observations based on their true species labels.\n\n# Scatter plot of true species labels\nfig, ax = plt.subplots()\nscatter = ax.scatter(X[:, 0], X[:, 1], c=y,\n                      cmap='viridis', edgecolors='k')\nax.legend(*scatter.legend_elements(), loc=\"upper left\",\n          title=\"Species\")\nplt.xlabel(\"Feature 1\")\nplt.ylabel(\"Feature 2\")\nplt.title(\"True Species Distribution\")\nplt.show()\n\n\n\n\n\n\n\n\nNow, we apply K-means clustering to the data.\n\n# Train K-means model\nKmean = KMeans(n_clusters=3, init='k-means++',\n               n_init=10, random_state=42)\nKmean.fit(X)\n\nKMeans(n_clusters=3, n_init=10, random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KMeans?Documentation for KMeansiFittedKMeans(n_clusters=3, n_init=10, random_state=42) \n\n\nSeveral parameters can be adjusted for better performance. See: &lt;https://scikit-learn.org/stable/modules/generated/ sklearn.cluster.KMeans.html&gt;\nK-means provides cluster centroids, representing the center of each cluster.\n\n# Print predicted cluster centers\nprint(\"Cluster Centers:\")\nprint(Kmean.cluster_centers_)\n\nCluster Centers:\n[[6.81276596 3.07446809]\n [5.77358491 2.69245283]\n [5.006      3.428     ]]\n\n\nWe plot the centroids along with clustered points.\n\n# Plot centroids on the scatter plot\nfig, ax = plt.subplots()\nax.scatter(X[:, 0], X[:, 1], c=Kmean.labels_,\n           cmap='viridis', edgecolors='k', alpha=0.5)\nax.scatter(Kmean.cluster_centers_[:, 0],\n           Kmean.cluster_centers_[:, 1],\n           c=\"black\", s=200, marker='s',\n           label=\"Centroids\")\nax.legend()\nplt.xlabel(\"Feature 1\")\nplt.ylabel(\"Feature 2\")\nplt.title(\"K-means Clustering Results\")\nplt.show()\n\n\n\n\n\n\n\n\n\n10.1.2.1 Comparing True and Predicted Labels\nBy plotting the results side by side, we can see how well K-means clustering approximates the true labels.\n\n# Compare true vs. predicted labels\nfig, axs = plt.subplots(ncols=2, figsize=(12, 5),\n                        constrained_layout=True)\n\n# True labels plot\naxs[0].scatter(X[:, 0], X[:, 1], c=y,\n               cmap='viridis', alpha=0.5,\n               edgecolors='k')\naxs[0].set_title(\"True Labels\")\naxs[0].set_xlabel(\"Feature 1\")\naxs[0].set_ylabel(\"Feature 2\")\n\n# Predicted clusters plot\naxs[1].scatter(X[:, 0], X[:, 1], c=Kmean.labels_,\n               cmap='viridis', alpha=0.5,\n               edgecolors='k')\naxs[1].scatter(Kmean.cluster_centers_[:, 0],\n               Kmean.cluster_centers_[:, 1],\n               marker=\"s\", c=\"black\", s=200,\n               alpha=1, label=\"Centroids\")\naxs[1].set_title(\"Predicted Clusters\")\naxs[1].set_xlabel(\"Feature 1\")\naxs[1].set_ylabel(\"Feature 2\")\naxs[1].legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n10.1.3 Making Predictions on New Data\nOnce trained, the model can classify new data points.\n\n# Sample test data points\nsample_test = np.array([[3, 4], [7, 4]])\n\n# Predict cluster assignment\nprint(\"Predicted Clusters:\", Kmean.predict(sample_test))\n\nPredicted Clusters: [2 0]\n\n\n\n\n10.1.4 Discussion\nK-means is intuitive but has limitations:\n\nSensitivity to initialization: Poor initialization can yield suboptimal results. k-means++ mitigates this issue.\nChoosing the number of clusters: The choice of \\(k\\) is critical. The elbow method helps determine an optimal value.\nAssumption of spherical clusters: K-means struggles when clusters have irregular shapes. Alternative methods such as kernel-based clustering may be more effective.\n\nDespite its limitations, K-means is a fundamental tool in exploratory data analysis and practical applications.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Unsupervised Learning</span>"
    ]
  },
  {
    "objectID": "unsupervised.html#stochastic-neighbor-embedding",
    "href": "unsupervised.html#stochastic-neighbor-embedding",
    "title": "10  Unsupervised Learning",
    "section": "10.2 Stochastic Neighbor Embedding",
    "text": "10.2 Stochastic Neighbor Embedding\nStochastic Neighbor Embedding (SNE) is a dimensionality reduction technique used to project high-dimensional data into a lower-dimensional space (often 2D or 3D) while preserving local neighborhoods of points. It is particularly popular for visualization tasks, helping to reveal clusters or groupings among similar points. Key characteristics include:\n\nUnsupervised: It does not require labels, relying on similarity or distance metrics among data points.\nProbabilistic framework: Pairwise distances in the original space are interpreted as conditional probabilities, which SNE attempts to replicate in the lower-dimensional space.\nCommon for exploratory data analysis: Especially useful for high-dimensional datasets such as images, text embeddings, or genetic data.\n\n\n10.2.1 Statistical Rationale\nThe core idea behind SNE is to preserve local neighborhoods of each point in the data:\n\nFor each point \\(x_i\\) in the high-dimensional space, SNE defines a conditional probability \\(p_{j|i}\\) that represents how likely \\(x_j\\) is a neighbor of \\(x_i\\).\nThe probability \\(p_{j|i}\\) is modeled using a Gaussian distribution centered on \\(x_i\\):\n\\[\np_{j|i} = \\frac{\\exp\\left(- \\| x_i - x_j \\|^2 / 2 \\sigma_i^2\\right)}{\\sum_{k \\neq i} \\exp\\left(- \\| x_i - x_k \\|^2 / 2 \\sigma_i^2\\right)},\n\\] where \\(\\sigma_i\\) is a variance parameter controlling the neighborhood size.\nEach point \\(x_i\\) is mapped to a lower-dimensional counterpart \\(y_i\\), and a corresponding probability \\(q_{j|i}\\) is defined similarly in that space.\nThe objective function minimizes the Kullback–Leibler (KL) divergence between the high-dimensional and low-dimensional conditional probabilities, encouraging a faithful representation of local neighborhoods.\n\n\n\n10.2.2 t-SNE Variation\nThe t-SNE (t-distributed Stochastic Neighbor Embedding) addresses two main issues in the original formulation of SNE:\n\nThe crowding problem: In high dimensions, pairwise distances tend to spread out; in 2D or 3D, they can crowd together. t-SNE uses a Student t-distribution (with one degree of freedom) in the low-dimensional space, which has heavier tails than a Gaussian.\nSymmetric probabilities: t-SNE symmetrizes probabilities \\(p_{ij} = (p_{j|i} + p_{i|j}) / (2N)\\), simplifying computation.\n\nThe Student t-distribution for low-dimensional similarity is given by: \\[\nq_{ij} = \\frac{\\bigl(1 + \\| y_i - y_j \\|^2 \\bigr)^{-1}}{\\sum_{k \\neq l} \\bigl(1 + \\| y_k - y_l \\|^2 \\bigr)^{-1}}.\n\\] This heavier tail ensures that distant points are not forced too close, thus reducing the crowding effect.\n\n\n10.2.3 Supervised Variation\nAlthough SNE and t-SNE are fundamentally unsupervised, it is possible to integrate label information. In a supervised variant, distances between similarly labeled points may be reduced (or differently weighted), and additional constraints can be imposed to promote class separation in the lower-dimensional embedding. These approaches can help when partial label information is available and you want to blend supervised and unsupervised insights.\n\n\n10.2.4 Demonstration with a Subset of the NIST Digits Data\nBelow is a brief example in Python using t-SNE on a small subset of the MNIST digits (which is itself a curated subset of the original NIST data).\n\nimport numpy as np\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\n\nmnist = fetch_openml('mnist_784', version=1)\nX = mnist.data[:2000]\ny = mnist.target[:2000]\n\ntsne = TSNE(n_components=2, perplexity=30, learning_rate='auto', \n            init='random', random_state=42)\nX_embedded = tsne.fit_transform(X)\n\n# Create a separate scatter plot for each digit to show a legend\nplt.figure()\ndigits = np.unique(y)\nfor digit in digits:\n    idx = (y == digit)\n    plt.scatter(\n        X_embedded[idx, 0],\n        X_embedded[idx, 1],\n        label=f\"Digit {digit}\",\n        alpha=0.5\n    )\nplt.title(\"t-SNE on a Subset of MNIST Digits (by class)\")\nplt.xlabel(\"Dimension 1\")\nplt.ylabel(\"Dimension 2\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nIn the visualization:\n\nPoints belonging to the same digit typically cluster together.\nAmbiguous or poorly written digits often end up bridging two clusters.\nSome digits, such as 3 and 5, may be visually similar and can appear partially overlapping in the 2D space.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Unsupervised Learning</span>"
    ]
  },
  {
    "objectID": "unsupervised.html#principal-component-analysis-pca",
    "href": "unsupervised.html#principal-component-analysis-pca",
    "title": "10  Unsupervised Learning",
    "section": "10.3 Principal Component Analysis (PCA)",
    "text": "10.3 Principal Component Analysis (PCA)\nThe following section is written by Mezmur Edo, a PhD student in the physics department. This section will focus on the motivation, intuition and theory behind PCA. It will also demonstrate the importance of scaling for proper implementation of PCA.\n\n10.3.1 Motivation\nSome of the motivations behind PCA are:\n\nComputation Efficiency\nFeature Extraction\nVisualization\nCurse of dimensionality\n\n\n10.3.1.1 Curse of Dimensionality\nThe Euclidean distance between data points, which we represent as vectors, shrinks with the number of dimensions. To demonstrate this, let’s generate 10,000 vectors of n dimensions each, where n ranges from 2 to 50, with integer entries ranging from -100 to 100. By selecting a random vector, Q, of the same dimension, we can calculate the Euclidean distance of Q to each of these 10,000 vectors. The plot below shows the logarithm, to the base 10, of difference between the maximum and minimum distances divided by the minimum distance as a function of the number of dimensions.\n\n#import libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler, scale, normalize\nimport os\nimport math\nfrom matplotlib.ticker import AutoMinorLocator\n\n#define a list to store delta values\n#delta is the logarithm, to the base 10, of difference between\n#the maximum and minimum Euclidean distances divided \n#by the minimum distance\ndeltas = []\n\n#loop through dimensions from 2 to 49\nfor N in range(2, 50):\n  #generate 10,000 random N-dimensional vectors, P, and\n  #a single random N-dimensional vector, Q\n  P = [np.random.randint(-100, 100, N) for _ in range(10000)]\n  Q = np.random.randint(-100, 100, N)\n  \n  #calculate the Euclidean distances between each point in P and Q\n  diffs = [np.linalg.norm(p - Q) for p in P]\n  \n  #find the maximum and minimum Euclidean distances\n  mxd = max(diffs)\n  mnd = min(diffs)\n  \n  #calculate delta\n  delta = math.log10(mxd - mnd) / mnd\n  deltas.append(delta)\n\n#plot delta versus N, the number of dimensions\nplt.plot(range(2, 50), deltas)\nplt.xlabel('Number of dimension', loc='right', fontsize=10)\nplt.ylabel('Euclidean Distance', loc='top', fontsize=10)\nax = plt.gca()\n\n#add minor locators to the axes\nax.xaxis.set_minor_locator(AutoMinorLocator())\nax.yaxis.set_minor_locator(AutoMinorLocator())\n\nplt.show()\n\n/var/folders/cq/5ysgnwfn7c3g0h46xyzvpj800000gn/T/ipykernel_88765/2366423172.py:31: RuntimeWarning:\n\ndivide by zero encountered in scalar divide\n\n\n\n\n\n\n\n\n\n\n\n\n\n10.3.2 Intuition\nWe aim to find orthogonal directions of maximum variance in data. Directions with sufficiently low variance in the data can be removed.\n\nrng = np.random.RandomState(0)\nn_samples = 200\n\n#generate a 2D dataset with 200 entries from \n#a multivariate normal distribution\n#with covariances [[3, 3], [3, 4]]\n#and mean [0, 0]\nX = rng.multivariate_normal(mean=[0,0], \\\ncov=[[3, 3], [3, 4]], size=n_samples)\n\n#perform PCA on the generated data to find \n#the two principal components\npca = PCA(n_components=2).fit(X)\n\n#plot the generated data wih label 'Data'\nplt.scatter(X[:,0], X[:,1], label = 'Data')\n\n#plot the first principal component scaled by \n#its explained variance\n#set color, linewidth and label\nfirst_principal_cpt_explained_var = pca.explained_variance_[0]\nfirst_principal_cpt = [[0, pca.components_[0][0]*first_principal_cpt_explained_var] \\\n, [0, pca.components_[0][1]*first_principal_cpt_explained_var]]\n\nplt.plot(first_principal_cpt[0], first_principal_cpt[1] \\\n, color='green', linewidth=5 \\\n, label = r'First Principal Component ($p_1$)')\n\n#plot the second principal component scaled by \n#its explained variance\n#set color, linewidth and label\nsecond_principal_cpt_explained_var = pca.explained_variance_[1]\nsecond_principal_cpt = [[0, pca.components_[1][0]*second_principal_cpt_explained_var] \\\n, [0, pca.components_[1][1]*second_principal_cpt_explained_var]]\n\nplt.plot(second_principal_cpt[0],  second_principal_cpt[1] \\\n, color='red', linewidth=5 \\\n, label = r'Second Principal Component ($p_2$)')\n\nplt.title(\"\")\nplt.xlabel(\"First Feature\", loc = 'right', fontsize = 10)\nplt.ylabel(\"Second Feature\", loc = 'top', fontsize = 10)\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nWe can then project the data onto the first principal component direction, \\(p_1\\).\n\n\n10.3.3 Theory\nLet \\(x\\) be a data point with features \\(f_1\\), \\(f_2\\), \\(f_3\\), …, \\(f_n\\),\n\\[x = \\begin{pmatrix}\nf_1\\\\\nf_2\\\\\nf_3\\\\\n.\\\\\n.\\\\\n.\\\\\nf_n\n\\end{pmatrix}.\n\\]\nThe projection of x onto p is then,\n\\[x^{T} \\frac{p}{||p||}.\\]\nHence, the projection of all data points onto the principal component direction, p, can be written as,\n\\[\\begin{pmatrix}\nx_1^{T} \\frac{p}{||p||}\\\\\nx_2^{T} \\frac{p}{||p||}\\\\\nx_3^{T} \\frac{p}{||p||}\\\\\n.\\\\\n.\\\\\n.\\\\\nx_m^{T} \\frac{p}{||p||}\n\\end{pmatrix}\n= X\\frac{p}{||p||},\\]\nwhere:\n\nX is the design matrix consisting m datapoints.\n\n\n10.3.3.1 The Optimization Problem\nLet \\(\\bar{x}\\) be the sample mean vector such that,\n\\[\\bar{x} = \\frac{1}{m}\\sum_{i=1}^{m}x^{(i)}.\\]\nThe sample covariance matrix is then given by,\n\\[S = \\frac{1}{m} X^TX - \\bar{x}\\bar{x}^T,\\]\nwhere:\n\n\\(S_{ij}\\) is the covarance of feature i and feature j.\n\nFor a sample mean of the projected data, \\(\\bar{a}\\),\n\\[\\bar{a} = \\frac{1}{m}\\sum_{i=1}^{m}x^{(i)T}p = \\bar{x}^Tp,\\]\nthe sample variance of the projected data can be written as,\n\\[\\sigma^{2}= \\frac{1}{m}\\sum_{i=1}^{m}(x^{(i)T}p)^2 - \\bar{a}^{2} = p^{T}Sp.\\]\nThen, our optimization problem simplifies to maximizing the sample variance,\n\\[\\max_p \\space p^{T}Sp \\space s.t. ||p||=1,\\]\nwhich has the following solution,\n\\[Sp = \\lambda p.\\]\n\n\n10.3.3.2 Scikit-learn Implementation\nComputation can be done using the single value decomposition of X,\n\\[X = U \\Sigma V^T.\\]\nIf the data is mean-centered (the default option in scikit-learn), the sample covariance matrix is given by,\n\\[S = \\frac{1}{m} X^TX = \\frac{1}{m} V\\Sigma U^T U \\Sigma V^T = V\\frac{1}{m}\\Sigma^2V^T,\\]\nwhich is the eigenvalue decomposition of S, with its eigenvectors as the columns of \\(V\\) and the corresponding eigenvalues as diagonal entries of \\(\\frac{1}{m}\\Sigma^2\\).\nThe variance explained by the j-th principal component, \\(p_j\\), is \\(\\lambda_{j}\\) and the total variance explained is the sum of all the eigenvalues, which is also equal to the trace of S. The total variance explained by the first k principal componentsis then given by,\n\\[\\frac{\\sum_{j=1}^{k} \\lambda_j}{trace(s)}.\\]\n\n\n\n10.3.4 PCA With and Without Scaling\nFor proper implementation of PCA, data must be scaled. To demonstrate this, we generate a dataset with the first 4 features selected from a normal distribution with mean 0 and standard deviation 1. We then append a fifth feature drawn from a uniform distribution with integer entries ranging from 1 to 10. The plot of the projection of the data onto first principal component versus the projection onto the second principal component does not show the expected noise structure unless the data is scaled.\n\nnp.random.seed(42)\n\n#generate a feature of size 10,000 with integer entries \n#ranging from 1 to 10\nfeature = np.random.randint(1, 10, 10000)\nN = 10000\nP = 4\n\n#generate a 4D dataset drawn from a normal distribution of 10,000 entries\n#then append the feature to X, making it a 5D dataset\nX = np.random.normal(size=[N,P])\nX = np.append(X, feature.reshape(10000,1), axis = 1)\n\n#perform PCA with 2 components on the dataset without scaling\npca = PCA(2)\npca_no_scale = pca.fit_transform(X)\n\n#plot the projection of the data onto the first principal\n#component versus the projection onto \n#the second principal component\nplt.scatter(pca_no_scale[:,0], pca_no_scale[:,1])\nplt.title(\"PCA without Scaling\")\nplt.xlabel(\"Principal Component 1\", loc = 'right', fontsize = 10)\nplt.ylabel(\"Principal Component 2\", loc = 'top', fontsize = 10)\nplt.show()\n\n#scale data, mean-center and divide by the standard deviation\nXn = scale(X)\n\n#perform PCA with 2 components on the scaled data\npca = PCA(2)\npca_scale = pca.fit_transform(Xn)\n\n#plot the projection of the data onto the first principal \n#component versus the projection onto\n#the second principal component\nplt.scatter(pca_scale[:,0], pca_scale[:,1])\nplt.title(\"PCA with Scaling\")\nplt.xlabel(\"Principal Component 1\", loc = 'right', fontsize = 10)\nplt.ylabel(\"Principal Component 2\", loc = 'top', fontsize = 10)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10.3.5 Summary\n\nPCA is a dimensionality reduction technique that projects data onto directions which explain the most variance in the data.\nThe principal component directions are the eigenvectors of the sample covariance matrix and the corresponding eigenvalues represent the variances explained.\nFor proper implementation of PCA, data must be mean-centered, scikit-learn default, and scaled.\n\n\n\n10.3.6 Further Readings\n\nPrincipal component analysis: a review and recent developments\nPrincipal Components Analysis",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Unsupervised Learning</span>"
    ]
  },
  {
    "objectID": "exercises.html",
    "href": "exercises.html",
    "title": "11  Exercises",
    "section": "",
    "text": "Quarto and Git setup Quarto and Git are two important tools for data science. Get familiar with them through the following tasks. Please use the templates/hw.qmd template to document, for each step, what you did, the obstacles you encountered, and how you overcame them. Think of this as a user manual for students who are new to this. Use the command line interface.\n\nSet up SSH authentication between your computer and your GitHub account.\nInstall Quarto onto your computer following the instructions of Get Started.\nPick a tool of your choice (e.g., VS Code, Jupyter Notebook, Emacs, etc.), follow the instructions to reproduce the example of line plot on polar axis.\nRender the homework into a pdf file and put the file into a release in your GitHub repo.\n\nWorking on Homework Problems All the requirements on homework styles have reasons. Reviewing these questions help you to understand them.\n\nWhat are the differences between binary and source files?\nWhy do we not want to track binary files in a repo?\nWhy do I require pdf output via release?\nWhy do I not want your files added via ‘upload’?\nWhy do I require line width under 80?\nWhy is it not a good idea to have spaces in file/folder names?\n\nContributing to the Class Notes To contribute to the classnotes, you need to have a working copy of the sources on your computer. Document the following steps in a qmd file in the form of a step-by-step manual, as if you are explaining them to someone who wants to contribute too. Make at least 10 commits for this task, each with an informative message.\n\nCreate a fork of the notes repo into your own GitHub account.\nClone it to an appropriate folder on your computer.\nRender the classnotes on your computer; document the obstacles and solutions.\nMake a new branch (and name it appropriately) to experiment with your changes.\nCheckout your branch and add your wishes to the wish list; commit with an informative message; and push the changes to your GitHub account.\nMake a pull request to class notes repo from your fork at GitHub. Make sure you have clear messages to document the changes.\n\nMonty Hall Consider a generalized Monty Hall experiment. Suppose that the game start with \\(n\\) doors; after you pick one, the host opens \\(m \\le n - 2\\) doors, that show no award. Include sufficient text around the code chunks to explain them.\n\nWrite a function to simulate the experiment once. The function takes two arguments ndoors and nempty, which represent the number of doors and the number of empty doors showed by the host, respectively, It returns the result of two strategies, switch and no-switch, from playing this game.\nPlay this game with 3 doors and 1 empty a few times.\nPlay this game with 10 doors and 8 empty a few times.\nWrite a function to demonstrate the Monty Hall problem through simulation. The function takes three arguments ndoors, nempty, and ntrials, where ntrial is the number of trials in a simulation. The function should return the proportion of wins for both the switch and no-switch strategy.\nApply your function with 3 doors (1 empty) and 10 doors (8 empty), both with 1000 trials. Summarize your results.\n\nApproximating \\(\\pi\\) Write a function to do a Monte Carlo approximation of \\(\\pi\\). The function takes a Monte Carlo sample size n as input, and returns a point estimate of \\(\\pi\\) and a 95% confidence interval. Apply your function with sample size 1000, 2000, 4000, and 8000. Repeat the experiment 1000 times for each sample size and check the empirical probability that the confidence intervals cover the true value of \\(\\pi\\). Comment on the results.\nGoogle Billboard Ad Find the first 10-digit prime number occurring in consecutive digits of \\(e\\). This was a Google recruiting ad.\nGame 24 The math game 24 is one of the addictive games among number lovers. With four randomly selected cards from a deck of poker cards, use all four values and elementary arithmetic operations (\\(+-\\times /\\)) to come up with 24. Let \\(\\square\\) be one of the four numbers. Let \\(\\bigcirc\\) represent one of the four operators. For example, \\[\\begin{equation*}\n(\\square \\bigcirc \\square) \\bigcirc (\\square \\bigcirc \\square)\n\\end{equation*}\\] is one way to group the the operations.\n\nList all the possible ways to group the four numbers.\nHow many possible ways are there to check for a solution?\nWrite a function to solve the problem in a brutal force way. The inputs of the function are four numbers. The function returns a list of solutions. Some of the solutions will be equivalent, but let us not worry about that for now.\n\nNYC Crash Data Cleaning The NYC motor vehicle collisions data with documentation is available from NYC Open Data. The raw data needs some cleaning.\n\nUse the filter from the website to download the crash data of the week of June 30, 2024 in CSV format; save it under a directory data with an informative name (e.g., nyccrashes_2024w0630_by20240916.csv); read the data into a Panda data frame with careful handling of the date time variables.\nClean up the variable names. Use lower cases and replace spaces with underscores.\nCheck the crash date and time to see if they really match the filter we intented. Remove the extra rows if needed.\nGet the basic summaries of each variables: missing percentage; descriptive statistics for continuous variables; frequency tables for discrete variables.\nAre their invalid longitude and latitude in the data? If so, replace them with NA.\nAre there zip_code values that are not legit NYC zip codes? If so, replace them with NA.\nAre there missing in zip_code and borough? Do they always co-occur?\nAre there cases where zip_code and borough are missing but the geo codes are not missing? If so, fill in zip_code and borough using the geo codes.\nIs it redundant to keep both location and the longitude/latitude at the NYC Open Data server?\nCheck the frequency of crash_time by hour. Is there a matter of bad luck at exactly midnight? How would you interpret this?\nAre the number of persons killed/injured the summation of the numbers of pedestrians, cyclist, and motorists killed/injured? If so, is it redundant to keep these two columns at the NYC Open Data server?\nPrint the whole frequency table of contributing_factor_vehicle_1. Convert lower cases to uppercases and check the frequencies again.\nProvided an opportunity to meet the data provider, what suggestions would you make based on your data exploration experience?\n\nNYC Crash Data Exploration Except for the first question, use the cleaned crash data in feather format.\n\nConstruct a contigency table for missing in geocode (latitude and longitude) by borough. Is the missing pattern the same across boroughs? Formulate a hypothesis and test it.\nConstruct a hour variable with integer values from 0 to 23. Plot the histogram of the number of crashes by hour. Plot it by borough.\nOverlay the locations of the crashes on a map of NYC. The map could be a static map or a Google map.\nCreate a new variable severe which is one if the number of persons injured or deaths is 1 or more; and zero otherwise. Construct a cross table for severe versus borough. Is the severity of the crashes the same across boroughs? Test the null hypothesis that the two variables are not associated with an appropriate test.\nMerge the crash data with the Census zip code database which contains zip-code level demographic or socioeconomic variables.\nFit a logistic model with severe as the outcome variable and covariates that are available in the data or can be engineered from the data. For example, zip code level covariates obtained from merging with the zip code database; crash hour; number of vehicles involved.\n\nNYC Crash severity modeling Using the cleaned NYC crash data, merged with zipcode level information, predict severe of a crash.\n\nSet random seed to 1234. Randomly select 20% of the crashes as testing data and leave the rest 80% as training data.\nFit a logistic model on the training data and validate the performance on the testing data. Explain the confusion matrix result from the testing data. Compute the F1 score.\nFit a logistic model on the training data with \\(L_1\\) regularization. Select the tuning parameter with 5-fold cross-validation in F1 score\nApply the regularized logistic regression to predict the severity of the crashes in the testing data. Compare the performance of the two logistic models in terms of accuracy, precision, recall, F1-score, and AUC.\n\nMidterm project: Street flood in NYC The class presentation at the 2025 NYC Open Data Weeik is scheduled for Monday, March 24, 2:00-3:00 pm.\nThe NYC Open Data of 311 Service Requests contains all service requests from 2010 to the present. This analysis focuses on two sewer-related complaints in 2024: Street Flooding (SF) and Catch Basin (CB). SF complaints serve as a practical indicator of street flooding, while CB complaints provide insights into a key infrastructural factor—when catch basins fail to drain rainwater properly due to blockages or structural issues, water accumulates on the streets. SF complaints are typically filed when residents observe standing water or flooding, whereas CB complaints report clogged basins, defective grates, or other drainage problems. The dataset is available in CSV format as data/nycflood2024.csv. Refer to the online data dictionary for a detailed explanation of variable meanings. Try to tell a story in your report while going through the questions.\n\nData cleaning.\n\nImport the data, rename the columns with our preferred styles.\nSummarize the missing information. Are there variables that are close to completely missing?\nAre there redundant information in the data? Try storing the data using the Arrow format and comment on the efficiency gain.\nAre there invalid NYC zipcode or borough? Can some of the missing values be filled? Fill them if yes.\nAre there date errors? Examples are earlier closed_date than created_date; closed_date and created_date matching to the second; dates exactly at midnight or noon to the second.\nSummarize your suggestions to the data curator in several bullet points.\n\nExploratory analysis.\n\nVisualize the locations of complaints on a NYC map, with different symbols for different descriptors.\nCreate a variable response_time, which is the duration from created_date to closed_date.\nVisualize the comparison of response time by complaint descriptor and borough. The original may not be the best given the long tail or outlers.\nIs there significant difference in response time between SF and CB complaints? Across different boroughs? Does the difference between SF and CB depend on borough? State your hypothesis, justify your test, and summarize your results in plain English.\nCreate a binary variable over3d to indicate that a service request took three days or longer to close.\nDoes over3d depend on the complaint descriptor, borough, or weekday (vs weekend/holiday)? State your hypotheses, justify your test, and summarize your results.\n\nModeling the occurrence of overly long response time.\n\nCreate a data set which contains the outcome variable over3d and variables that might be useful in predicting it. Consider including time-of-day effects (e.g., rush hour vs. late-night), seasonal trends, and neighborhood-level demographics. Zip code level information could be useful too, such as the zip code area and the ACS 2023 variables (data/nyc_zip_areas.feather and data/acs2023.feather).\nRandomly select 20% of the complaints as testing data with seeds 1234. Build a logistic model to predict over3d for the complaints with the training data. If you have tuning parameters, justify how they were selected.\nConstruct the confusion matrix from your prediction with a threshold of 1/2 on both training and testing data. Explain your accuracy, recall, precision, and F1 score to a New Yorker.\nConstruct the ROC curve of your fitted logistic model and obtain the AUROC for both training and testing data. Explain your results to a New Yorker.\nIdentify the most important predictors of over3d. Use model coefficients or feature importance (e.g., odds ratios, standardized coefficients, or SHAP values).\nSummarize your results to a New Yorker who is not data science savvy in several bullet points.\n\nModeling the count of SF complains by zip code.\n\nCreate a data set by aggregate the count of SF and SB complains by day for each zipcode.\nMerge the NYC precipitation (data/rainfall_CP.csv), by day to this data set.\nMerge the NYC zip code level landscape variables (data/nyc_zip_lands.csv) and ACS 2023 variables into the data set.\nFor each day, create two variables representing 1-day lag of the precipitation and the number of CB complaints.\nFilter data from March 1 to November 30, excluding winter months when flooding is less frequent. November 30.\nCompare a Poisson regression with a Negative Binomial regression to account for overdispersion. Which model fits better? Explain the results to a New Yorker.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Exercises</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Agonafir, C., Lakhankar, T., Khanbilvardi, R., Krakauer, N., Radell, D.,\n& Devineni, N. (2022). A machine learning approach to evaluate the\nspatial variability of New York\nCity’s 311 street flooding complaints. Computers,\nEnvironment and Urban Systems, 97, 101854.\n\n\nAgonafir, C., Pabon, A. R., Lakhankar, T., Khanbilvardi, R., &\nDevineni, N. (2022). Understanding New York\nCity street flooding through 311 complaints. Journal of\nHydrology, 605, 127300.\n\n\n(ASA), A. S. A. (2018). Ethical guidelines for statistical\npractice.\n\n\nBansal, R. (2024). SQL using python.\n\n\nBreiman, L., Friedman, J. H., Olshen, R., & Stone, C. J. (1984).\nClassification and regression trees. Wadsworth.\n\n\nCafferky, B. (2019). Master using SQL with python: Lesson 1 - using\nSQL with pandas.\n\n\nChen, T., & Guestrin, C. (2016). XGBoost: A scalable tree boosting\nsystem. Proceedings of the 22nd ACM SIGKDD International Conference\non Knowledge Discovery and Data Mining, 785–794. https://doi.org/10.1145/2939672.2939785\n\n\nComputing Machinery (ACM), A. for. (2018). Code of ethics and\nprofessional conduct.\n\n\nCone, M. (2025). Markdown cheat sheet | markdown guide. https://www.markdownguide.org/cheat-sheet/\n\n\nCongress, U. S. (1990). Americans with disabilities act of 1990\n(ADA).\n\n\nDervieux, C. (2025). Markdown-basics. https://quarto.org/docs/authoring/markdown-basics.html\n\n\nFriedman, J. H. (2001). Greedy function approximation: A gradient\nboosting machine. The Annals of Statistics, 29(5),\n1189–1232.\n\n\nFriedman, J. H. (2002). Stochastic gradient boosting. Computational\nStatistics & Data Analysis, 38(4), 367–378.\n\n\nHastie, T., Tibshirani, R., & Friedman, J. (2009). The elements\nof statistical learning: Data mining, inference, and prediction.\nSpringer.\n\n\nHealth, U. S. D. of, & Services, H. (1996). Health insurance\nportability and accountability act of 1996 (HIPAA).\n\n\nKe, G., Meng, Q., Finley, T., Wang, T., Chen, W., Ma, W., Ye, Q., &\nLiu, T.-Y. (2017). LightGBM: A highly efficient gradient\nboosting decision tree. Advances in Neural Information Processing\nSystems, 3146–3154.\n\n\nMacFarlane, J. (2006). Pandoc user’s guide. https://pandoc.org/MANUAL.html#pandocs-markdown\n\n\nMacFarlane, J. (2019). GitHub flavored markdown spec. https://github.github.com/gfm/\n\n\nProkhorenkova, L., Gusev, G., Vorobev, A., Dorogush, A. V., & Gulin,\nA. (2018). CatBoost: Unbiased boosting with categorical features.\nAdvances in Neural Information Processing Systems, 6638–6648.\n\n\nProtection of Human Subjects of Biomedical, N. C. for the, &\nResearch, B. (1979). The belmont report: Ethical principles and\nguidelines for the protection of human subjects of research.\n\n\nPrzybyla, M. (2024). How to use SQL in python.\n\n\nTeam, F. D. S. D. (2019). Federal data strategy 2020 action\nplan.\n\n\nTibshirani, R. (1996). Regression shrinkage and selection via the\nLASSO. Journal of the Royal Statistical Society: Series\nB (Methodological), 58(1), 267–288.\n\n\nVanderPlas, J. (2016). Python data science handbook:\nEssential tools for working with data. O’Reilly Media,\nInc.\n\n\nW3Schools. (2025). Python MySQL.",
    "crumbs": [
      "References"
    ]
  }
]