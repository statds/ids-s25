[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Data Science",
    "section": "",
    "text": "Preliminaries\nThe notes were developed with Quarto; for details about Quarto, visit https://quarto.org/docs/books.\nThis book free and is licensed under a Creative Commons Attribution-NonCommercial-NoDerivs 3.0 United States License.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#sources-at-github",
    "href": "index.html#sources-at-github",
    "title": "Introduction to Data Science",
    "section": "Sources at GitHub",
    "text": "Sources at GitHub\nThese lecture notes for STAT 3255/5255 in Spring 2025 represent a collaborative effort between Professor Jun Yan and the students enrolled in the course. This cooperative approach to education was facilitated through the use of GitHub, a platform that encourages collaborative coding and content development. To view these contributions and the lecture notes in their entirety, please visit our GitHub repository at https://github.com/statds/ids-s25.\nStudents contributed to the lecture notes by submitting pull requests to our GitHub repository. This method not only enriched the course material but also provided students with practical experience in collaborative software development and version control.\nFor those interested, class notes from Fall 2024, Spring 2024, Spring 2023, and Spring 2022 are also publicly accessible. These archives offer insights into the evolution of the course content and the different perspectives brought by successive student cohorts.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#compiling-the-classnotes",
    "href": "index.html#compiling-the-classnotes",
    "title": "Introduction to Data Science",
    "section": "Compiling the Classnotes",
    "text": "Compiling the Classnotes\nTo reproduce the classnotes output on your own computer, here are the necessary steps. See Section 3.2 Compiling the Classnotes for details.\n\nClone the classnotes repository to an appropriate location on your computer; see Chapter 2  Project Management for using Git.\nSet up a Python virtual environment in the root folder of the source; see Section 4.7 Virtual Environment.\nActivate your virtual environment.\nInstall all the packages specified in requirements.txt in your virtual environment:\n\npip install -r requirements.txt\n\nFor some chapters that need to interact with certain sites that require account information. For example, for Google map services, you need to save your API key in a file named api_key.txt in the root folder of the source.\nRender the book with quarto render from the root folder on a terminal; the rendered book will be stored under _book.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#midterm-project",
    "href": "index.html#midterm-project",
    "title": "Introduction to Data Science",
    "section": "Midterm Project",
    "text": "Midterm Project\nReproduce NYC street flood research (Agonafir, Lakhankar, et al., 2022; Agonafir, Pabon, et al., 2022).\nFour students will be selected to present their work in a workshop at the 2025 NYC Open Data Week. You are welcome to invite your family and friends to join the the workshop.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#final-project",
    "href": "index.html#final-project",
    "title": "Introduction to Data Science",
    "section": "Final Project",
    "text": "Final Project\nStudents are encouraged to start designing their final projects from the beginning of the semester. There are many open data that can be used. Here is a list of data challenges that you may find useful:\n\nASA Data Challenge Expo: big data in 2025\nKaggle.\nDrivenData.\n15 Data Science Hackathons to Test Your Skills in 2025\nIf you work on sports analytics, you are welcome to submit a poster to Connecticut Sports Analytics Symposium (CSAS) 2025.\nA good resource for sports analytics is ScoreNetwork.\nPaleobiology Database.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#adapting-to-rapid-skill-acquisition",
    "href": "index.html#adapting-to-rapid-skill-acquisition",
    "title": "Introduction to Data Science",
    "section": "Adapting to Rapid Skill Acquisition",
    "text": "Adapting to Rapid Skill Acquisition\nIn this course, students are expected to rapidly acquire new skills, a critical aspect of data science. To emphasize this, consider this insightful quote from VanderPlas (2016):\n\nWhen a technologically-minded person is asked to help a friend, family member, or colleague with a computer problem, most of the time it’s less a matter of knowing the answer as much as knowing how to quickly find an unknown answer. In data science it’s the same: searchable web resources such as online documentation, mailing-list threads, and StackOverflow answers contain a wealth of information, even (especially?) if it is a topic you’ve found yourself searching before. Being an effective practitioner of data science is less about memorizing the tool or command you should use for every possible situation, and more about learning to effectively find the information you don’t know, whether through a web search engine or another means.\n\nThis quote captures the essence of what we aim to develop in our students: the ability to swiftly navigate and utilize the vast resources available to solve complex problems in data science. Examples tasks are: install needed software (or even hardware); search and find solutions to encountered problems.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#wishlist",
    "href": "index.html#wishlist",
    "title": "Introduction to Data Science",
    "section": "Wishlist",
    "text": "Wishlist\nThis is a wish list from all members of the class (alphabetical order, last name first, comma, then first name). Here is an example.\n\nYan, Jun\n\nMake practical data science tools accessible to undergraduates.\nPass real-world data science project experience to students.\nCo-develop a Quarto book in collaboration with the students.\nTrain students to participate in real data science competitions.\n\n\nAdd yours through a pull request; note the syntax of nested list in Markdown.\n\nStudents in 3255\n\nAckerman, John\n\nGet comfortable with command line interface\nHands-on experience with AI\nLearn practical tools & tricks for professional data scientist\n\nAlsadadi, Ammar Shaker\n\nLearn about the applications of Data Science in Finance\nLearn more about time series and random walk\n\nChen, Yifei\n\nLearn more advanced python programming skills.\nLearn to use github for future projects\nGet a good grade in this class.\n\nEl Zein, Amer Hani\n\nTo gain a deeper undestanding of data preparation.\nTo develop intution on what the best tool for a given project is.\n\nFebles, Xavier Milan\n\nFurther develop skills with git\nLearn more about specific tools used for data science\nBecome more comfortable with sql\n\nHorn, Alyssa Noelle\n\nBe confident in using Git and Github\nLearn how to collaborate with others on projects through Github\n\nJun, Joann\n\nBecome proficient in using GitHub\nLearn more about the applications of data science\n\nKline, Daniel Esteban\nLagutin, Vladislav\n\nLearn how to do data science projects in python and interact with them using git\nLearn how to do good visualizations of the data; explore appropriate libraries\n\nLang, Lang\n\nBecome more proficient with python\nLearn about the applications of Data Science\nLearn how to make collaborative project by using GitHub\nHave a good grade in this course\n\nLi, Shiyi\n\nLearn to visualize the plots and results using the ggplot package.\nLearn to use the common functions of the SciPy, scikit-learn, and statsmodels libraries in Python\nLearn how to query, extract, and manipulate structured and unstructured data in a large database.\nLearn the basics of artificial neural networks, CNNs for image data, NLP techniques.\nLearn some of the data analysis models that will be commonly used in the workplace.\nLearn some common applications of optimization techniques in data analysis.\nPass this course with an A grade.\n\nLin, Selena\n\nGet a good grade in this class.\nLearn and get familier with using GitHub.\nHands on experience with the data science skills learned in this class.\n\nLong, Ethan Kenneth\n\nBecome more comfortable using Git commands and CLI\nLearn more about the data science field\nUnderstand proper coding grammar\nDevelop good learning habits\n\nNasejje, Ruth Nicole\n\nDevelop an organized coding style in python, quarto, & git\nLearn various packages in python related to data science\nDeepen knowledge in statistical modeling and data analysis\n\nPfeifer, Nicholas Theodore\n\nLearn about data science techniques in python\nLearn and thoroughly practice using git and github\nGet more comfortable with decision trees and random forests\n\nReed, Kyle Daniel\n\nGain full confidence using Git/GitHub and corresponding applications.\nUnderstand the workflow in professional data science projects.\nBuild on existing python skills.\n\nRoy, Luke William\n\nHave fun\nDevelop skills in financial data analysis using python and relevant libraries like pandas and numpy.\nLearn advanced data visualization techniques with a focus on the grammar of graphics.\nGet an introduction to machine learning via scikit-learn, and explore applications in financial analysis and forensic accounting.\n\nSchittina, Thomas\n\nBecome more comfortable using git and GitHub\nBecome more familiar with popular data science packages in Python\n\nSymula, Sebastian\n\nLearn SQL\nBecome better at working through each step in the data science pipeline to make better, cleaner looking projects\n\nTamhane, Shubhan\n\nLearn intersection between SQL and Python for a data science project\nLearn machine learning algorithms like random forest and clustering\n\nTomaino, Mario Anthony\nXu, Peiwen\n\nLearn some data analysis techniques\nLearn how to use git and other essential tools for data science\n\n\n\n\nStudents in 5255\n\nEdo, Mezmur Wossenu\n\nI hope to become adept working with github.\nI hope to work on real-World data science projects.\nI hope to learn about the different machine learning techniques.\n\nMundiwala, Mohammad Moiz\n\nBecome more familiar with collaboration process of programming so that I can be more orderly while working with others.\nI hope to become more efficient processing data that is messy, unstructured, or unlabeled.\nPresent engaging, intuitive, and interactive figures and animations for complex math and stat concepts.\n\nVellore, Ajeeth Krishna\n\nUnderstand the utility provided by GitHub and practice using its tools\nLearn how to participate in a large-scale development project like how they are done in industry\nLearn how to code properly and professionally instead of using “backyard” computer science techniques and formatting\nUnderstand principles of coding documentation and readability while practicing their application\n\nZhang, Gaofei\n\nGain confidence in using Git and GitHub for version control and collaboration.\nDevelop a structured approach to data cleaning and preprocessing for complex datasets.\nEnhance skills in statistical modeling and machine learning techniques relevant to public health research.\nImprove efficiency in working with large-scale data using Python and SQL.\n\nKravette, Noah\n\nBecome better at program collaboration.\nBecome adept with git and github.\nBe able to quickly and efficently process and analyze any data.\nGain better skills at data prep, organization, and visulization.\nLearn new helpful statistical tools for data.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#course-logistics",
    "href": "index.html#course-logistics",
    "title": "Introduction to Data Science",
    "section": "Course Logistics",
    "text": "Course Logistics\n\nPresentation Orders\nThe topic presentation order is set up in class.\n\nwith open('rosters/3255.txt', 'r') as file:\n    ug = [line.strip() for line in file]\nwith open('rosters/5255.txt', 'r') as file:\n    gr = [line.strip() for line in file]\npresenters = ug + gr\n\nimport random\n## seed jointly set by the class\nrandom.seed(6895 + 283 + 3184 + 3078 + 5901 + 36)\nrandom.sample(presenters, len(presenters))\n## random.shuffle(presenters) # This would shuffle the list in place\n\n['Li,Shiyi',\n 'Jun,Joann',\n 'Alsadadi,Ammar Shaker',\n 'Lang,Lang',\n 'Ackerman,John',\n 'Horn,Alyssa Noelle',\n 'Xu,Peiwen',\n 'Schittina,Thomas',\n 'Kline,Daniel Esteban',\n 'Edo,Mezmur Wossenu',\n 'Roy,Luke William',\n 'Febles,Xavier Milan',\n 'Tamhane,Shubhan',\n 'Nasejje,Ruth Nicole',\n 'Lagutin,Vladislav',\n 'Zhang,Gaofei',\n 'Long,Ethan Kenneth',\n 'El Zein,Amer Hani',\n 'Kravette,Noah',\n 'Symula,Sebastian',\n 'Tomaino,Mario Anthony',\n 'Reed,Kyle Daniel',\n 'Chen,Yifei',\n 'Mundiwala,Mohammad Moiz',\n 'Lin,Selena',\n 'Pfeifer,Nicholas Theodore',\n 'Vellore,Ajeeth Krishna']\n\n\nSwitching slots is allowed as long as you find someone who is willing to switch with you. In this case, make a pull request to switch the order and let me know.\nYou are welcome to choose a topic that you are interested the most, subject to some order restrictions. For example, decision tree should be presented before random forest or extreme gradient boosting. This justifies certain requests for switching slots.\n\n\nPresentation Task Board\nTalk to the professor about your topics at least one week prior to your scheduled presentation. Here are some example tasks:\n\nMaking presentations with Quarto\nMarkdown jumpstart\nEffective data science communication\nImport/Export data\nData manipulation with Pandas\nAccessing US census data\nArrow as a cross-platform data format\nStatistical analysis for proportions and rates\nDatabase operation with Structured query language (SQL)\nGrammer of graphics\nHandling spatial data\nSpatial data with GeoPandas\nVisualize spatial data in a Google map\nAnimation\nSupport vector machine\nRandom forest\nGradient boosting machine\nNaive Bayes\nNeural networks basics\nMLP/ANN/CNN/RNN/LSTM\nUniform manifold approximation and projection\nAutomatic differentiation\nDeep learning\nTensorFlow\nAutoencoders\nK-means clustering\nPrincipal component analysis\nReinforcement learning\nDeveloping a Python package\nWeb scraping\nPersonal webpage on GitHub\n\n\n\nTopic Presentation Schedule\nThe topic presentation is 20 points. It includes:\n\nTopic selection consultation on week in advance (4 points).\nDelivering the presentation in class (10 points).\nContribute to the class notes within two weeks following the presentation (6 points).\n\nTips on topic contribution:\n\nNo plagiarism (see instructions on Contributing to Class Notes).\nAvoid external graphics.\nUse simulated data.\nUse data from homework assignments.\nCite article/book references (learn how from our sources).\nInclude a subsection of Further Readings.\nTest on your own computer before making a pull request.\nSend me your presentation two days in advance for feedbacks.\n\nPlease use the following table to sign up.\n\n\n\n\n\n\n\n\nDate\nPresenter\nTopic\n\n\n\n\n02/10\nLi, Shiyi\nA Primer of Markdown\n\n\n02/12\nJun, Joann\nMaking Presentations with Quarto\n\n\n02/17\nRoy, Luke William\nGrammar of Graphics with Plotnine\n\n\n02/19\nLang, Lang\nData manipulation with Pandas\n\n\n02/24\nAckerman, John\nPerforming Statistical Tests (SciPy)\n\n\n02/26\nHorn, Alyssa Noelle\nDatabase operation with Structured query language (SQL)\n\n\n03/03\nEl Zein, Amer Hani\nEffective Communication in Data Science\n\n\n03/05\nSchittina, Thomas\nSpatial Data With Geopandas & Google Maps\n\n\n03/10\nKline, Daniel Esteban\n\n\n\n03/12\nEdo, Mezmur Wossenu\nPrincipal Component Analysis (PCA)\n\n\n03/26\nAlsadadi, Ammar Shaker\nSentiment Analysis with Python\n\n\n03/26\nFables, Xavier Milan\nVariable Importance Metrics in Supervised Learning\n\n\n03/31\nTamhane, Shubhan\nSupport Vector Machine\n\n\n03/31\nNasejje, Ruth Nicole\nPersonal webpage on GitHub\n\n\n04/02\nLagutin, Vladislav\nGoogle Maps visualizations using Folium library\n\n\n04/02\nZhang, Gaofei\nRandom forest\n\n\n04/07\nLong, Ethan Kenneth\nSynthetic Minority Oversampling Technique (SMOTE)\n\n\n04/07\nXu, Peiwen\nDeveloping a Python package\n\n\n04/09\nKravette, Noah\nWorking with NetCDF Data\n\n\n04/09\nSymula, Sebastian\nImputation Methods for Missing Data\n\n\n04/09\nTomaino, Mario Anthony\nK-Prototypes Clustering\n\n\n04/14\nReed, Kyle Daniel\nAutoencoders\n\n\n04/14\nChen, Yifei\nExplaining XGBoost Predictions with SHAP\n\n\n04/14\nMundiwala, Mohammad Moiz\nMath animations with manim\n\n\n04/16\nLin, Selena\nNaive Bayes\n\n\n04/16\nPfeifer, Nicholas Theodore\nChoosing the number of clusters in clustering\n\n\n04/16\nVellore, Ajeeth Krishna\nNeural Network Basics\n\n\n\n\n\nFinal Project Presentation Schedule\nWe use the same order as the topic presentation for undergraduate final presentation. An introduction on how to use Quarto to prepare presentation slides is availabe under the templates directory in the classnotes source tree, thank to Zachary Blanchard, which can be used as a template to start with.\n\n\n\n\n\n\n\nDate\nPresenter\n\n\n\n\n04/21\nShiyi Li; Joann Jun; Ammar Alsadadi; Lang Lang\n\n\n04/23\nJohn Ackerman; Alyssa Horn; Peiwen Xu; Thomas Schittina\n\n\n04/28\nDaniel Kline; Luke Roy; Xavier Fables; Shubhan Tamhane\n\n\n04/30\nRuth Nasejje; Vladislav Lagutin; Ethan Long; Amer El Zein\n\n\n05/05 (10:30-12:30)\nSebastian Symula; Mario Tomiano; Kyle Reed; Yifei Chen; Selena Lin; Nick Pfeifer\n\n\n\n\n\nContributing to the Class Notes\nContribution to the class notes is through a `pull request’.\n\nStart a new branch and switch to the new branch.\nOn the new branch, add a qmd file for your presentation\nIf using Python, create and activate a virtual environment with requirements.txt\nEdit _quarto.yml add a line for your qmd file to include it in the notes.\nWork on your qmd file, test with quarto render.\nWhen satisfied, commit and make a pull request with your quarto files and an updated requirements.txt.\n\nI have added a template file mysection.qmd and a new line to _quarto.yml as an example.\nFor more detailed style guidance, please see my notes on statistical writing.\nPlagiarism is to be prevented. Remember that these class notes are publicly available online with your names attached. Here are some resources on how to avoid plagiarism. In particular, in our course, one convenient way to avoid plagiarism is to use our own data (e.g., NYC Open Data). Combined with your own explanation of the code chunks, it would be hard to plagiarize.\n\n\nHomework Logistics\n\nWorkflow of Submitting Homework Assisngment\n\nClick the GitHub classroom assignment link in HuskCT announcement.\nAccept the assignment and follow the instructions to an empty repository.\nMake a clone of the repo at an appropriate folder on your own computer with git clone.\nGo to this folder, add your qmd source, work on it, and group your changes to different commits.\nPush your work to your GitHub repo with git push.\nCreate a new release and put the generated pdf file in it for ease of grading.\n\n\n\nRequirements\n\nUse the repo from Git Classroom to submit your work. See Chapter 2  Project Management.\n\nKeep the repo clean (no tracking generated files).\n\nNever “Upload” your files; use the git command lines.\nMake commit message informative (think about the readers).\n\nMake at least 10 commits and form a style of frequent small commits.\n\nTrack quarto sources only in your repo. See Chapter 3  Reproducible Data Science.\nFor the convenience of grading, add your standalone html or pdf output to a release in your repo.\nFor standalone pdf output, you will need to have LaTeX installed.\n\n\n\n\nQuizzes about Syllabus\n\nDo I accept late homework?\nCould you list a few examples of email etiquette?\nHow would you lose style points?\nWould you use CLI and GUI?\nHow many students will present at 2025 NYC ODW and when will the presentations be?\nWhat’s the first date on which you have to complete something about your final project?\nCan you use AI for any task in this course?\nAnybody needs a reference letter? How could you help me to help you?",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#practical-tips",
    "href": "index.html#practical-tips",
    "title": "Introduction to Data Science",
    "section": "Practical Tips",
    "text": "Practical Tips\n\nData analysis\n\nUse an IDE so you can play with the data interactively\nCollect codes that have tested out into a script for batch processing\nDuring data cleaning, keep in mind how each variable will be used later\nNo keeping large data files in a repo; assume a reasonable location with your collaborators\n\n\n\nPresentation\n\nDon’t forget to introduce yourself if there is no moderator.\nHighlight your research questions and results, not code.\nGive an outline, carry it out, and summarize.\nUse your own examples to reduce the risk of plagiarism.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#my-presentation-topic-template",
    "href": "index.html#my-presentation-topic-template",
    "title": "Introduction to Data Science",
    "section": "My Presentation Topic (Template)",
    "text": "My Presentation Topic (Template)\nThis section was prepared by John Smith.\nUse Markdown syntax. If not clear on what to do, learn from the class notes sources.\n\nPay attention to the sectioning levels.\nCite references with their bib key.\nIn examples, maximize usage of data set that the class is familiar with.\nCould use datasets in Python packages or downloadable on the fly.\nTest your section by quarto render &lt;filename.qmd&gt;.\n\n\nIntroduction\nHere is an overview.\n\n\nSub Topic 1\nPut materials on topic 1 here\nPython examples can be put into python code chunks:\n\n# import pandas as pd\n\n# do something\n\n\n\nSub Topic 2\nPut materials on topic 2 here.\n\n\nSub Topic 3\nPut matreials on topic 3 here.\n\n\nConclusion\nPut sumaries here.\n\n\nFurther Readings\nPut links to further materials.\n\n\n\n\nAgonafir, C., Lakhankar, T., Khanbilvardi, R., Krakauer, N., Radell, D., & Devineni, N. (2022). A machine learning approach to evaluate the spatial variability of New York City’s 311 street flooding complaints. Computers, Environment and Urban Systems, 97, 101854.\n\n\nAgonafir, C., Pabon, A. R., Lakhankar, T., Khanbilvardi, R., & Devineni, N. (2022). Understanding New York City street flooding through 311 complaints. Journal of Hydrology, 605, 127300.\n\n\nVanderPlas, J. (2016). Python data science handbook: Essential tools for working with data. O’Reilly Media, Inc.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 What Is Data Science?\nData science is a multifaceted field, often conceptualized as resting on three fundamental pillars: mathematics/statistics, computer science, and domain-specific knowledge. This framework helps to underscore the interdisciplinary nature of data science, where expertise in one area is often complemented by foundational knowledge in the others.\nA compelling definition was offered by Prof. Bin Yu in her 2014 Presidential Address to the Institute of Mathematical Statistics. She defines \\[\\begin{equation*}\n\\mbox{Data Science} =\n\\mbox{S}\\mbox{D}\\mbox{C}^3,\n\\end{equation*}\\] where\nComputing underscores the need for proficiency in programming and algorithmic thinking, collaboration/teamwork reflects the inherently collaborative nature of data science projects, often requiring teams with diverse skill sets, and communication to outsiders emphasizes the importance of translating complex data insights into understandable and actionable information for non-experts.\nThis definition neatly captures the essence of data science, emphasizing a balance between technical skills, teamwork, and the ability to communicate effectively.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#what-is-data-science",
    "href": "intro.html#what-is-data-science",
    "title": "1  Introduction",
    "section": "",
    "text": "‘S’ represents Statistics, signifying the crucial role of statistical methods in understanding and interpreting data;\n‘D’ stands for domain or science knowledge, indicating the importance of specialized expertise in a particular field of study;\nthe three ’C’s denotes computing, collaboration/teamwork, and communication to outsiders.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#expectations-from-this-course",
    "href": "intro.html#expectations-from-this-course",
    "title": "1  Introduction",
    "section": "1.2 Expectations from This Course",
    "text": "1.2 Expectations from This Course\nIn this course, students will be expected to achieve the following outcomes:\n\nProficiency in Project Management with Git: Develop a solid understanding of Git for efficient and effective project management. This involves mastering version control, branching, and collaboration through this powerful tool.\nProficiency in Project Reporting with Quarto: Gain expertise in using Quarto for professional-grade project reporting. This encompasses creating comprehensive and visually appealing reports that effectively communicate your findings.\nHands-On Experience with Real-World Data Science Projects: Engage in practical data science projects that reflect real-world scenarios. This hands-on approach is designed to provide you with direct experience in tackling actual data science challenges.\nCompetency in Using Python and Its Extensions for Data Science: Build strong skills in Python, focusing on its extensions relevant to data science. This includes libraries like Pandas, NumPy, and Matplotlib, among others, which are critical for data analysis and visualization.\nFull Grasp of the Meaning of Results from Data Science Algorithms: Learn to not only apply data science algorithms but also to deeply understand the implications and meanings of their results. This is crucial for making informed decisions based on these outcomes.\nBasic Understanding of the Principles of Data Science Methods: Acquire a foundational knowledge of the underlying principles of various data science methods. This understanding is key to effectively applying these methods in practice.\nCommitment to the Ethics of Data Science: Emphasize the importance of ethical considerations in data science. This includes understanding data privacy, bias in data and algorithms, and the broader social implications of data science work.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#computing-environment",
    "href": "intro.html#computing-environment",
    "title": "1  Introduction",
    "section": "1.3 Computing Environment",
    "text": "1.3 Computing Environment\nAll setups are operating system dependent. As soon as possible, stay away from Windows. Otherwise, good luck (you will need it).\n\n1.3.1 Operating System\nYour computer has an operating system (OS), which is responsible for managing the software packages on your computer. Each operating system has its own package management system. For example:\n\nLinux: Linux distributions have a variety of package managers depending on the distribution. For instance, Ubuntu uses APT (Advanced Package Tool), Fedora uses DNF (Dandified Yum), and Arch Linux uses Pacman. These package managers are integral to the Linux experience, allowing users to install, update, and manage software packages easily from repositories.\nmacOS: macOS uses Homebrew as its primary package manager. Homebrew simplifies the installation of software and tools that aren’t included in the standard macOS installation, using simple commands in the terminal.\nWindows: Windows users often rely on the Microsoft Store for apps and software. For more developer-focused package management, tools like Chocolatey and Windows Package Manager (Winget) are used. Additionally, recent versions of Windows have introduced the Windows Subsystem for Linux (WSL). WSL allows Windows users to run a Linux environment directly on Windows, unifying Windows and Linux applications and tools. This is particularly useful for developers and data scientists who need to run Linux-specific software or scripts. It saves a lot of trouble Windows users used to have before its time.\n\nUnderstanding the package management system of your operating system is crucial for effectively managing and installing software, especially for data science tools and applications.\n\n\n1.3.2 File System\nA file system is a fundamental aspect of a computer’s operating system, responsible for managing how data is stored and retrieved on a storage device, such as a hard drive, SSD, or USB flash drive. Essentially, it provides a way for the OS and users to organize and keep track of files. Different operating systems typically use different file systems. For instance, NTFS and FAT32 are common in Windows, APFS and HFS+ in macOS, and Ext4 in many Linux distributions. Each file system has its own set of rules for controlling the allocation of space on the drive and the naming, storage, and access of files, which impacts performance, security, and compatibility. Understanding file systems is crucial for tasks such as data recovery, disk partitioning, and managing file permissions, making it an important concept for anyone working with computers, especially in data science and IT fields.\nNavigating through folders in the command line, especially in Unix-like environments such as Linux or macOS, and Windows Subsystem for Linux (WSL), is an essential skill for effective file management. The command cd (change directory) is central to this process. To move into a specific directory, you use cd followed by the directory name, like cd Documents. To go up one level in the directory hierarchy, you use cd ... To return to the home directory, simply typing cd or cd ~ will suffice. The ls command lists all files and folders in the current directory, providing a clear view of your options for navigation. Mastering these commands, along with others like pwd (print working directory), which displays your current directory, equips you with the basics of moving around the file system in the command line, an indispensable skill for a wide range of computing tasks in Unix-like systems.\n\n\n1.3.3 Command Line Interface\nOn Linux or MacOS, simply open a terminal.\nOn Windows, several options can be considered.\n\nWindows Subsystem Linux (WSL): https://learn.microsoft.com/en-us/windows/wsl/\nCygwin (with X): https://x.cygwin.com\nGit Bash: https://www.gitkraken.com/blog/what-is-git-bash\n\nTo jump start, here is a tutorial: Ubunto Linux for beginners.\nAt least, you need to know how to handle files and traverse across directories. The tab completion and introspection supports are very useful.\nHere are several commonly used shell commands:\n\ncd: change directory; .. means parent directory.\npwd: present working directory.\nls: list the content of a folder; -l long version; -a show hidden files; -t ordered by modification time.\nmkdir: create a new directory.\ncp: copy file/folder from a source to a target.\nmv: move file/folder from a source to a target.\nrm: remove a file a folder.\n\n\n\n1.3.4 Python\nSet up Python on your computer:\n\nPython 3.\nPython package manager miniconda or pip.\nIntegrated Development Environment (IDE) (Jupyter Notebook; RStudio; VS Code; Emacs; etc.)\n\nI will be using VS Code in class.\nReadability is important! Check your Python coding styles against the recommended styles: https://peps.python.org/pep-0008/. A good place to start is the Section on “Code Lay-out”.\nOnline books on Python for data science:\n\n“Python Data Science Handbook: Essential Tools for Working with Data,” First Edition, by Jake VanderPlas, O’Reilly Media, 2016.\n\n\n“Python for Data Analysis: Data Wrangling with Pandas, NumPy, and IPython.” Third Edition, by Wes McK- inney, O’Reilly Media, 2022.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#data-science-ethics",
    "href": "intro.html#data-science-ethics",
    "title": "1  Introduction",
    "section": "1.4 Data Science Ethics",
    "text": "1.4 Data Science Ethics\n\n1.4.1 Introduction\nEthics in data science is a fundamental consideration throughout the lifecycle of any project. Data science ethics refers to the principles and practices that guide responsible and fair use of data to ensure that individual rights are respected, societal welfare is prioritized, and harmful outcomes are avoided. Ethical frameworks like the Belmont Report (Protection of Human Subjects of Biomedical & Research, 1979)} and regulations such as the Health Insurance Portability and Accountability Act (HIPAA) (Health & Services, 1996) have established foundational principles that inspire ethical considerations in research and data use. This section explores key principles of ethical data science and provides guidance on implementing these principles in practice.\n\n\n1.4.2 Principles of Ethical Data Science\n\n1.4.2.1 Respect for Privacy\nSafeguarding privacy is critical in data science. Projects should comply with data protection regulations, such as the General Data Protection Regulation (GDPR) or the California Consumer Privacy Act (CCPA). Techniques like anonymization and pseudonymization must be applied to protect sensitive information. Beyond legal compliance, data scientists should consider the ethical implications of using personal data.\nThe principles established by the Belmont Report emphasize respect for persons, which aligns with safeguarding individual privacy. Protecting privacy also involves limiting data collection to what is strictly necessary. Minimizing the use of identifiable information and implementing secure data storage practices are essential steps. Transparency about how data is used further builds trust with stakeholders.\n\n\n1.4.2.2 Commitment to Fairness\nBias can arise at any stage of the data science pipeline, from data collection to algorithm development. Ethical practice requires actively identifying and addressing biases to prevent harm to underrepresented groups. Fairness should guide the design and deployment of models, ensuring equitable treatment across diverse populations.\nTo achieve fairness, data scientists must assess datasets for representativeness and use tools to detect potential biases. Regular evaluation of model outcomes against fairness metrics helps ensure that systems remain non-discriminatory. The Americans with Disabilities Act (ADA) (Congress, 1990) provides a legal framework emphasizing equitable access, which can inspire fairness in algorithmic design. Collaborating with domain experts and stakeholders can provide additional insights into fairness issues.\n\n\n1.4.2.3 Emphasis on Transparency\nTransparency builds trust and accountability in data science. Models should be interpretable, with clear documentation explaining their design, assumptions, and decision-making processes. Data scientists must communicate results in a way that stakeholders can understand, avoiding unnecessary complexity or obfuscation.\nTransparent practices include providing stakeholders access to relevant information about model performance and limitations. The Federal Data Strategy (Team, 2019) calls for transparency in public sector data use, offering inspiration for practices in broader contexts. Visualizing decision pathways and using tools like LIME or SHAP can enhance interpretability. Establishing clear communication protocols ensures that non-technical audiences can engage with the findings effectively.\n\n\n1.4.2.4 Focus on Social Responsibility\nData science projects must align with ethical goals and anticipate their broader societal and environmental impacts. This includes considering how outputs may be used or misused and avoiding harm to vulnerable populations. Data scientists should aim to use their expertise to promote public welfare, addressing critical societal challenges such as health disparities, climate change, and education access.\nEngaging with diverse perspectives helps align projects with societal values. Ethical codes, such as those from the Association for Computing Machinery (ACM) (Computing Machinery (ACM), 2018), offer guidance on using technology for social good. Collaborating with policymakers and community representatives ensures that data-driven initiatives address real needs and avoid unintended consequences. Regular impact assessments help measure whether projects meet their ethical objectives.\n\n\n1.4.2.5 Adherence to Professional Integrity\nProfessional integrity underpins all ethical practices in data science. Adhering to established ethical guidelines, such as those from the American Statistical Association (ASA) ((ASA), 2018), ensures accountability. Practices like maintaining informed consent, avoiding data manipulation, and upholding rigor in analyses are essential for maintaining public trust in the field.\nEthical integrity also involves fostering a culture of honesty and openness within data science teams. Peer review and independent validation of findings can help identify potential errors or biases. Documenting methodologies and maintaining transparency in reporting further strengthen trust.\n\n\n\n1.4.3 Ensuring Ethics in Practice\n\n1.4.3.1 Building Ethical Awareness\nPromoting ethical awareness begins with education and training. Institutions should integrate ethics into data science curricula, emphasizing real-world scenarios and decision-making. Organizations should conduct regular training to ensure their teams remain informed about emerging ethical challenges.\nWorkshops and case studies can help data scientists understand the complexities of ethical decision-making. Providing access to resources, such as ethical guidelines and tools, supports continuous learning. Leadership support is critical for embedding ethics into organizational culture.\n\n\n1.4.3.2 Embedding Ethics in Workflows\nEthics must be embedded into every stage of the data science pipeline. Establishing frameworks for ethical review, such as ethics boards or peer-review processes, helps identify potential issues early. Tools for bias detection, explainability, and privacy protection should be standard components of workflows.\nStandard operating procedures for ethical reviews can formalize the consideration of ethics in project planning. Developing templates for documenting ethical decisions ensures consistency and accountability. Collaboration across teams enhances the ability to address ethical challenges comprehensively.\n\n\n1.4.3.3 Establishing Accountability Mechanisms\nClear accountability mechanisms are essential for ethical governance. This includes maintaining documentation for all decisions, establishing audit trails, and assigning responsibility for the outputs of data-driven systems. Organizations should encourage open dialogue about ethical concerns and support whistleblowers who raise issues.\nPeriodic audits of data science projects help ensure compliance with ethical standards. Organizations can benefit from external reviews to identify blind spots and improve their practices. Accountability fosters trust and aligns teams with ethical objectives.\n\n\n1.4.3.4 Engaging Stakeholders\nEthical data science requires collaboration with diverse stakeholders. Including perspectives from affected communities, policymakers, and interdisciplinary experts ensures that projects address real needs and avoid unintended consequences. Stakeholder engagement fosters trust and aligns projects with societal values.\nPublic consultations and focus groups can provide valuable feedback on the potential impacts of data science projects. Engaging with regulators and advocacy groups helps align projects with legal and ethical expectations. Transparent communication with stakeholders builds long-term relationships.\n\n\n1.4.3.5 Continuous Improvement\nEthics in data science is not static; it evolves with technology and societal expectations. Continuous improvement requires regular review of ethical practices, learning from past projects, and adapting to new challenges. Organizations should foster a culture of reflection and growth to remain aligned with ethical best practices.\nEstablishing mechanisms for feedback on ethical practices can identify areas for development. Sharing lessons learned through conferences and publications helps the broader community advance its understanding of ethics in data science.\n\n\n\n1.4.4 Conclusion\nData science ethics is a dynamic and integral aspect of the discipline. By adhering to principles of privacy, fairness, transparency, social responsibility, and integrity, data scientists can ensure their work contributes positively to society. Implementing these principles through structured workflows, stakeholder engagement, and continuous improvement establishes a foundation for trustworthy and impactful data science.\n\n\n\n\n(ASA), A. S. A. (2018). Ethical guidelines for statistical practice.\n\n\nComputing Machinery (ACM), A. for. (2018). Code of ethics and professional conduct.\n\n\nCongress, U. S. (1990). Americans with disabilities act of 1990 (ADA).\n\n\nHealth, U. S. D. of, & Services, H. (1996). Health insurance portability and accountability act of 1996 (HIPAA).\n\n\nProtection of Human Subjects of Biomedical, N. C. for the, & Research, B. (1979). The belmont report: Ethical principles and guidelines for the protection of human subjects of research.\n\n\nTeam, F. D. S. D. (2019). Federal data strategy 2020 action plan.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "git.html",
    "href": "git.html",
    "title": "2  Project Management",
    "section": "",
    "text": "2.1 Set Up Git/GitHub\nMany tutorials are available in different formats. Here is a YouTube video ``Git and GitHub for Beginners — Crash Course’’. The video also covers GitHub, a cloud service for Git which provides a cloud back up of your work and makes collaboration with co-workers easy. Similar services are, for example, bitbucket and GitLab.\nThere are tools that make learning Git easy.\nDownload Git if you don’t have it already.\nTo set up GitHub (other services like Bitbucket or GitLab are similar), you need to\nSee how to get started with GitHub account.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Management</span>"
    ]
  },
  {
    "objectID": "git.html#set-up-gitgithub",
    "href": "git.html#set-up-gitgithub",
    "title": "2  Project Management",
    "section": "",
    "text": "Generate an SSH key if you don’t have one already.\nSign up an GitHub account.\nAdd the SSH key to your GitHub account",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Management</span>"
    ]
  },
  {
    "objectID": "git.html#most-frequently-used-git-commands",
    "href": "git.html#most-frequently-used-git-commands",
    "title": "2  Project Management",
    "section": "2.2 Most Frequently Used Git Commands",
    "text": "2.2 Most Frequently Used Git Commands\nThe following seven commands will get you started and they may be all that you need most of the time.\n\ngit clone:\n\nUsed to clone a repository to a local folder.\nRequires either HTTPS link or SSH key to authenticate.\n\ngit pull:\n\nDownloads any updates made to the remote repository and automatically updates the local repository.\n\ngit status:\n\nReturns the state of the working directory.\nLists the files that have been modified, and are yet to be or have been staged and/or committed.\nShows if the local repository is begind or ahead a remote branch.\n\ngit add:\n\nAdds new or modified files to the Git staging area.\nGives the option to select which files are to be sent to the remote repository\n\ngit rm:\n\nUsed to remove files from the staging index or the local repository.\n\ngit commit:\n\nCommits changes made to the local repository and saves it like a snapshot.\nA message is recommended with every commit to keep track of changes made.\n\ngit push:\n\nUsed to send commits made on local repository to the remote repository.\n\n\nFor more advanced usages:\n\ngit diff\ngit branch\ngit reset",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Management</span>"
    ]
  },
  {
    "objectID": "git.html#tips-on-using-git",
    "href": "git.html#tips-on-using-git",
    "title": "2  Project Management",
    "section": "2.3 Tips on using Git:",
    "text": "2.3 Tips on using Git:\n\nUse the command line interface instead of the web interface (e.g., upload on GitHub)\nMake frequent small commits instead of rare large commits.\nMake commit messages informative and meaningful.\nName your files/folders by some reasonable convention.\n\nLower cases are better than upper cases.\nNo blanks in file/folder names.\n\nKeep the repo clean by not tracking generated files.\nCreat a .gitignore file for better output from git status.\nKeep the linewidth of sources to under 80 for better git diff view.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Management</span>"
    ]
  },
  {
    "objectID": "git.html#pull-request",
    "href": "git.html#pull-request",
    "title": "2  Project Management",
    "section": "2.4 Pull Request",
    "text": "2.4 Pull Request\nTo contribute to an open source project (e.g., our classnotes), use pull requests. Pull requests “let you tell others about changes you’ve pushed to a branch in a repository on GitHub. Once a pull request is opened, you can discuss and review the potential changes with collaborators and add follow-up commits before your changes are merged into the base branch.”\nWatch this YouTube video: GitHub pull requests in 100 seconds.\nThe following are step-by-step instructions on how to make a pull request to the class notes contributed by Nick Pfeifer..\n\nCreate a fork of the class repository on the GitHub website.\n\nMake sure your fork is up to date by clicking Sync fork if necessary.\n\nClone your fork into a folder on your computer.\n\ngit clone https://github.com/GitHub_Username/ids-s25.git\nReplace GitHub_Username with your personal GitHub Username.\n\nCheck to see if you can access the folder/cloned repository in your code editor.\n\nThe class notes home page is located in the index.qmd file.\n\nMake a branch and give it a good name.\n\nMove into the directory with the cloned repository.\nCreate a branch using:\n\ngit checkout -b branch_name\nReplace branch_name with a more descriptive name.\n\nYou can check your branches using:\n\ngit branch\nThe branch in use will have an asterisk to the left of it.\n\nIf you are not in the right branch you can use the following command:\n\ngit checkout existing-branch\nReplace existing-branch with the name of the branch you want to use.\n\n\nRun git status to verify that no changes have been made.\nMake changes to a file in the class notes repository.\n\nFor example: add your wishes to the Wishlist in index.qmd using nested list syntax in markdown.\nRemember to save your changes.\n\nRun git status again to see that changes have been made.\nUse the add command.\n\ngit add filename\nExample usage: git add index.qmd\n\nMake a commit.\n\ngit commit -m \"Informative Message\"\nBe clear about what you changed and perhaps include your name in the message.\n\nPush the files to GitHub.\n\ngit push origin branch-name\nReplace branch-name with the name of your current branch.\n\nGo to your forked repository on GitHub and refresh the page, you should see a button that says Compare and Pull Request.\n\nDescribe the changes you made in the pull request.\nClick Create pull request.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Management</span>"
    ]
  },
  {
    "objectID": "quarto.html",
    "href": "quarto.html",
    "title": "3  Reproducible Data Science",
    "section": "",
    "text": "3.1 Introduction to Quarto\nData science projects should be reproducible to be trustworthy. Dynamic documents facilitate reproducibility. Quarto is an open-source dynamic document preparation system, ideal for scientific and technical publishing. From the official websites, Quarto can be used to:\nTo get started with Quarto, see documentation at Quarto.\nFor a clean style, I suggest that you use VS Code as your IDE. The ipynb files have extra formats in plain texts, which are not as clean as qmd files. There are, of course, tools to convert between the two representations of a notebook. For example:\nWe will use Quarto for homework assignments, classnotes, and presentations. You will see them in action through in-class demonstrations. The following sections in the Quarto Guide are immediately useful.\nA template for homework is in this repo (hwtemp.qmd) to get you started with homework assignments.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reproducible Data Science</span>"
    ]
  },
  {
    "objectID": "quarto.html#introduction-to-quarto",
    "href": "quarto.html#introduction-to-quarto",
    "title": "3  Reproducible Data Science",
    "section": "",
    "text": "quarto convert hello.ipynb # converts to qmd\nquarto convert hello.qmd   # converts to ipynb\n\n\nMarkdown basics\nUsing Python\nPresentations",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reproducible Data Science</span>"
    ]
  },
  {
    "objectID": "quarto.html#sec-buildnotes",
    "href": "quarto.html#sec-buildnotes",
    "title": "3  Reproducible Data Science",
    "section": "3.2 Compiling the Classnotes",
    "text": "3.2 Compiling the Classnotes\nThe sources of the classnotes are at https://github.com/statds/ids-s25. This is also the source tree that you will contributed to this semester. I expect that you clone the repository to your own computer, update it frequently, and compile the latest version on your computer (reproducibility).\nTo compile the classnotes, you need the following tools: Git, Quarto, and Python.\n\n3.2.1 Set up your Python Virtual Environment\nI suggest that a Python virtual environment for the classnotes be set up in the current directory for reproducibility. A Python virtual environment is simply a directory with a particular file structure, which contains a specific Python interpreter and software libraries and binaries needed to support a project. It allows us to isolate our Python development projects from our system installed Python and other Python environments.\nTo create a Python virtual environment for our classnotes:\npython3 -m venv .ids-s25-venv\nHere .ids-s25-venv is the name of the virtual environment to be created. Choose an informative name. This only needs to be set up once.\nTo activate this virtual environment:\n. .ids-s25-venv/bin/activate\nAfter activating the virtual environment, you will see (.ids-s25-venv) at the beginning of your shell prompt. Then, the Python interpreter and packages needed will be the local versions in this virtual environment without interfering your system-wide installation or other virtual environments.\nTo install the Python packages that are needed to compile the classnotes, we have a requirements.txt file that specifies the packages and their versions. They can be installed easily with:\npip install -r requirements.txt\nIf you are interested in learning how to create the requirements.txt file, just put your question into a Google search.\nTo exit the virtual environment, simply type deactivate in your command line. This will return you to your system’s global Python environment.\n\n\n3.2.2 Clone the Repository\nClone the repository to your own computer. In a terminal (command line), go to an appropriate directory (folder), and clone the repo. For example, if you use ssh for authentication:\ngit clone git@github.com:statds/ids-s25.git\n\n\n3.2.3 Render the Classnotes\nAssuming quarto has been set up, we render the classnotes in the cloned repository\ncd ids-s25\nquarto render\nIf there are error messages, search and find solutions to clear them. Otherwise, the html version of the notes will be available under _book/index.html, which is default location of the output.\n\n\n3.2.4 Login Requirements\nFor some illustrations, you need to interact with certain sites that require account information. For example, for Google map services, you need to save your API key in a file named api_key.txt in the root folder of the source. Another example is to access the US Census API, where you would need to register an account and get your Census API Key.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reproducible Data Science</span>"
    ]
  },
  {
    "objectID": "quarto.html#a-primer-of-markdown",
    "href": "quarto.html#a-primer-of-markdown",
    "title": "3  Reproducible Data Science",
    "section": "3.3 A Primer of Markdown",
    "text": "3.3 A Primer of Markdown\nThis section was prepared by Shiyi Li. I am a senior majoring in Mathematics/ Statistics and minoring in CSE. I aim to graduate from the University of Connecticut in August and continue my master’s in Data Science program.\n\n3.3.1 Introduction\nToday’s presentation, I will introduce some basic syntax on markdown. Markdown is a plain text format, an easy way to write content for a web interface. It is widely used in open-source documentation, including GitHub, R Markdown, Quarto, Jupyter Notebooks, etc. Its syntax is easy to learn, write, read, and seamlessly convert to HTML, PDF, and DOCX formats. I will divide my presentation into three parts, Structuring the Document, Formatting Text, and Enhancing Content.\n\n\n3.3.2 Structuring the Document\nThere are some elements can define sections and hierarchy to help organize content in a clear and structured manner.\n\n3.3.2.1 Headings\n\nTo create heading levels for topics, sections, or subsections, you can add number signs, “#”, before you write any content for them.\n\n\nExample:\n\nYou can code like this:\n\n# Heading 1 (Main Title)\n\n## Heading 2 (Section)\n\n### Heading 3 (Subsection)\n\n#### Heading 4\n\n##### Heading 5\n\n###### Heading 6\n\nBe careful, you need to make sure that there is a blank line before and after your heading levels and a space between your number sign “#” and the heading name.\n\n\nAlternatively, you can also use any number of double equal sign, “==”, or double dashes sign, “–”, to create a heading.\n\n\nExample:\n\nYou can code like this:\n\nHeading 1 (Main Title)\n======\nHeading 2 (Section)\n------\n\nHowever, this method can only be used to create two heading levels like above.\n\n\n\n3.3.2.2 Horizontal Rules - line Seperator\n\nA horizontal rule adds a visual break between parts of text by adding three or more asterisks, “***“, dashes,”—“, or underscores,”___“, on a line by themselves.\nExample:\n\nYou can code like this:\nThis is the part 1.\n\n--------\n\nThis is the part 2.\n\n********\n\nThis is the part 3.\n\n_________\n\nThis is the part 4.\n\nThis code chunk will output like this:\n\nThis is the part 1.\n\nThis is the part 2.\n\nThis is the part 3.\n\nThis is the part 4.\n\nBe careful. You need to make sure you add a blank line before and after your separator line.\n\n\n\n3.3.2.3 Paragraphs\n\nTo create paragraphs, you can add a blank line between two paragraphs just like you usually create paragraphs in an essay.\nExample:\n\nYou can code like this:\nThis is the first paragraph ........................................\n.....................................................................\n.....................................................................\n... end the first paragraph.\n\nThis is the second paragraph .........................................................\n...................................................................\n.....................................................................\n...end the second paragraph.\n\nThis will output like this:\n\nThis is the first paragraph …………………………………. …………………………………………………………… …………………………………………………………… … end the first paragraph.\nThis is the second paragraph ………………………………………………… …………………………………………………………. …………………………………………………………… …end the second paragraph.\n\n\n3.3.2.4 Blockquotes\n\nTo highlight multiple line important text, cite multiple line references, or quote multiple line important points, you can add a greater than sign, “&gt;”, at the begining of each line of the text.\n\n\nExample:\n\nYou can code like this:\n&gt; This is an important text.\n&gt;\n&gt; This is a citation/reference.\n&gt;\n&gt; This is a quotation.\n\nThis code will output like this:\n\n\nThis is an important text.\nThis is a citation/reference.\nThis is a quotation.\n\n\nWrong Example:\n\nIf you code like this:\n&gt; This is an important text.\n&gt; This is a citation/reference.\n&gt; This is a quotation.\n\nIt will output like this:\n\n\nThis is an important text. This is a citation/reference. This is a quotation.\n\nBe sure to add a blank line with a greater-than sign, “&gt;”, otherwise the output will display multiple lines of text in a single line.\n\nYou can also nest blockquotes by adding two or more greater than signs, “&gt;&gt;”.\n\n\nExample:\n\nYou can code like this:\n&gt; This is an important text.\n&gt;\n&gt;&gt; This is a citation/reference.\n&gt;&gt;\n&gt;&gt;&gt; This is a quotation.\n\nThis code will output like this:\n\n\nThis is an important text.\n\nThis is a citation/reference.\n\nThis is a quotation.\n\n\n\nBe sure to add a blank line before the first line of your blockquotes, otherwise, it will not look right.\n\n\n\n3.3.3 Formatting Text\nThere are some elements to make text stand out, improving emphasis and readability like Bold, Italic, and Strikethrough text.\n\n3.3.3.1 Bold\n\nTo bold text, you can add two asterisks, “**“, or underscores,”__“, before and after the text.\nExample:\n\nYou can code like this:\n**This is the first important text.**\n\n__This is the second important text.__\n\nThis is the __third important__ text.\n\nThis is the **fourth important** text.\n\nThis is the f**if**th important text.\n\nThis will output like this:\n\nThis is the first important text.\nThis is the second important text.\nThis is the third important text.\nThis is the fourth important text.\nThis is the fifth important text.\n\nBe careful do not use underscores, “__“, to bold charachers inside a word, like this”This is the fifth im__portan__t text”.***\n\n\n\n3.3.3.2 Italic\n\nTo italicize a text, you can add one asterisk, “*“, or one underscore,”_“, before and after a text.\nExample:\n\nYou can code like this:\n*ThIs Is the fIrst Important TexT.*\n\n_ThIs Is the second Important TexT._\n\nThis is the _Third ImportanT_ text.\n\nThis is the *fourth ImportanT* text.\n\nFifth im*PORtaN*t text.\n\nThis code chunk will output like this:\n\nThIs Is the fIrst Important TexT.\nThIs Is the second Important TexT.\nThis is the Third ImportanT text.\nThis is the fourth ImportanT text.\nFifth imPORtaNt text.\n\nBe careful don’t use underscores, “_“, to italicize charachers inside a word, like this”Fifth im_porta_nt text”.\n\n\n\n3.3.3.3 Bold & Italic\nTo bold and italicize for the same text, you can use three asterisks, “***“, or three underscores,”___“, before and after a text.\n\nExample:\n\nYou can code like this:\n***This is the first important text.***\n\n___This is the second important text.___\n\nThis is the ___third important___ text.\n\nThis is the ***fourth important*** text.\n\nFifth i***mportan***t text.\n\nThis code chunk will output like this:\n\nThis is the first important text.\nThis is the second important text.\nThis is the third important text.\nThis is the fourth important text.\nFifth important text.\n\nBe careful don’t use underscores, “___“, to bold and italicize charachers inside a word, like this”Fifth im___porta___nt text”.\n\n\n\n3.3.3.4 Highlight\n\nTo highlight a text, you can add this sign, “&lt;mark&gt;”, before the text and add this sign, “&lt;/mark&gt;”, after the text.\nExample:\n\nYou can code like this:\n&lt;mark&gt;This is a text that needs to be highlighted.&lt;/mark&gt;\n\nThis is a te&lt;mark&gt;xt that needs to be high&lt;/mark&gt;lighted.\n\nThis code chunk will output like this:\n\nThis is a text that needs to be highlighted.\nThis is a text that needs to be highlighted.\n\n\n3.3.3.5 Strikethrough - Deleted or Removed Text\n\nTo show a deletion or correction on a text, you can add double tilde signs, “~~”, before and after the part of the deletion or correction on your text.\nExample:\n\nYou can code like this:\n~~This text is struck through~~ This is the correct text.\n\nThis code chunk will output like this:\n\nThis text is struck through This is the correct text.\n\n\n\n3.3.4 Enhancing Content\nTo enhance the illustrative capabilities of your content, there are several elements you can add to your document, including subscript, superscript, lists, tables, footnotes, links, images, math notations, etc.\n\n3.3.4.1 Subscript\n\nTo add a subscript before, after or within a number or word, you can add a tilde symbol, “~”, before and after the text you want to subscript.\nExample:\n\nYou can code like this:\nThis is a subscript before and after a word: \n\n~subscript~word~subscript~\n\nThis is a subscript within a word: \n\nWor~111000~ds\n\nThis is a subscript before and after a number: \n\n~7878~11111~7878~ \n\nThis is a subscript within a number: \n\n999~subscript~999\n\nThis code chunk will output like this:\n\nThis is a subscript before and after a word:\nsubscriptwordsubscript\nThis is a subscript within a word:\nWor111000ds\nThis is a subscript before and after a number:\n7878111117878\nThis is a subscript within a number:\n999subscript999\n\nBe sure not to add any spaces or tabs between the two tilde symbols, “~ ~”.\n\n\n\n3.3.4.2 Superscript\n\nTo add a superscript before, after or within a number or word, you can add a caret symbol, “^”, before and after the text you want to superscript.\nExample:\n\nYou can code like this:\n\nThis is a superscript before and after a word:\n\n^787878^Words^787878^\n\nThis is a superscript within a word:\n\nWor^787878^ds\n\nThis is a superscript before and after a number:\n\n^superscript^1111111^superscript^  \n\nThis is a superscript within a number:\n\n999^superscript^999\n\nThis will output like this:\n\nThis is a superscript before and after a word:\n787878Words787878\nThis is a superscript within a word:\nWor787878ds\nThis is a superscript before and after a number:\nsuperscript1111111superscript\nThis is a superscript within a number:\n999superscript999\n\nBe sure not to add any spaces or tabs between the two caret symbols, “…”.\n\n\n\n3.3.4.3 Lists\nTo organize a list (nested list), you can use ordered or unordered numbers or alphabets followed by a period sign, “.”, dashes, “-”, asterisks, “*“, or plus signs,”+“, in front of line items. Markdown is smart, it will automatically detect and organize a list for you.\n1. Using Ordered numbers followed by a period sign:\n\nExample:\n\nYou can code like this:\n\n1. First item\n2. Second item\n    1. Third item\n    2. Fourth item\n3. Fifth item\n\nThis code chunk will output like this:\n\nUsing Ordered numbers followed by a period sign:\n\nFirst item\nSecond item\n\nThird item\nFourth item\n\nFifth item\n\n2. Using Unordered numbers followed by a period sign:\n\nExample:\n\nYou can code like this:\n\n2. First item\n2. Second item\n    2. Third item\n    2. Fourth item\n2. Fifth item\n\n2. First item\n7. Second item\n    9. Third item\n    2. Fourth item\n10. Fifth item\n\nThis code chunk will output like this:\n\nUsing Unordered numbers followed by a period sign:\n\nFirst item\nSecond item\n\nThird item\nFourth item\n\nFifth item\nFirst item\nSecond item\n\nThird item\nFourth item\n\nFifth item\n\n\nBe careful, for an unordered list to work as you want, you need to take care of the first number or letter of the first item of your (nested) list, because markdown will order the list starting with the first number or alphabet of your (nested) list.\n\n3. Using dashes:\n\nExample:\n\nYou can code like this:\n\n- First item\n- Second item\n    - Third item\n    - Fourth item\n- Fifth item\n\nThis code chunk will output like this:\n\nUsing dashes:\n\nFirst item\nSecond item\n\nThird item\nFourth item\n\nFifth item\n\n4. Using asterisks:\n\nExample:\n\nYou can code like this:\n\n* First item\n* Second item\n    * Third item\n    * Fourth item\n* Fifth item\n\nThis code chunk will output like this:\n\nUsing asterisks:\n\nFirst item\nSecond item\n\nThird item\nFourth item\n\nFifth item\n\n5. Using plus signs:\n\nExample:\n\nYou can code like this:\n\n+ First item\n+ Second item\n    + Third item\n    + Fourth item\n+ Fifth item\n\nThis code chunk will output like this:\n\nUsing plus signs:\n\nFirst item\nSecond item\n\nThird item\nFourth item\n\nFifth item\n\n6. Using alphabets:\n\nExample:\n\nYou can code like this:\n\na. First item\nb. Second item\n    a. Third item\n    b. Fourth item\nc. Fifth item\n\nw. First item\na. Second item\n    c. Third item\n    y. Fourth item\na. Fifth item\n\nThis code chunk will output like this:\n\nUsing alphabets:\n\nFirst item\nSecond item\n\nThird item\nFourth item\n\nFifth item\nFirst item\nSecond item\n\nThird item\nFourth item\n\nFifth item\n\n7. Using different delimiters:\n\nExample:\n\nYou can code like this:\n\n+ First item\n- Second item\n    * Third item\n    + Fourth item\n* Fifth item\n\nThis code chunk will output like this:\n\nUsing different delimiters:\n\nFirst item\nSecond item\n\nThird item\nFourth item\n\nFifth item\n\n\nUsing different delimiters in the same list has no effect on the list organized by markdown.\nBe careful, you need to make sure you add a blank line before the list starts.\nYou can also use a backslash, “\", to escape a period,”.”, if you do not want to create a list and still need a period after a number or alphabet.\n\n\n\n3.3.4.4 Task Lists - To-Do List\n\nTo add a task/to-do list, you can add this sign, “- [ ]”, or this sign, “- [x]”, before each item in your task/to-do list.\nExample:\n\nYou can code like this:\n\n- [ ] Task not completed\n- [x] Task completed\n\nThis code chunk will output like this:\nTask not completed\nTask completed\n\n\nIn your rendered HTML file, you can check or uncheck the completion marks in the small box at the front of each task.\n\n\n\n3.3.4.5 Links\n\nYou can add a link in your text by enclosing your added link with parentheses, “()”. In addition, you can also optionally add a name or a short description for the link by enclosing them with brackets, “[]”, before the link and this will appear as a tooltip when the user hovers over the link\n\n\n\n\n\nExample:\n\nYou can code like this:\n\nWe can use the \n[Markdown Cheat Sheet](https://www.markdownguide.org/basic-syntax/#code-blocks) \nto learn more generally used markdown syntax. \n\nThis code chunk will output like this:\n\nWe can use the Markdown Cheat Sheet to learn more generally used markdown syntax.\n\nYou can add an Reference Style Link by enclosing the name or description of the link and a number/word pointed to the link with brackets, “[]”.\n\n\nExample:\n\nYou can code like this:\n\nWe can use the [Markdown Cheat Sheet][4] to learn more generally used \nmarkdown syntax. \n\n[4]: https://www.markdownguide.org/basic-syntax/#code-blocks\n\nThis code chunk will output like this:\n\nWe can use the Markdown Cheat Sheet to learn more generally used markdown syntax.\n\nRemember to start a new line and add the link as a footnote after the number or word pointed to the reference enclosed by brackets, “[]”.\n\n\n\n3.3.4.6 Images\n\nTo add images from your local computer or a website, you can add an exclamation mark, “!”, followed by a description or other text enclosing with brackets, “[]”, and a path/URL to the image enclosing with parentheses, “()”.\nExample:\n\nYou can code like this:\n\n![This is a description to an online image](https://today.uconn.edu/2023/01/\nuconn-on-campus-construction-update-january-2023/)\n\n![This is a description to a local image](/Users/shiyili/\nDesktop/UCONN.jpg)\n\nThe render output will show like this:\n\n\n\n\nThis is a description to an online image.\n\n\n\n\n3.3.4.7 Tables\n\nTo create a table, you can use three or more hyphens, “—”, and pipes, “|”, to create and separate each column respectively.\nExample:\n\nYou can code like this:\n\n1. Each cell with the same width in code chunk:\n\n|            | 1st Column | 2nd Column | 3rd Column | ...... |\n| ---------- | ---------- | ---------- | ---------- | ------ |\n| 1st Row    |    123     |     123    |     123    |   123  |\n| 2nd Row    |    123     |     123    |     123    |   123  |\n| 3rd Row    |    123     |     123    |     123    |   123  |\n| ......     |    123     |     123    |     123    |   123  |\n\n2. Each cell with vary width in code chunk:\n\n|            | 1st Column | 2nd Column | 3rd Column | ...... |\n| ------ | ---------- | ---------- | ---------- | ------ |\n| 1st Row |     123       |      123      |     123       |    123    |\n| 2nd Row      |     123       |     123       |      123      |   123     |\n| 3rd Row  |      123      |      123      |     123       |    123    |\n| ......         |      123      |     123       |      123      |    123    |\n\nThis code chunk will output like this:\n\n\nEach cell with the same width in code chunk:\n\n\n\n\n\n\n\n\n\n\n\n\n1st Column\n2nd Column\n3rd Column\n……\n\n\n\n\n1st Row\n123\n123\n123\n123\n\n\n2nd Row\n123\n123\n123\n123\n\n\n3rd Row\n123\n123\n123\n123\n\n\n……\n123\n123\n123\n123\n\n\n\n\nEach cell with vary width in code chunk:\n\n\n\n\n\n\n\n\n\n\n\n\n1st Column\n2nd Column\n3rd Column\n……\n\n\n\n\n1st Row\n123\n123\n123\n123\n\n\n2nd Row\n123\n123\n123\n123\n\n\n3rd Row\n123\n123\n123\n123\n\n\n……\n123\n123\n123\n123\n\n\n\n\nCell width can vary, because markdown will automatically detect and organize the table for you with the same width.***\n\n\nYou can align text in the columns to the left, right, or center by adding a colon, “:”, to the left, right, or on both side of the hyphens, “—”, within the header (Cone, 2025).\n\n\nExample:\n\nYou can code like this:\n\n|            | 1st Column | 2nd Column | 3rd Column | ...... |\n| :----------: | :---------- | :----------: | ----------: | ------: |\n| 1st Row    |            123|       123     |  123          |        123|\n| 2nd Row    |123            |    123        |           123 |123        |\n| 3rd Row    |     123       |123            |     123       |   123     |\n| ......     |   123         |            123| 123           |123        |\n\nThis code chunk will output like this:\n\n\n\n\n\n\n\n\n\n\n\n\n1st Column\n2nd Column\n3rd Column\n……\n\n\n\n\n1st Row\n123\n123\n123\n123\n\n\n2nd Row\n123\n123\n123\n123\n\n\n3rd Row\n123\n123\n123\n123\n\n\n……\n123\n123\n123\n123\n\n\n\n\n\n3.3.4.8 Code\n\nTo add an inline code, you can add a backtick, “`”, before and after a word or a text.\n\n\nExample:\n\nYou can code like this:\n\nThis is my `inline` code.\n\n`This is my inline code.`\n\nThis will output like this:\n\nThis is my inline code.\nThis is my inline code.\n\nIf you want to display a text with a backtick, “`”“, as an inline code, you can add a double backtick,”``“, before and after the text.\n\n\nExample:\n\nYou can code like this:\n\n``This is a `text` with backticks.``\n\nThis will output like this:\n\nThis is a `text` with backticks.\n\nTo add a code block, you can indent each line of the text with more than four spaces or one tab.\n\n\nExample:\n\nYou can code like this:\n\n    This is a code block.\n\n        This is a code block.\n\n            This is a code block.\n\n    This is a code block.\n\nThis will output like this:\n\n    This is a code block.\n\n        This is a code block.\n\n            This is a code block.\n\n    This is a code block.\n\nTo add a fenced code block, you can add a blank line begining with three backticks, “```”, or three tilde signs, “~~~”, before and after your code blok.\n\n\nExample:\n\nYou can code like this:\n\n    ```\n    {\n    This is my fenced code block.\n    This is my fenced code block.\n    }\n    ```\n\nThis will output like this:\n\n{\nThis is my fenced code block.\nThis is my fenced code block.\n}\n\nTo add a nonexecutive Python code chunk, you can first add a fence code block for your code, then add a word, “python”, after the three backticks, “```”, in the first line of your fence code block.\n\n\nExample :\n\nYou can code like this:\n    ```python\n    print(\"This is a Python code chunk.\")\n    ```\n\nThis will output like this:\n\nprint(\"This is a Python code chunk.\")\n\nTo add an executive Python code chunk, you can first add a fence code block for your code, then add a word, “python”, with brackets, “{}”after the three backticks, “```”, in the first line of your fence code block.\n\n\nExample:\n\nYou can code like this:\n    ```{python}\n    print(\"This is a Python code chunk.\")\n    ```\n\nThis will output like this:\n\n\nprint(\"This is a Python code chunk.\")\n\nThis is a Python code chunk.\n\n\n\nYou can also make an executive Python code chunk not execute the commands inside the code chunk by adding “#| eval: false” to the first line of your code block.\n\n\nExample:\n\nYou can code like this:\n    ```{python}\n    #| eval: false\n\n    print(\"This is a Python code chunk.\")\n    ```\n\nThis will output like this:\n\n\nprint(\"This is a Python code chunk.\")\n\n\nIf you add just “#| eval: true” to your code chunk, this code chunk will execute the commands as usual and output the results.\nIf you just add “#| echo: false” to your code chunk, then your code chunk will not be displayed in your rendered output, but the commands in your code chunk will still be executed as usual and the result of the code chunk will be displayed.\nIf you just add “#| output: false” to your code chunk, then the commands in your code chunk will be displayed as usual in the rendered output, but the result of your code chunk will not be displayed.\n\n\n\n3.3.4.9 Math Notation - Using LaTeX in Markdown\n\nTo displace an inline math equation, you can add a dollar sign, “$”, before and after the math equation.\n\n\nExample:\n\nYou can code like this:\n\nThis is a quadratic equation, $ax^2+bx+c=0$.\n\nThis is a quadratic equation roots formula, $x = {(-b \\pm \\sqrt{b^2-4ac})}/{2b}$.\n\nThis code chunk will output like this:\n\nThis is a quadratic equation, \\(ax^2+bx+c=0\\).\nThis is a quadratic equation roots formula, \\(x = {(-b \\pm \\sqrt{b^2-4ac})}/{2b}\\).\n\nIn an inline math equation, make sure you use the division symbol instead of “\\frac{}{}”.\n\n\nYou can center an math equation by adding double dollar signs, “$$”, before and after the math equation.\n\n\nExample:\n\nYou can code like this:\n\nThis is a quadratic equation \n$$\nax^2+bx+c=0.\n$$\n\nThis is a quadratic equation roots formula \n$$\nx = {(-b \\pm \\sqrt{b^2-4ac})}/{2b}.\n$$\n\nThis code chunk will output like this:\n\nThis is a quadratic equation \\[\nax^2+bx+c=0.\n\\]\nThis is a quadratic equation roots formula \\[\nx = {(-b \\pm \\sqrt{b^2-4ac})}/{2b}.\n\\]\n\nBe careful not to add punctuation before a centered math equation.\nIf you want to add a period after a centered math equation, make sure you add it after the end of the math equation and before the second double dollar sign, “$$”.\n\n\n\n3.3.4.10 Footnotes\n\nTo add notes and refercences without confusing for body content, you can add a caret sign, “^”, follwed by a identifier number or words without any spaces and tabs inside a brackets, “[]”.\nExample:\n\nyou can code like this:\n\nThis is a body paragraph, and I have a note [^short_footnotes] want to \nadd here. [^long_footnotes]\n\n[^short_footnotes]: This is a notes.\n\n[^long_footnotes]: This is a notes with multiple paragraphs.\n    You can include paragraphs in your footnotes using indentation like this.\n    ```\n    {\n        This is a fenced code block.\n    }\n    ```\n    This is the end of this footnote.\n\nThis code chunk will output like this:\n\nThis is a body paragraph, and I have two notes 1 want to add here. 2\n\nClicking on the footnote number in the body content, will take you to the location where the footnote exists in the document.\nAll footnotes are automatically numbered sequentially at the end of the rendered HTML files and at the bottom of the page where the footnote exists in rendered PDF files.\n\nIn the rendered HTML file, each footnote displayed at the end of the document is followed by a link that, when clicked, takes you back to the specific location of the footnote in your body content. You can also move your mouse over the footnote number in the body content, and the content of the footnote will automatically appear below the footnote number.\n\n\n\n\n\n3.3.5 Conclusion\nMarkdown is a simple yet powerful way to format text for documentation, blogging, and technical writing. With its easy-to-read syntax, you can structure documents, highlight key points, and present data effectively. It’s widely used in open-source projects, academic writing, and web content due to its flexibility and seamless conversion to HTML, PDF, and DOCX. Mastering Markdown can help you create clear, well-organized content with minimal effort.\n\n\n3.3.6 Further Readings\n\n[Markdown Cheet Sheet] (Cone, 2025): A quick reference to the Markdown syntax.\n[Quarto Markdown Basic] (Dervieux, 2025): Markdown basic for Quarto.\n[GitHub Flavored Markdown (GFM)] (MacFarlane, 2019): Markdown features specific to GitHub.\n[Jupyter Notebook Markdown] (MacFarlane, 2006): Use Markdown for interactive data science.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reproducible Data Science</span>"
    ]
  },
  {
    "objectID": "quarto.html#creating-presentations-using-quarto",
    "href": "quarto.html#creating-presentations-using-quarto",
    "title": "3  Reproducible Data Science",
    "section": "3.4 Creating Presentations Using Quarto",
    "text": "3.4 Creating Presentations Using Quarto\n\n3.4.1 Introduction\nHi! This section is written by Joann Jun, a Junior majoring in Statistical Data Science and minoring in Mathematics at the University of Connecticut.\nThis section will focus on how to create a presentation using Quarto. By the end of this section, you will be able to understand how to:\n\nStructure a Quarto presentation\nCustomize themes, transitions, and interactivity\nKeyboard Shortcuts\n\n\n\n3.4.2 Why Use Quarto for Presentations?\n\nSeamlessly integrate code, data analysis, and visualizations into a single document\nGenerate interactive slides with minimal effort\nSupport multiple output formats, such as HTML (reveal.js), PowerPoint (pptx), and PDF (beamer).\n\n\n\n3.4.3 Presentation Formats\n\n3.4.3.1 Formats\nThere are several formats you can use to create a presentation in Quarto. This includes:\n\nrevealjs - reveal.js (HTML)\nbeamer - Beamer (LaTex/PDF).\n\n\n\n3.4.3.2 Differences Between Formats\n\n\n\n\n\n\n\n\nFeature\nrevealjs\nbeamer\n\n\n\n\nOutput Format\nHTML slideshow or PDF\nPDF\n\n\nPros\n- Works well with Markdown  - Interactive and dynamic\n- Good math support  - Professional typesetting (LaTeX)\n\n\nCons\n- Requires a browser to present\n- Requires knowledge of LaTeX\n\n\n\nIn this section, I will focus on using revealjs.\n\n\n3.4.3.3 How to Change Format\nTo change the format of your presentation, in the YAML header next format add revealjs or beamer.\n---\ntitle: Quarto Presentation\nauthor: Joann Jun\nformat: revealjs # This where you edit the format.\n---\n\n\n3.4.3.4 YAML Heading for revealjs\nAnything you put in the YAML heading will become part of the global environment. This means that it will be applied to all slides.\n\nembed-resources: true - Creates self-contained file to distribute more easily.\nmultiplex: true - Allows your audience to follow the slides that you control on their own device.\n\nWhen you render, it’ll create 2 HTML files:\n\npresentations.html - Publish online for the audience to use.\npresentations-speaker.html - File you present from and you don’t publish this.\n\n\nchalkboard - Allows you to draw on your presentation\n\n\n\n3.4.3.5 Some Stylistic YAML Headings for revealjs\n\ntheme: [slide theme] - Allows you to switch to any of Reveals 11 themes (or make your own)\n\ndefault, dark, beige, simple, serif\n\ntransition: [transition] - Adds transitions to slides\n\nnone, slide, fade, convex, concave, zoom\n\nlogo: logo.png - Allows you to add logo to bottom of each slide.\nfooter: \"Footer Note\" - Adds a footer to bottom of each slide.\nslide-number: true - Displays the slide number at bottom of screen.\nincrimental: true - Displays the bullet points one by one.\n\n\n\n3.4.3.6 Example\n---\ntitle: \"How to Make A Presentation Using Quarto\"\nauthor: \"Joann Jun\"\nformat:\n  revealjs: \n    embed-resources: true\n    theme: serif\n    slide-number: true\n    preview-links: true\n    css: [default, styles.css] # Don't need this unless customizing more\n    incremental: true   \n    transition: slide\n    footer: \"STAT 3255\"\n\n---\n\n\n\n3.4.4 Slides syntax\n\n3.4.4.1 How to Create a New Slide\nThe start of all slides are marked by a heading. You can do this by using\n\nLevel 1 header (#) - Used to create title slide.\nLevel 2 header (##) - Used to create headings.\nHorizontal rules (---) - Used when you don’t want to add a heading or title.\nNote: ### will create a subheading in the slide.\n\n\n# Title 1\n\nHello, World!\n\n## Slide Heading 1\n\n### Subjeading 1\n\nHello, World!\n\n--- # Makes slide without title/heading\n\nHello, World!\n\n\n\n3.4.5 Code\n\n3.4.5.1 auto-animate=true\nThis setting will allow smooth transitions across similar slides. You use this when you want to show gradual changes between slides.\nFor example, let’s say you have a block of code and then add another block, we can show the changes by using this. The first slide should only have part of the code, and the second should have the full code.\nSlide 1\n\n\n## Smooth Transition Slide 1 {auto-animate=true}\n\nfrom math import sqrt\n\nSlide 2\n\n\n## Smooth Transition Slide 2 {auto-animate=true}\n\nfrom math import sqrt\n\ndef pythagorean(a,b):\n    c2 = a**2 + b**2\n    c = sqrt(c2)\n    return c\n\n\n\n3.4.5.2 Highlighting Code\nTo highlight code, you can use the code-line-numbers attribute and use \"[start line #]-[end line #]\".\nFor example, in this slide let’s say I wanted to highlight the addition and subtraction function. I do this by putting {.python code-line-numbers=“2-6”} next to my 3 back ticks that start the code fence.\n#| echo: true\n\ndef addition(x,y):\n    return x + y\n\ndef subtraction(x,y):\n    return x - y\n\ndef multiplication(x,y):\n    return x * y\nIf the lines you want to highlight are separated by another line, you can use a comma.\nIn this slide I wanted to highlight return for each function, so I used {.python code-line-numbers=“3,6,9”}\n#| echo: true\n\ndef addition(x,y):\n    return x + y\n\ndef subtraction(x,y):\n    return x - y\n\ndef multiplication(x,y):\n    return x * y\n\n\n3.4.5.3 echo\nBy default, the code block does not echo the source code. By this I mean it does not show the source code and only shows the output by default.\nIn order to show the source code, we set the setting to #| echo: true. We add this part inside of the code fence at the top.\nEx: echo: false\n\n\n\n\n\n\n\n\n\nEx: echo: true\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx = np.linspace(0, 10, 100)\ny = np.sin(x)\n\nplt.figure(figsize=(12, 6))  # Width=12, Height=6\nplt.plot(x, y, label='sin(x)')\nplt.title('Sine Function')\nplt.xlabel('x')\nplt.ylabel('sin(x)')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n3.4.5.4 Figures\n\nPython figure sizes (MatPlotLib and PlotlyExpress) are automatically set to fill the slide area below the title\nR (Knitr figure) have similar default figure width and length sizes\nYou will most likely have to change the size of these figures or change the output location\n\n\n\n3.4.5.5 output-location\nThe output-location option can modify where the output of the code goes. There are several options to choose from such as:\n\ncolumn - Displays the output in a column next to the code.\ncolumn-fragment - Displays the output in a column next to the code and delays showing it until you advance.\nfragment - Displays the output as a Fragment. It delays showing it until you advance.\nslide - Displays the output on the slide after.\n\nEx: This uses column.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx = np.linspace(0, 10, 100)\ny = np.sin(x)\n\nplt.figure(figsize=(12, 6))\nplt.plot(x, y, label='sin(x)')\nplt.title('Sine Function')\nplt.xlabel('x')\nplt.ylabel('sin(x)')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n3.4.6 Interactive Code - HTML Gagdets\n\nHTML Gadget allows you to embed interactive elements into the HTML document.\nEnhance user engagement and data exploration.\nSeamless integration with R and Python in Quarto.\nVisualize complex data interactively.\nSome examples of these are:\n\nInteractive maps (e.g., Google Maps, Leaflet)\nData tables with sorting and searching\nDynamic plots with tooltips and zooming\nShiny apps for data exploration\n\n\n\n3.4.6.1 HTML Gagdget Tools\n\nLeaflet - Used to make interactive maps which allows you to display location based data.\nPlotly - Used for creating interactive plots.\nShiny - Used to make interactive statistical model that allows you to tweak parameters and see impacts.\nDT - Used for creating searchable data tables.\n\n\n\n\n\n\n\n\n\n\n\nTool\nLeaflet\nPlotly\nShiny\nDT\n\n\n\n\nPurpose\nInteractive maps\nInteractive plots\nBuild web apps\nInteractive tables\n\n\nKey Features\n- Markers, popups, and layers- Custom styles- Zoom and pan- Various tile providers\n- Scatter, line, and bar charts- Tooltips, hover effects, zoom\n- Dynamic with sliders and inputs- Real-time updates- Widgets for interactivity\n- Sorting, filtering, and pages- Customizable styling- Searchable columns\n\n\nUse Cases\n- Visualizing locations and routes\n- Analyzing data trends and relationships\n- Dashboards, reports, and data exploration\n- Displaying and exploring large datasets\n\n\n\n\n\n3.4.6.2 How to Get Started\n\nInstall the tools necessary using install.packages(c(\"plotly\", \"shiny\", \"DT\", \"leaflet\"), repos = \"https://cloud.r-project.org/\").\nThe code above should also allow you to avoid the CRAN error.\nNote: Shiny app should be run in a separate R session or browser window.\n\n\n\n3.4.6.3 Map of UCONN (Leaflet)\n\n## Install these R packages if you don't have them already\n## options(repos = c(CRAN = \"https://cloud.r-project.org/\"))\n## install.packages(c(\"leaflet\", \"plotly\", \"shiny\", \"DT\",\n##                             \"htmlwidgets\", \"webshot\", \"knitr\"))\n\nlibrary(leaflet)\n\n# Create the interactive leaflet map\nmap &lt;- leaflet() %&gt;%\n  addTiles() %&gt;%\n  setView(lng = -72.2565, lat = 41.8084, zoom = 15) %&gt;%\n  addMarkers(lng = -72.2565, lat = 41.8084, popup = \"UConn Main Campus\")\n\n## Ensure the 'images' folder exists\nif (!dir.exists(\"images\")) {\n    dir.create(\"images\")\n}\n\n# Conditional rendering based on output format\nif (knitr::is_html_output()) {\n  map  # Show interactive map in HTML\n} else {\n  # Define file paths\n  html_file &lt;- \"images/leaflet_temp.html\"\n  png_file &lt;- \"images/uconn_static.png\"\n\n  # Save the leaflet map as an HTML file\n  htmlwidgets::saveWidget(map, html_file, selfcontained = TRUE)\n\n  # Take a screenshot of the HTML file as a PNG image\n  webshot::webshot(html_file, file = png_file, delay = 2, vwidth = 800, vheight = 600)\n\n  # Include the PNG in the PDF output\n  knitr::include_graphics(png_file)\n}\n\n\n\n\n\n\n\n3.4.6.4 Interactive Plot (PlotLy)\n\nlibrary(plotly)\n\nfig &lt;- plot_ly(mtcars, x = ~mpg, y = ~hp, type = 'scatter', mode = 'markers', \n               marker = list(size = 10, color = ~cyl, colorscale = 'Blue'))\nfig &lt;- fig %&gt;% layout(title = \"Interactive Scatter Plot of MPG vs HP\",\n                      xaxis = list(title = \"Miles Per Gallon\"),\n                      yaxis = list(title = \"Horsepower\"))\n\n# Conditional rendering based on output format\nif (knitr::is_html_output()) {\n  fig  # Show interactive plot in HTML\n} else {\n  # Define file paths\n  html_file &lt;- \"images/plotly_temp.html\"\n  png_file &lt;- \"images/mtcars_static.png\"\n\n  # Save the plotly chart as an HTML file\n  htmlwidgets::saveWidget(fig, html_file, selfcontained = TRUE)\n\n  # Take a screenshot of the HTML file as a PNG image\n  webshot::webshot(html_file, file = png_file, delay = 2, vwidth = 800, vheight = 600)\n\n  knitr::include_graphics(png_file)\n}\n\n\n\n\n\n\n\n\n3.4.7 Rendering Your Presentation\nIn your terminal enter the following code:\n\nquarto render &lt;presentation_name&gt;.qmd --to revealjs\n\nThis will produce a HTML slideshow output that you can present.\n\n\n3.4.8 Keyboard Shortcuts\n\nS - Brings you to speaker view\nF - Fullscreen\n→,SPACE,N - Next slide\n←,P - Previous slide\nAlt →, Alt ← - Navigates without Fragments\nShift →, Shift ← - Navigates to first or last slide\n\n\n\n3.4.9 More Information/Resources\n\nRevealjs\nPowerPoint\nBeamer\nShiny\nPlotLy\nPlotly_Interactive\nDT\nLeaflet\n\n\n\n\n\nCone, M. (2025). Markdown cheat sheet | markdown guide. https://www.markdownguide.org/cheat-sheet/\n\n\nDervieux, C. (2025). Markdown-basics. https://quarto.org/docs/authoring/markdown-basics.html\n\n\nMacFarlane, J. (2006). Pandoc user’s guide. https://pandoc.org/MANUAL.html#pandocs-markdown\n\n\nMacFarlane, J. (2019). GitHub flavored markdown spec. https://github.github.com/gfm/",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reproducible Data Science</span>"
    ]
  },
  {
    "objectID": "quarto.html#footnotes",
    "href": "quarto.html#footnotes",
    "title": "3  Reproducible Data Science",
    "section": "",
    "text": "This is a notes.↩︎\nThis is a notes with multiple paragraphs. You can include paragraphs in your footnotes using indentation like this.\n    print(This is a fenced code block.)\n\n    You can add any thing inside this fenced code block.\nThis is the end of this footnote.↩︎",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reproducible Data Science</span>"
    ]
  },
  {
    "objectID": "python.html",
    "href": "python.html",
    "title": "4  Python Refreshment",
    "section": "",
    "text": "4.1 The Python World\nYou have programmed in Python. Regardless of your skill level, let us do some refreshing.\nSee, for example, how to build a Python libratry.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "python.html#the-python-world",
    "href": "python.html#the-python-world",
    "title": "4  Python Refreshment",
    "section": "",
    "text": "Function: a block of organized, reusable code to complete certain task.\nModule: a file containing a collection of functions, variables, and statements.\nPackage: a structured directory containing collections of modules and an __init.py__ file by which the directory is interpreted as a package.\nLibrary: a collection of related functionality of codes. It is a reusable chunk of code that we can use by importing it in our program, we can just use it by importing that library and calling the method of that library with period(.).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "python.html#standard-library",
    "href": "python.html#standard-library",
    "title": "4  Python Refreshment",
    "section": "4.2 Standard Library",
    "text": "4.2 Standard Library\nPython’s has an extensive standard library that offers a wide range of facilities as indicated by the long table of contents listed below. See documentation online.\n\nThe library contains built-in modules (written in C) that provide access to system functionality such as file I/O that would otherwise be inaccessible to Python programmers, as well as modules written in Python that provide standardized solutions for many problems that occur in everyday programming. Some of these modules are explicitly designed to encourage and enhance the portability of Python programs by abstracting away platform-specifics into platform-neutral APIs.\n\nQuestion: How to get the constant \\(e\\) to an arbitary precision?\nThe constant is only represented by a given double precision.\n\nimport math\nprint(\"%0.20f\" % math.e)\nprint(\"%0.80f\" % math.e)\n\n2.71828182845904509080\n2.71828182845904509079559829842764884233474731445312500000000000000000000000000000\n\n\nNow use package decimal to export with an arbitary precision.\n\nimport decimal  # for what?\n\n## set the required number digits to 150\ndecimal.getcontext().prec = 150\ndecimal.Decimal(1).exp().to_eng_string()\ndecimal.Decimal(1).exp().to_eng_string()[2:]\n\n'71828182845904523536028747135266249775724709369995957496696762772407663035354759457138217852516642742746639193200305992181741359662904357290033429526'",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "python.html#important-libraries",
    "href": "python.html#important-libraries",
    "title": "4  Python Refreshment",
    "section": "4.3 Important Libraries",
    "text": "4.3 Important Libraries\n\nNumPy\npandas\nmatplotlib\nIPython/Jupyter\nSciPy\nscikit-learn\nstatsmodels\n\nQuestion: how to draw a random sample from a normal distribution and evaluate the density and distributions at these points?\n\nfrom scipy.stats import norm\n\nmu, sigma = 2, 4\nmean, var, skew, kurt = norm.stats(mu, sigma, moments='mvsk')\nprint(mean, var, skew, kurt)\nx = norm.rvs(loc = mu, scale = sigma, size = 10)\nx\n\n2.0 16.0 0.0 0.0\n\n\narray([-7.61542223,  4.04410857, -0.66018039, -3.22051506,  3.46732088,\n        8.19518069, -0.249417  , -1.29855302,  4.58570583,  4.19481766])\n\n\nThe pdf and cdf can be evaluated:\n\nnorm.pdf(x, loc = mu, scale = sigma)\n\narray([0.00554702, 0.08752706, 0.07994828, 0.04255689, 0.09324591,\n       0.03005831, 0.08514888, 0.07098782, 0.08093034, 0.08579701])",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "python.html#writing-a-function",
    "href": "python.html#writing-a-function",
    "title": "4  Python Refreshment",
    "section": "4.4 Writing a Function",
    "text": "4.4 Writing a Function\nConsider the Fibonacci Sequence \\(1, 1, 2, 3, 5, 8, 13, 21, 34, ...\\). The next number is found by adding up the two numbers before it. We are going to use 3 ways to solve the problems.\nThe first is a recursive solution.\n\ndef fib_rs(n):\n    if (n==1 or n==2):\n        return 1\n    else:\n        return fib_rs(n - 1) + fib_rs(n - 2)\n\n%timeit fib_rs(10)\n\n8.83 μs ± 51 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n\n\nThe second uses dynamic programming memoization.\n\ndef fib_dm_helper(n, mem):\n    if mem[n] is not None:\n        return mem[n]\n    elif (n == 1 or n == 2):\n        result = 1\n    else:\n        result = fib_dm_helper(n - 1, mem) + fib_dm_helper(n - 2, mem)\n    mem[n] = result\n    return result\n\ndef fib_dm(n):\n    mem = [None] * (n + 1)\n    return fib_dm_helper(n, mem)\n\n%timeit fib_dm(10)\n\n1.93 μs ± 9.57 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)\n\n\nThe third is still dynamic programming but bottom-up.\n\ndef fib_dbu(n):\n    mem = [None] * (n + 1)\n    mem[1] = 1;\n    mem[2] = 1;\n    for i in range(3, n + 1):\n        mem[i] = mem[i - 1] + mem[i - 2]\n    return mem[n]\n\n\n%timeit fib_dbu(500)\n\n66.4 μs ± 918 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n\n\nApparently, the three solutions have very different performance for larger n.\n\n4.4.1 Monty Hall\nHere is a function that performs the Monty Hall experiments. In this version, the host opens only one empty door.\n\nimport numpy as np\n\ndef montyhall(ndoors, ntrials):\n    doors = np.arange(1, ndoors + 1) / 10\n    prize = np.random.choice(doors, size=ntrials)\n    player = np.random.choice(doors, size=ntrials)\n    host = np.array([np.random.choice([d for d in doors\n                                       if d not in [player[x], prize[x]]])\n                     for x in range(ntrials)])\n    player2 = np.array([np.random.choice([d for d in doors\n                                          if d not in [player[x], host[x]]])\n                        for x in range(ntrials)])\n    return {'noswitch': np.sum(prize == player), 'switch': np.sum(prize == player2)}\n\nTest it out:\n\nmontyhall(3, 1000)\nmontyhall(4, 1000)\n\n{'noswitch': np.int64(242), 'switch': np.int64(381)}\n\n\nThe true value for the two strategies with \\(n\\) doors are, respectively, \\(1 / n\\) and \\(\\frac{n - 1}{n (n - 2)}\\).\nIn the homework exercise, the host opens \\(m\\) doors that are empty. An argument nempty could be added to the function.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "python.html#variables-versus-objects",
    "href": "python.html#variables-versus-objects",
    "title": "4  Python Refreshment",
    "section": "4.5 Variables versus Objects",
    "text": "4.5 Variables versus Objects\nIn Python, variables and the objects they point to actually live in two different places in the computer memory. Think of variables as pointers to the objects they’re associated with, rather than being those objects. This matters when multiple variables point to the same object.\n\nx = [1, 2, 3]  # create a list; x points to the list\ny = x          # y also points to the same list in the memory\ny.append(4)    # append to y\nx              # x changed!\n\n[1, 2, 3, 4]\n\n\nNow check their addresses\n\nprint(id(x))   # address of x\nprint(id(y))   # address of y\n\n4696292544\n4696292544\n\n\nNonetheless, some data types in Python are “immutable”, meaning that their values cannot be changed in place. One such example is strings.\n\nx = \"abc\"\ny = x\ny = \"xyz\"\nx\n\n'abc'\n\n\nNow check their addresses\n\nprint(id(x))   # address of x\nprint(id(y))   # address of y\n\n4526902032\n4610332432\n\n\nQuestion: What’s mutable and what’s immutable?\nAnything that is a collection of other objects is mutable, except tuples.\nNot all manipulations of mutable objects change the object rather than create a new object. Sometimes when you do something to a mutable object, you get back a new object. Manipulations that change an existing object, rather than create a new one, are referred to as “in-place mutations” or just “mutations.” So:\n\nAll manipulations of immutable types create new objects.\nSome manipulations of mutable types create new objects.\n\nDifferent variables may all be pointing at the same object is preserved through function calls (a behavior known as “pass by object-reference”). So if you pass a list to a function, and that function manipulates that list using an in-place mutation, that change will affect any variable that was pointing to that same object outside the function.\n\nx = [1, 2, 3]\ny = x\n\ndef append_42(input_list):\n    input_list.append(42)\n    return input_list\n\nappend_42(x)\n\n[1, 2, 3, 42]\n\n\nNote that both x and y have been appended by \\(42\\).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "python.html#number-representation",
    "href": "python.html#number-representation",
    "title": "4  Python Refreshment",
    "section": "4.6 Number Representation",
    "text": "4.6 Number Representation\nNumers in a computer’s memory are represented by binary styles (on and off of bits).\n\n4.6.1 Integers\nIf not careful, It is easy to be bitten by overflow with integers when using Numpy and Pandas in Python.\n\nimport numpy as np\n\nx = np.array(2 ** 63 - 1 , dtype = 'int')\nx\n# This should be the largest number numpy can display, with\n# the default int8 type (64 bits)\n\narray(9223372036854775807)\n\n\nNote: on Windows and other platforms, dtype = 'int' may have to be changed to dtype = np.int64 for the code to execute. Source: Stackoverflow\nWhat if we increment it by 1?\n\ny = np.array(x + 1, dtype = 'int')\ny\n# Because of the overflow, it becomes negative!\n\narray(-9223372036854775808)\n\n\nFor vanilla Python, the overflow errors are checked and more digits are allocated when needed, at the cost of being slow.\n\n2 ** 63 * 1000\n\n9223372036854775808000\n\n\nThis number is 1000 times larger than the prior number, but still displayed perfectly without any overflows\n\n\n4.6.2 Floating Number\nStandard double-precision floating point number uses 64 bits. Among them, 1 is for sign, 11 is for exponent, and 52 are fraction significand, See https://en.wikipedia.org/wiki/Double-precision_floating-point_format. The bottom line is that, of course, not every real number is exactly representable.\nIf you have played the Game 24, here is a tricky one:\n\n8 / (3 - 8 / 3) == 24\n\nFalse\n\n\nSurprise?\nThere are more.\n\n0.1 + 0.1 + 0.1 == 0.3\n\nFalse\n\n\n\n0.3 - 0.2 == 0.1\n\nFalse\n\n\nWhat is really going on?\n\nimport decimal\ndecimal.Decimal(0.1)\n\nDecimal('0.1000000000000000055511151231257827021181583404541015625')\n\n\n\ndecimal.Decimal(8 / (3 - 8 / 3))\n\nDecimal('23.999999999999989341858963598497211933135986328125')\n\n\nBecause the mantissa bits are limited, it can not represent a floating point that’s both very big and very precise. Most computers can represent all integers up to \\(2^{53}\\), after that it starts skipping numbers.\n\n2.1 ** 53 + 1 == 2.1 ** 53\n\n# Find a number larger than 2 to the 53rd\n\nTrue\n\n\n\nx = 2.1 ** 53\nfor i in range(1000000):\n    x = x + 1\nx == 2.1 ** 53\n\nTrue\n\n\nWe add 1 to x by 1000000 times, but it still equal to its initial value, 2.1 ** 53. This is because this number is too big that computer can’t handle it with precision like add 1.\nMachine epsilon is the smallest positive floating-point number x such that 1 + x != 1.\n\nprint(np.finfo(float).eps)\nprint(np.finfo(np.float32).eps)\n\n2.220446049250313e-16\n1.1920929e-07",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "python.html#sec-python-venv",
    "href": "python.html#sec-python-venv",
    "title": "4  Python Refreshment",
    "section": "4.7 Virtual Environment",
    "text": "4.7 Virtual Environment\nVirtual environments in Python are essential tools for managing dependencies and ensuring consistency across projects. They allow you to create isolated environments for each project, with its own set of installed packages, separate from the global Python installation. This isolation prevents conflicts between project dependencies and versions, making your projects more reliable and easier to manage. It’s particularly useful when working on multiple projects with differing requirements, or when collaborating with others who may have different setups.\nTo set up a virtual environment, you first need to ensure that Python is installed on your system. Most modern Python installations come with the venv module, which is used to create virtual environments. Here’s how to set one up:\n\nOpen your command line interface.\nNavigate to your project directory.\nRun python3 -m venv myenv, where myenv is the name of the virtual environment to be created. Choose an informative name.\n\nThis command creates a new directory named myenv (or your chosen name) in your project directory, containing the virtual environment.\nTo start using this environment, you need to activate it. The activation command varies depending on your operating system:\n\nOn Windows, run myenv\\Scripts\\activate.\nOn Linux or MacOS, use source myenv/bin/activate or . myenv/bin/activate.\n\nOnce activated, your command line will typically show the name of the virtual environment, and you can then install and use packages within this isolated environment without affecting your global Python setup.\nTo exit the virtual environment, simply type deactivate in your command line. This will return you to your system’s global Python environment.\nAs an example, let’s install a package, like numpy, in this newly created virtual environment:\n\nEnsure your virtual environment is activated.\nRun pip install numpy.\n\nThis command installs the requests library in your virtual environment. You can verify the installation by running pip list, which should show requests along with its version.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "python.html#numpy",
    "href": "python.html#numpy",
    "title": "4  Python Refreshment",
    "section": "4.8 Numpy",
    "text": "4.8 Numpy",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "manipulation.html",
    "href": "manipulation.html",
    "title": "5  Data Manipulation",
    "section": "",
    "text": "5.1 Introduction\nData manipulation is crucial for transforming raw data into a more analyzable format, essential for uncovering patterns and ensuring accurate analysis. This chapter introduces the core techniques for data manipulation in Python, utilizing the Pandas library, a cornerstone for data handling within Python’s data science toolkit.\nPython’s ecosystem is rich with libraries that facilitate not just data manipulation but comprehensive data analysis. Pandas, in particular, provides extensive functionality for data manipulation tasks including reading, cleaning, transforming, and summarizing data. Using real-world datasets, we will explore how to leverage Python for practical data manipulation tasks.\nBy the end of this chapter, you will learn to:",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Manipulation</span>"
    ]
  },
  {
    "objectID": "manipulation.html#introduction",
    "href": "manipulation.html#introduction",
    "title": "5  Data Manipulation",
    "section": "",
    "text": "Import/export data from/to diverse sources.\nClean and preprocess data efficiently.\nTransform and aggregate data to derive insights.\nMerge and concatenate datasets from various origins.\nAnalyze real-world datasets using these techniques.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Manipulation</span>"
    ]
  },
  {
    "objectID": "manipulation.html#data-manipulation-with-pandas",
    "href": "manipulation.html#data-manipulation-with-pandas",
    "title": "5  Data Manipulation",
    "section": "5.2 Data Manipulation with Pandas",
    "text": "5.2 Data Manipulation with Pandas\nThis section is prepared by Lang Lang. I am a senior student double majoring in\ndata science and economics in University of Connecticut.\n\n5.2.1 Introduction\nIn this section, I will introduce about data manipulation using Pandas,\nwhich is a powerful Python library for working with data. I’ll walk through\nsome basic operations like filtering, merging, and summarizing data using a real\ndata set of NYC motor vehicle collisions.\nPandas is a powerful Python library for data manipulation and analysis. It\nprovides two key data structures:\n\nSeries: A one-dimensional labeled array.\nData Frame: A two-dimensional labeled table with rows and columns.\n\n\n5.2.1.1 Why Use Pandas?\n\nEfficiently handles large data sets.\nProvides flexible data manipulation functions.\nWorks well with NumPy and visualization libraries like Matplotlib.\n\n\n\n\n5.2.2 Loading Data\n\n5.2.2.1 Reading NYC Crash Data\nWe’ll work with the NYC Motor Vehicle Collisions data set in the class notes\nrepository.\n\nUsing the following code to import the Pandas library in Python\n\n\nimport pandas as pd\n\n\nUsing the following code to load the data set\n\n\ndf = pd.read_csv(\"data/nyccrashes_2024w0630_by20250212.csv\")\n\n\n\n5.2.2.2 Renaming The Columns\nUsing the following codes to rename the columns.\n\ndf.columns = [col.strip().lower().replace(\" \", \"_\") for col in df.columns]\ndf.columns\n\nIndex(['crash_date', 'crash_time', 'borough', 'zip_code', 'latitude',\n       'longitude', 'location', 'on_street_name', 'cross_street_name',\n       'off_street_name', 'number_of_persons_injured',\n       'number_of_persons_killed', 'number_of_pedestrians_injured',\n       'number_of_pedestrians_killed', 'number_of_cyclist_injured',\n       'number_of_cyclist_killed', 'number_of_motorist_injured',\n       'number_of_motorist_killed', 'contributing_factor_vehicle_1',\n       'contributing_factor_vehicle_2', 'contributing_factor_vehicle_3',\n       'contributing_factor_vehicle_4', 'contributing_factor_vehicle_5',\n       'collision_id', 'vehicle_type_code_1', 'vehicle_type_code_2',\n       'vehicle_type_code_3', 'vehicle_type_code_4', 'vehicle_type_code_5'],\n      dtype='object')\n\n\n\n\n5.2.2.3 Viewing the First Few Rows\nThe head function in Pandas is used to display the first few rows of a DataFrame.\n\ndf.head() # Default value of n is 5\n\n\n\n\n\n\n\n\ncrash_date\ncrash_time\nborough\nzip_code\nlatitude\nlongitude\nlocation\non_street_name\ncross_street_name\noff_street_name\n...\ncontributing_factor_vehicle_2\ncontributing_factor_vehicle_3\ncontributing_factor_vehicle_4\ncontributing_factor_vehicle_5\ncollision_id\nvehicle_type_code_1\nvehicle_type_code_2\nvehicle_type_code_3\nvehicle_type_code_4\nvehicle_type_code_5\n\n\n\n\n0\n06/30/2024\n23:17\nBRONX\n10460.0\n40.838844\n-73.87817\n(40.838844, -73.87817)\nEAST 177 STREET\nDEVOE AVENUE\nNaN\n...\nUnspecified\nNaN\nNaN\nNaN\n4737486\nSedan\nPick-up Truck\nNaN\nNaN\nNaN\n\n\n1\n06/30/2024\n8:30\nBRONX\n10468.0\n40.862732\n-73.90333\n(40.862732, -73.90333)\nWEST FORDHAM ROAD\nGRAND AVENUE\nNaN\n...\nUnspecified\nNaN\nNaN\nNaN\n4737502\nSedan\nNaN\nNaN\nNaN\nNaN\n\n\n2\n06/30/2024\n20:47\nNaN\nNaN\n40.763630\n-73.95330\n(40.76363, -73.9533)\nFDR DRIVE\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\n4737510\nSedan\nNaN\nNaN\nNaN\nNaN\n\n\n3\n06/30/2024\n13:10\nBROOKLYN\n11234.0\n40.617030\n-73.91989\n(40.61703, -73.91989)\nEAST 57 STREET\nAVENUE O\nNaN\n...\nDriver Inattention/Distraction\nNaN\nNaN\nNaN\n4737499\nSedan\nSedan\nNaN\nNaN\nNaN\n\n\n4\n06/30/2024\n16:42\nNaN\nNaN\nNaN\nNaN\nNaN\n33 STREET\nASTORIA BOULEVARD\nNaN\n...\nUnspecified\nNaN\nNaN\nNaN\n4736925\nSedan\nStation Wagon/Sport Utility Vehicle\nNaN\nNaN\nNaN\n\n\n\n\n5 rows × 29 columns\n\n\n\n\n\n5.2.2.4 Checking Dataset Structure\nThe info function in Pandas provides a summary of a DataFrame, including:\n\nNumber of rows and columns\nColumn names and data types\nNumber of non-null values per column\nMemory usage\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1876 entries, 0 to 1875\nData columns (total 29 columns):\n #   Column                         Non-Null Count  Dtype  \n---  ------                         --------------  -----  \n 0   crash_date                     1876 non-null   object \n 1   crash_time                     1876 non-null   object \n 2   borough                        1334 non-null   object \n 3   zip_code                       1334 non-null   float64\n 4   latitude                       1745 non-null   float64\n 5   longitude                      1745 non-null   float64\n 6   location                       1745 non-null   object \n 7   on_street_name                 1330 non-null   object \n 8   cross_street_name              944 non-null    object \n 9   off_street_name                546 non-null    object \n 10  number_of_persons_injured      1876 non-null   int64  \n 11  number_of_persons_killed       1876 non-null   int64  \n 12  number_of_pedestrians_injured  1876 non-null   int64  \n 13  number_of_pedestrians_killed   1876 non-null   int64  \n 14  number_of_cyclist_injured      1876 non-null   int64  \n 15  number_of_cyclist_killed       1876 non-null   int64  \n 16  number_of_motorist_injured     1876 non-null   int64  \n 17  number_of_motorist_killed      1876 non-null   int64  \n 18  contributing_factor_vehicle_1  1865 non-null   object \n 19  contributing_factor_vehicle_2  1426 non-null   object \n 20  contributing_factor_vehicle_3  174 non-null    object \n 21  contributing_factor_vehicle_4  52 non-null     object \n 22  contributing_factor_vehicle_5  14 non-null     object \n 23  collision_id                   1876 non-null   int64  \n 24  vehicle_type_code_1            1843 non-null   object \n 25  vehicle_type_code_2            1231 non-null   object \n 26  vehicle_type_code_3            162 non-null    object \n 27  vehicle_type_code_4            48 non-null     object \n 28  vehicle_type_code_5            14 non-null     object \ndtypes: float64(3), int64(9), object(17)\nmemory usage: 425.2+ KB\n\n\nThis tells us:\n\nThe dataset has 1876 rows and 29 columns.\nData types include float64(3), int64(9), object(17).\nThere are no missing values in any column.\nThe dataset consumes 425.2+ KB of memory.\n\n\n\n5.2.2.5 Descriptive Statistics\nThe discribe function provides summary statistics for numerical columns in a\nPandas DataFrame.\n\ndf.describe()\n\n\n\n\n\n\n\n\nzip_code\nlatitude\nlongitude\nnumber_of_persons_injured\nnumber_of_persons_killed\nnumber_of_pedestrians_injured\nnumber_of_pedestrians_killed\nnumber_of_cyclist_injured\nnumber_of_cyclist_killed\nnumber_of_motorist_injured\nnumber_of_motorist_killed\ncollision_id\n\n\n\n\ncount\n1334.000000\n1745.000000\n1745.000000\n1876.000000\n1876.000000\n1876.000000\n1876.000000\n1876.000000\n1876.0\n1876.000000\n1876.000000\n1.876000e+03\n\n\nmean\n10902.185157\n40.649264\n-73.792754\n0.616738\n0.004264\n0.093284\n0.002665\n0.065032\n0.0\n0.432836\n0.001599\n4.738602e+06\n\n\nstd\n526.984169\n1.689337\n3.064378\n0.915477\n0.103191\n0.338369\n0.095182\n0.246648\n0.0\n0.891003\n0.039968\n1.772834e+03\n\n\nmin\n10001.000000\n0.000000\n-74.237366\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.0\n0.000000\n0.000000\n4.736561e+06\n\n\n25%\n10457.000000\n40.661804\n-73.968540\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.0\n0.000000\n0.000000\n4.737667e+06\n\n\n50%\n11208.500000\n40.712696\n-73.922960\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.0\n0.000000\n0.000000\n4.738258e+06\n\n\n75%\n11239.000000\n40.767690\n-73.869180\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.0\n1.000000\n0.000000\n4.738886e+06\n\n\nmax\n11694.000000\n40.907246\n0.000000\n11.000000\n4.000000\n7.000000\n4.000000\n1.000000\n0.0\n11.000000\n1.000000\n4.765601e+06\n\n\n\n\n\n\n\nThis provides insights such as:\n\nTotal count of entries\nMean, min, and max values\nStandard deviation\n\n\n\n\n5.2.3 Selecting and Filtering Data\n\n5.2.3.1 Selecting Specific Columns\nSometimes, we only need certain columns.\nIn this case, you can use select function.\n\ndf_selected = df[['crash_date', 'crash_time', 'borough', \n                  'number_of_persons_injured']]\ndf_selected.head() \n\n\n\n\n\n\n\n\ncrash_date\ncrash_time\nborough\nnumber_of_persons_injured\n\n\n\n\n0\n06/30/2024\n23:17\nBRONX\n0\n\n\n1\n06/30/2024\n8:30\nBRONX\n0\n\n\n2\n06/30/2024\n20:47\nNaN\n0\n\n\n3\n06/30/2024\n13:10\nBROOKLYN\n1\n\n\n4\n06/30/2024\n16:42\nNaN\n1\n\n\n\n\n\n\n\n\n\n5.2.3.2 Filtering Data\nwhen you would like to filter certain specific data (e.g., Crashes in 2024-06-30),\nyou can using the following data to define:\n\n# Convert crash date to datetime format\ndf['crash_date'] = pd.to_datetime(df['crash_date'])\n\njune30_df = df[df['crash_date'] == '2024-06-30']\njune30_df.head()\n\n\n\n\n\n\n\n\ncrash_date\ncrash_time\nborough\nzip_code\nlatitude\nlongitude\nlocation\non_street_name\ncross_street_name\noff_street_name\n...\ncontributing_factor_vehicle_2\ncontributing_factor_vehicle_3\ncontributing_factor_vehicle_4\ncontributing_factor_vehicle_5\ncollision_id\nvehicle_type_code_1\nvehicle_type_code_2\nvehicle_type_code_3\nvehicle_type_code_4\nvehicle_type_code_5\n\n\n\n\n0\n2024-06-30\n23:17\nBRONX\n10460.0\n40.838844\n-73.87817\n(40.838844, -73.87817)\nEAST 177 STREET\nDEVOE AVENUE\nNaN\n...\nUnspecified\nNaN\nNaN\nNaN\n4737486\nSedan\nPick-up Truck\nNaN\nNaN\nNaN\n\n\n1\n2024-06-30\n8:30\nBRONX\n10468.0\n40.862732\n-73.90333\n(40.862732, -73.90333)\nWEST FORDHAM ROAD\nGRAND AVENUE\nNaN\n...\nUnspecified\nNaN\nNaN\nNaN\n4737502\nSedan\nNaN\nNaN\nNaN\nNaN\n\n\n2\n2024-06-30\n20:47\nNaN\nNaN\n40.763630\n-73.95330\n(40.76363, -73.9533)\nFDR DRIVE\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\n4737510\nSedan\nNaN\nNaN\nNaN\nNaN\n\n\n3\n2024-06-30\n13:10\nBROOKLYN\n11234.0\n40.617030\n-73.91989\n(40.61703, -73.91989)\nEAST 57 STREET\nAVENUE O\nNaN\n...\nDriver Inattention/Distraction\nNaN\nNaN\nNaN\n4737499\nSedan\nSedan\nNaN\nNaN\nNaN\n\n\n4\n2024-06-30\n16:42\nNaN\nNaN\nNaN\nNaN\nNaN\n33 STREET\nASTORIA BOULEVARD\nNaN\n...\nUnspecified\nNaN\nNaN\nNaN\n4736925\nSedan\nStation Wagon/Sport Utility Vehicle\nNaN\nNaN\nNaN\n\n\n\n\n5 rows × 29 columns\n\n\n\n\n\n5.2.3.3 Filter A DataFrame\nThe query function in Pandas is used to filter a DataFrame using a more\nreadable, SQL-like syntax. For example, now I would like to find the crashes with\nthe number of persons injured more than 2. We can using the following code:\n\ndf_filtered = df.query(\"`number_of_persons_injured` &gt; 2\")\ndf_filtered.head()\n\n\n\n\n\n\n\n\ncrash_date\ncrash_time\nborough\nzip_code\nlatitude\nlongitude\nlocation\non_street_name\ncross_street_name\noff_street_name\n...\ncontributing_factor_vehicle_2\ncontributing_factor_vehicle_3\ncontributing_factor_vehicle_4\ncontributing_factor_vehicle_5\ncollision_id\nvehicle_type_code_1\nvehicle_type_code_2\nvehicle_type_code_3\nvehicle_type_code_4\nvehicle_type_code_5\n\n\n\n\n20\n2024-06-30\n13:05\nNaN\nNaN\n40.797447\n-73.946710\n(40.797447, -73.94671)\nMADISON AVENUE\nNaN\nNaN\n...\nUnspecified\nNaN\nNaN\nNaN\n4736750\nSedan\nStation Wagon/Sport Utility Vehicle\nNaN\nNaN\nNaN\n\n\n33\n2024-06-30\n15:30\nBROOKLYN\n11229.0\n40.610653\n-73.953550\n(40.610653, -73.95355)\nKINGS HIGHWAY\nOCEAN AVENUE\nNaN\n...\nNaN\nNaN\nNaN\nNaN\n4737460\nSedan\nNaN\nNaN\nNaN\nNaN\n\n\n35\n2024-06-30\n16:28\nSTATEN ISLAND\n10304.0\n40.589180\n-74.098946\n(40.58918, -74.098946)\nJEFFERSON STREET\nLIBERTY AVENUE\nNaN\n...\nUnspecified\nNaN\nNaN\nNaN\n4736985\nSedan\nSedan\nNaN\nNaN\nNaN\n\n\n37\n2024-06-30\n20:29\nMANHATTAN\n10027.0\n40.807667\n-73.949290\n(40.807667, -73.94929)\n7 AVENUE\nWEST 123 STREET\nNaN\n...\nUnspecified\nNaN\nNaN\nNaN\n4737257\nStation Wagon/Sport Utility Vehicle\nSedan\nNaN\nNaN\nNaN\n\n\n45\n2024-06-30\n19:56\nBROOKLYN\n11237.0\n40.709003\n-73.922340\n(40.709003, -73.92234)\nSCOTT AVENUE\nFLUSHING AVENUE\nNaN\n...\nUnspecified\nUnspecified\nNaN\nNaN\n4736995\nSedan\nStation Wagon/Sport Utility Vehicle\nMoped\nNaN\nNaN\n\n\n\n\n5 rows × 29 columns\n\n\n\n\n\n\n5.2.4 Merging DataFrames\nIn Pandas, the merge function is used to combine multiple DataFrames based\non a common column.\nThis is useful when working with multiple datasets that need to be joined for\nanalysis.\n\n5.2.4.1 Example: Merging Crash Data with Total Injuries per Borough\nSuppose we have a dataset containing crash details, and we want to analyze how\nthe total number of injuries in each borough relates to individual crashes.\nWe can achieve this by aggregating the total injuries per borough and merging it\nwith the crash dataset.\nThe following code:\n\nAggregates the total injuries per borough using .groupby().\nSelects relevant columns from the main dataset (collision_id, borough,\nnumber_of_persons_injured).\nMerges the aggregated injury data with the crash dataset using merge function on\nthe borough column.\n\n\n# Aggregate total injuries per borough\ndf_borough_info = df.groupby(\n          'borough', as_index=False\n          )['number_of_persons_injured'].sum()\ndf_borough_info.rename(\n          columns={'number_of_persons_injured': 'total_injuries'}, \n                  inplace=True)\n\n# Select relevant columns from the main dataset\ndf_crashes = df[['collision_id', 'borough', 'number_of_persons_injured']]\n\n# Merge crash data with total injuries per borough\ndf_merged = pd.merge(df_crashes, df_borough_info, on='borough', how='left')\n\n# Display first few rows of the merged dataset\ndf_merged.head()\n\n\n\n\n\n\n\n\ncollision_id\nborough\nnumber_of_persons_injured\ntotal_injuries\n\n\n\n\n0\n4737486\nBRONX\n0\n126.0\n\n\n1\n4737502\nBRONX\n0\n126.0\n\n\n2\n4737510\nNaN\n0\nNaN\n\n\n3\n4737499\nBROOKLYN\n1\n295.0\n\n\n4\n4736925\nNaN\n1\nNaN\n\n\n\n\n\n\n\nThe merged dataset now includes:\n\ncollision_id: Unique identifier for each crash.\nborough: The borough where the crash occurred.\nnumber_of_persons_injured: Number of injuries in a specific crash.\ntotal_injuries: The total number of injuries reported in that borough.\n\nThis merged dataset allows us to compare individual crash injuries with the\noverall injury trend in each borough, helping in further data analysis and\nvisualization.\n\n\n\n5.2.5 Data Visualization\nVisualizing data is crucial to understanding patterns and relationships within\nthe dataset. In this section, we will create one-variable tables (frequency\ntable), two-variable tables (contingency table).\n\n5.2.5.1 One-Variable Table\nA one-variable table (also called a frequency table) shows the distribution of\nvalues for a single categorical variable.\nFor example, we can count the number of crashes per borough:\n\nborough_counts = df['borough'].value_counts()\nborough_counts\n\nborough\nBROOKLYN         462\nQUEENS           381\nMANHATTAN        228\nBRONX            213\nSTATEN ISLAND     50\nName: count, dtype: int64\n\n\nThis table displays the number of accidents recorded in each borough of NYC. It\nhelps identify which borough has the highest accident frequency.\n\n\n5.2.5.2 Two-Variable Table\nA two-variable table (also called a contingency table) shows the relationship\nbetween two categorical variables.\nFor example, we can analyze the number of crashes per borough per day:\n\n# make pivot table\nborough_day_table = df.pivot_table(\n    index='crash_date',\n    columns='borough',\n    values='collision_id',\n    aggfunc='count'\n)\nborough_day_table\n\n\n\n\n\n\n\nborough\nBRONX\nBROOKLYN\nMANHATTAN\nQUEENS\nSTATEN ISLAND\n\n\ncrash_date\n\n\n\n\n\n\n\n\n\n2024-06-30\n24\n69\n40\n40\n8\n\n\n2024-07-01\n27\n62\n35\n45\n3\n\n\n2024-07-02\n19\n53\n37\n54\n8\n\n\n2024-07-03\n33\n59\n25\n58\n3\n\n\n2024-07-04\n27\n47\n31\n44\n8\n\n\n2024-07-05\n32\n64\n28\n58\n9\n\n\n2024-07-06\n27\n62\n16\n37\n6\n\n\n2024-07-07\n24\n46\n16\n45\n5\n\n\n\n\n\n\n\nThis table shows the number of accidents per borough for each day in the dataset.\n\n\n\n5.2.6 Data Cleaning and Transformation\nThis part is from the textbook “Python for Data Analysis: Data Wrangling with\nPandas, NumPy, and IPython.” Chapter 5, Third Edition by Wes McK- inney, O’Reilly\nMedia, 2022. https://wesmckinney.com/book/.\n\n5.2.6.1 Changing Data Types\nThe following functions in Pandas are used to convert data types. This is\nimportant when working with dates and categorical data to ensure proper analysis.\nFor example, we want to:\n\n# Convert crash date to datetime format\ndf['crash_date'] = pd.to_datetime(df['crash_date'])\ndf['crash_date']\n  \n# Convert ZIP code to string\ndf['zip_code'] = df['zip_code'].astype(str)\ndf['zip_code']\n\n0       10460.0\n1       10468.0\n2           nan\n3       11234.0\n4           nan\n         ...   \n1871        nan\n1872        nan\n1873    10468.0\n1874        nan\n1875        nan\nName: zip_code, Length: 1876, dtype: object\n\n\n\n\n5.2.6.2 Sorting Data\nWe can sort data by one or more columns:\n\n# Sort by date and time\ndf_sorted = df.sort_values(\n    by=['crash_date', 'crash_time'], \n    ascending=[True, True]\n)\ndf_sorted.head()\n\n\n\n\n\n\n\n\ncrash_date\ncrash_time\nborough\nzip_code\nlatitude\nlongitude\nlocation\non_street_name\ncross_street_name\noff_street_name\n...\ncontributing_factor_vehicle_2\ncontributing_factor_vehicle_3\ncontributing_factor_vehicle_4\ncontributing_factor_vehicle_5\ncollision_id\nvehicle_type_code_1\nvehicle_type_code_2\nvehicle_type_code_3\nvehicle_type_code_4\nvehicle_type_code_5\n\n\n\n\n90\n2024-06-30\n0:00\nNaN\nnan\n40.638268\n-74.07880\n(40.638268, -74.0788)\nVICTORY BOULEVARD\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\n4737312\nPick-up Truck\nNaN\nNaN\nNaN\nNaN\n\n\n180\n2024-06-30\n0:00\nMANHATTAN\n10004.0\n40.704834\n-74.01658\n(40.704834, -74.01658)\nNaN\nNaN\n17 BATTERY PLACE\n...\nUnspecified\nNaN\nNaN\nNaN\n4737900\nTaxi\nSedan\nNaN\nNaN\nNaN\n\n\n188\n2024-06-30\n0:00\nBRONX\n10455.0\n40.815754\n-73.89529\n(40.815754, -73.89529)\nLONGWOOD AVENUE\nBRUCKNER BOULEVARD\nNaN\n...\nUnspecified\nNaN\nNaN\nNaN\n4737875\nSedan\nSedan\nNaN\nNaN\nNaN\n\n\n248\n2024-06-30\n0:04\nNaN\nnan\n40.712490\n-73.96854\n(40.71249, -73.96854)\nWILLIAMSBURG BRIDGE\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\n4737093\nBike\nNaN\nNaN\nNaN\nNaN\n\n\n137\n2024-06-30\n0:05\nNaN\nnan\n40.793980\n-73.97229\n(40.79398, -73.97229)\nBROADWAY\nNaN\nNaN\n...\nUnspecified\nNaN\nNaN\nNaN\n4736952\nSedan\nBike\nNaN\nNaN\nNaN\n\n\n\n\n5 rows × 29 columns\n\n\n\nNow you can see the data is sorted by crash time.\n\n\n5.2.6.3 Handling Missing Data\nPandas provides methods for handling missing values.\nFor example, you can using the following codes to fix the missing data.\n\n# Check for missing values\ndf.isna().sum()\n\ncrash_date                          0\ncrash_time                          0\nborough                           542\nzip_code                            0\nlatitude                          131\nlongitude                         131\nlocation                          131\non_street_name                    546\ncross_street_name                 932\noff_street_name                  1330\nnumber_of_persons_injured           0\nnumber_of_persons_killed            0\nnumber_of_pedestrians_injured       0\nnumber_of_pedestrians_killed        0\nnumber_of_cyclist_injured           0\nnumber_of_cyclist_killed            0\nnumber_of_motorist_injured          0\nnumber_of_motorist_killed           0\ncontributing_factor_vehicle_1      11\ncontributing_factor_vehicle_2     450\ncontributing_factor_vehicle_3    1702\ncontributing_factor_vehicle_4    1824\ncontributing_factor_vehicle_5    1862\ncollision_id                        0\nvehicle_type_code_1                33\nvehicle_type_code_2               645\nvehicle_type_code_3              1714\nvehicle_type_code_4              1828\nvehicle_type_code_5              1862\ndtype: int64\n\n\n\n# Fill missing values with NaN\ndf.fillna(float('nan'), inplace=True)\ndf.fillna\n\n&lt;bound method NDFrame.fillna of      crash_date crash_time   borough zip_code   latitude  longitude  \\\n0    2024-06-30      23:17     BRONX  10460.0  40.838844 -73.878170   \n1    2024-06-30       8:30     BRONX  10468.0  40.862732 -73.903330   \n2    2024-06-30      20:47       NaN      nan  40.763630 -73.953300   \n3    2024-06-30      13:10  BROOKLYN  11234.0  40.617030 -73.919890   \n4    2024-06-30      16:42       NaN      nan        NaN        NaN   \n...         ...        ...       ...      ...        ...        ...   \n1871 2024-07-07      20:15       NaN      nan  40.677982 -73.791214   \n1872 2024-07-07      14:45       NaN      nan  40.843822 -73.927500   \n1873 2024-07-07      14:12     BRONX  10468.0  40.861084 -73.911490   \n1874 2024-07-07       1:40       NaN      nan  40.813114 -73.931470   \n1875 2024-07-07      19:00       NaN      nan  40.680960 -73.773575   \n\n                     location           on_street_name  cross_street_name  \\\n0      (40.838844, -73.87817)          EAST 177 STREET       DEVOE AVENUE   \n1      (40.862732, -73.90333)        WEST FORDHAM ROAD       GRAND AVENUE   \n2        (40.76363, -73.9533)                FDR DRIVE                NaN   \n3       (40.61703, -73.91989)           EAST 57 STREET           AVENUE O   \n4                         NaN                33 STREET  ASTORIA BOULEVARD   \n...                       ...                      ...                ...   \n1871  (40.677982, -73.791214)        SUTPHIN BOULEVARD         120 AVENUE   \n1872    (40.843822, -73.9275)  MAJOR DEEGAN EXPRESSWAY                NaN   \n1873   (40.861084, -73.91149)                      NaN                NaN   \n1874   (40.813114, -73.93147)  MAJOR DEEGAN EXPRESSWAY                NaN   \n1875   (40.68096, -73.773575)           MARSDEN STREET                NaN   \n\n              off_street_name  ...   contributing_factor_vehicle_2  \\\n0                         NaN  ...                     Unspecified   \n1                         NaN  ...                     Unspecified   \n2                         NaN  ...                             NaN   \n3                         NaN  ...  Driver Inattention/Distraction   \n4                         NaN  ...                     Unspecified   \n...                       ...  ...                             ...   \n1871                      NaN  ...                     Unspecified   \n1872                      NaN  ...                     Unspecified   \n1873  2258      HAMPDEN PLACE  ...                             NaN   \n1874                      NaN  ...                     Unspecified   \n1875                      NaN  ...                             NaN   \n\n      contributing_factor_vehicle_3  contributing_factor_vehicle_4  \\\n0                               NaN                            NaN   \n1                               NaN                            NaN   \n2                               NaN                            NaN   \n3                               NaN                            NaN   \n4                               NaN                            NaN   \n...                             ...                            ...   \n1871                            NaN                            NaN   \n1872                            NaN                            NaN   \n1873                            NaN                            NaN   \n1874                            NaN                            NaN   \n1875                            NaN                            NaN   \n\n      contributing_factor_vehicle_5  collision_id  vehicle_type_code_1  \\\n0                               NaN       4737486                Sedan   \n1                               NaN       4737502                Sedan   \n2                               NaN       4737510                Sedan   \n3                               NaN       4737499                Sedan   \n4                               NaN       4736925                Sedan   \n...                             ...           ...                  ...   \n1871                            NaN       4745391                Sedan   \n1872                            NaN       4746540                Sedan   \n1873                            NaN       4746320                Sedan   \n1874                            NaN       4747076                Sedan   \n1875                            NaN       4749679                Sedan   \n\n                      vehicle_type_code_2  vehicle_type_code_3  \\\n0                           Pick-up Truck                  NaN   \n1                                     NaN                  NaN   \n2                                     NaN                  NaN   \n3                                   Sedan                  NaN   \n4     Station Wagon/Sport Utility Vehicle                  NaN   \n...                                   ...                  ...   \n1871                                Sedan                  NaN   \n1872                                Sedan                  NaN   \n1873                                  NaN                  NaN   \n1874  Station Wagon/Sport Utility Vehicle                  NaN   \n1875                                  NaN                  NaN   \n\n     vehicle_type_code_4 vehicle_type_code_5  \n0                    NaN                 NaN  \n1                    NaN                 NaN  \n2                    NaN                 NaN  \n3                    NaN                 NaN  \n4                    NaN                 NaN  \n...                  ...                 ...  \n1871                 NaN                 NaN  \n1872                 NaN                 NaN  \n1873                 NaN                 NaN  \n1874                 NaN                 NaN  \n1875                 NaN                 NaN  \n\n[1876 rows x 29 columns]&gt;\n\n\n\n\n\n5.2.7 Conclusion\nPandas is a powerful tool for data analysis. Learning how to use Pandas will\nallow you to perform more advanced analytics and become more proficent in using\npython.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Manipulation</span>"
    ]
  },
  {
    "objectID": "manipulation.html#example-nyc-crash-data",
    "href": "manipulation.html#example-nyc-crash-data",
    "title": "5  Data Manipulation",
    "section": "5.3 Example: NYC Crash Data",
    "text": "5.3 Example: NYC Crash Data\nConsider a subset of the NYC Crash Data, which contains all NYC motor vehicle collisions data with documentation from NYC Open Data. We downloaded the crash data for the week of June 30, 2024, on February 12, 2025, in CSC format.\n\nimport numpy as np\nimport pandas as pd\n\n# Load the dataset\nfile_path = 'data/nyccrashes_2024w0630_by20250212.csv'\ndf = pd.read_csv(file_path,\n                 dtype={'LATITUDE': np.float32,\n                        'LONGITUDE': np.float32,\n                        'ZIP CODE': str})\n\n# Replace column names: convert to lowercase and replace spaces with underscores\ndf.columns = df.columns.str.lower().str.replace(' ', '_')\n\n# Check for missing values\ndf.isnull().sum()\n\ncrash_date                          0\ncrash_time                          0\nborough                           542\nzip_code                          542\nlatitude                          131\nlongitude                         131\nlocation                          131\non_street_name                    546\ncross_street_name                 932\noff_street_name                  1330\nnumber_of_persons_injured           0\nnumber_of_persons_killed            0\nnumber_of_pedestrians_injured       0\nnumber_of_pedestrians_killed        0\nnumber_of_cyclist_injured           0\nnumber_of_cyclist_killed            0\nnumber_of_motorist_injured          0\nnumber_of_motorist_killed           0\ncontributing_factor_vehicle_1      11\ncontributing_factor_vehicle_2     450\ncontributing_factor_vehicle_3    1702\ncontributing_factor_vehicle_4    1824\ncontributing_factor_vehicle_5    1862\ncollision_id                        0\nvehicle_type_code_1                33\nvehicle_type_code_2               645\nvehicle_type_code_3              1714\nvehicle_type_code_4              1828\nvehicle_type_code_5              1862\ndtype: int64\n\n\nTake a peek at the first five rows:\n\ndf.head()\n\n\n\n\n\n\n\n\ncrash_date\ncrash_time\nborough\nzip_code\nlatitude\nlongitude\nlocation\non_street_name\ncross_street_name\noff_street_name\n...\ncontributing_factor_vehicle_2\ncontributing_factor_vehicle_3\ncontributing_factor_vehicle_4\ncontributing_factor_vehicle_5\ncollision_id\nvehicle_type_code_1\nvehicle_type_code_2\nvehicle_type_code_3\nvehicle_type_code_4\nvehicle_type_code_5\n\n\n\n\n0\n06/30/2024\n23:17\nBRONX\n10460\n40.838844\n-73.878166\n(40.838844, -73.87817)\nEAST 177 STREET\nDEVOE AVENUE\nNaN\n...\nUnspecified\nNaN\nNaN\nNaN\n4737486\nSedan\nPick-up Truck\nNaN\nNaN\nNaN\n\n\n1\n06/30/2024\n8:30\nBRONX\n10468\n40.862732\n-73.903328\n(40.862732, -73.90333)\nWEST FORDHAM ROAD\nGRAND AVENUE\nNaN\n...\nUnspecified\nNaN\nNaN\nNaN\n4737502\nSedan\nNaN\nNaN\nNaN\nNaN\n\n\n2\n06/30/2024\n20:47\nNaN\nNaN\n40.763630\n-73.953300\n(40.76363, -73.9533)\nFDR DRIVE\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\n4737510\nSedan\nNaN\nNaN\nNaN\nNaN\n\n\n3\n06/30/2024\n13:10\nBROOKLYN\n11234\n40.617031\n-73.919891\n(40.61703, -73.91989)\nEAST 57 STREET\nAVENUE O\nNaN\n...\nDriver Inattention/Distraction\nNaN\nNaN\nNaN\n4737499\nSedan\nSedan\nNaN\nNaN\nNaN\n\n\n4\n06/30/2024\n16:42\nNaN\nNaN\nNaN\nNaN\nNaN\n33 STREET\nASTORIA BOULEVARD\nNaN\n...\nUnspecified\nNaN\nNaN\nNaN\n4736925\nSedan\nStation Wagon/Sport Utility Vehicle\nNaN\nNaN\nNaN\n\n\n\n\n5 rows × 29 columns\n\n\n\nA quick summary of the data types of the columns:\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1876 entries, 0 to 1875\nData columns (total 29 columns):\n #   Column                         Non-Null Count  Dtype  \n---  ------                         --------------  -----  \n 0   crash_date                     1876 non-null   object \n 1   crash_time                     1876 non-null   object \n 2   borough                        1334 non-null   object \n 3   zip_code                       1334 non-null   object \n 4   latitude                       1745 non-null   float32\n 5   longitude                      1745 non-null   float32\n 6   location                       1745 non-null   object \n 7   on_street_name                 1330 non-null   object \n 8   cross_street_name              944 non-null    object \n 9   off_street_name                546 non-null    object \n 10  number_of_persons_injured      1876 non-null   int64  \n 11  number_of_persons_killed       1876 non-null   int64  \n 12  number_of_pedestrians_injured  1876 non-null   int64  \n 13  number_of_pedestrians_killed   1876 non-null   int64  \n 14  number_of_cyclist_injured      1876 non-null   int64  \n 15  number_of_cyclist_killed       1876 non-null   int64  \n 16  number_of_motorist_injured     1876 non-null   int64  \n 17  number_of_motorist_killed      1876 non-null   int64  \n 18  contributing_factor_vehicle_1  1865 non-null   object \n 19  contributing_factor_vehicle_2  1426 non-null   object \n 20  contributing_factor_vehicle_3  174 non-null    object \n 21  contributing_factor_vehicle_4  52 non-null     object \n 22  contributing_factor_vehicle_5  14 non-null     object \n 23  collision_id                   1876 non-null   int64  \n 24  vehicle_type_code_1            1843 non-null   object \n 25  vehicle_type_code_2            1231 non-null   object \n 26  vehicle_type_code_3            162 non-null    object \n 27  vehicle_type_code_4            48 non-null     object \n 28  vehicle_type_code_5            14 non-null     object \ndtypes: float32(2), int64(9), object(18)\nmemory usage: 410.5+ KB\n\n\nNow we can do some cleaning after a quick browse.\n\n# Replace invalid coordinates (latitude=0, longitude=0 or NaN) with NaN\ndf.loc[(df['latitude'] == 0) & (df['longitude'] == 0), \n       ['latitude', 'longitude']] = pd.NA\ndf['latitude'] = df['latitude'].replace(0, pd.NA)\ndf['longitude'] = df['longitude'].replace(0, pd.NA)\n\n# Drop the redundant `latitute` and `longitude` columns\ndf = df.drop(columns=['location'])\n\n# Converting 'crash_date' and 'crash_time' columns into a single datetime column\ndf['crash_datetime'] = pd.to_datetime(df['crash_date'] + ' ' \n                       + df['crash_time'], format='%m/%d/%Y %H:%M', errors='coerce')\n\n# Drop the original 'crash_date' and 'crash_time' columns\ndf = df.drop(columns=['crash_date', 'crash_time'])\n\nLet’s get some basic frequency tables of borough and zip_code, whose values could be used to check their validity against the legitmate values.\n\n# Frequency table for 'borough' without filling missing values\nborough_freq = df['borough'].value_counts(dropna=False).reset_index()\nborough_freq.columns = ['borough', 'count']\n\n# Frequency table for 'zip_code' without filling missing values\nzip_code_freq = df['zip_code'].value_counts(dropna=False).reset_index()\nzip_code_freq.columns = ['zip_code', 'count']\nzip_code_freq\n\n\n\n\n\n\n\n\nzip_code\ncount\n\n\n\n\n0\nNaN\n542\n\n\n1\n11207\n31\n\n\n2\n11208\n28\n\n\n3\n11236\n28\n\n\n4\n11101\n23\n\n\n...\n...\n...\n\n\n164\n10470\n1\n\n\n165\n11040\n1\n\n\n166\n11693\n1\n\n\n167\n11415\n1\n\n\n168\n10025\n1\n\n\n\n\n169 rows × 2 columns\n\n\n\nA comprehensive list of ZIP codes by borough can be obtained, for example, from the New York City Department of Health’s UHF Codes. We can use this list to check the validity of the zip codes in the data.\n\n# List of valid NYC ZIP codes compiled from UHF codes\n# Define all_valid_zips based on the earlier extracted ZIP codes\nall_valid_zips = {\n    10463, 10471, 10466, 10469, 10470, 10475, 10458, 10467, 10468,\n    10461, 10462, 10464, 10465, 10472, 10473, 10453, 10457, 10460,\n    10451, 10452, 10456, 10454, 10455, 10459, 10474, 11211, 11222,\n    11201, 11205, 11215, 11217, 11231, 11213, 11212, 11216, 11233,\n    11238, 11207, 11208, 11220, 11232, 11204, 11218, 11219, 11230,\n    11203, 11210, 11225, 11226, 11234, 11236, 11239, 11209, 11214,\n    11228, 11223, 11224, 11229, 11235, 11206, 11221, 11237, 10031,\n    10032, 10033, 10034, 10040, 10026, 10027, 10030, 10037, 10039,\n    10029, 10035, 10023, 10024, 10025, 10021, 10028, 10044, 10128,\n    10001, 10011, 10018, 10019, 10020, 10036, 10010, 10016, 10017,\n    10022, 10012, 10013, 10014, 10002, 10003, 10009, 10004, 10005,\n    10006, 10007, 10038, 10280, 11101, 11102, 11103, 11104, 11105,\n    11106, 11368, 11369, 11370, 11372, 11373, 11377, 11378, 11354,\n    11355, 11356, 11357, 11358, 11359, 11360, 11361, 11362, 11363,\n    11364, 11374, 11375, 11379, 11385, 11365, 11366, 11367, 11414,\n    11415, 11416, 11417, 11418, 11419, 11420, 11421, 11412, 11423,\n    11432, 11433, 11434, 11435, 11436, 11004, 11005, 11411, 11413,\n    11422, 11426, 11427, 11428, 11429, 11691, 11692, 11693, 11694,\n    11695, 11697, 10302, 10303, 10310, 10301, 10304, 10305, 10314,\n    10306, 10307, 10308, 10309, 10312\n}\n\n    \n# Convert set to list of strings\nall_valid_zips = list(map(str, all_valid_zips))\n\n# Identify invalid ZIP codes (including NaN)\ninvalid_zips = df[\n    df['zip_code'].isna() | ~df['zip_code'].isin(all_valid_zips)\n    ]['zip_code']\n\n# Calculate frequency of invalid ZIP codes\ninvalid_zip_freq = invalid_zips.value_counts(dropna=False).reset_index()\ninvalid_zip_freq.columns = ['zip_code', 'frequency']\n\ninvalid_zip_freq\n\n\n\n\n\n\n\n\nzip_code\nfrequency\n\n\n\n\n0\nNaN\n542\n\n\n1\n10065\n7\n\n\n2\n11249\n4\n\n\n3\n10112\n1\n\n\n4\n11040\n1\n\n\n\n\n\n\n\nAs it turns out, the collection of valid NYC zip codes differ from different sources. From United States Zip Codes, 10065 appears to be a valid NYC zip code. Under this circumstance, it might be safer to not remove any zip code from the data.\nTo be safe, let’s concatenate valid and invalid zips.\n\n# Convert invalid ZIP codes to a set of strings\ninvalid_zips_set = set(invalid_zip_freq['zip_code'].dropna().astype(str))\n\n# Convert all_valid_zips to a set of strings (if not already)\nvalid_zips_set = set(map(str, all_valid_zips))\n\n# Merge both sets\nmerged_zips = invalid_zips_set | valid_zips_set  # Union of both sets\n\nAre missing in zip code and borough always co-occur?\n\n# Check if missing values in 'zip_code' and 'borough' always co-occur\n# Count rows where both are missing\nmissing_cooccur = df[['zip_code', 'borough']].isnull().all(axis=1).sum()\n# Count total missing in 'zip_code' and 'borough', respectively\ntotal_missing_zip_code = df['zip_code'].isnull().sum()\ntotal_missing_borough = df['borough'].isnull().sum()\n\n# If missing in both columns always co-occur, the number of missing\n# co-occurrences should be equal to the total missing in either column\nnp.array([missing_cooccur, total_missing_zip_code, total_missing_borough])\n\narray([542, 542, 542])\n\n\nAre there cases where zip_code and borough are missing but the geo codes are not missing? If so, fill in zip_code and borough using the geo codes by reverse geocoding.\nFirst make sure geopy is installed.\npip install geopy\nNow we use model Nominatim in package geopy to reverse geocode.\n\nfrom geopy.geocoders import Nominatim\nimport time\n\n# Initialize the geocoder; the `user_agent` is your identifier \n# when using the service. Be mindful not to crash the server\n# by unlimited number of queries, especially invalid code.\ngeolocator = Nominatim(user_agent=\"jyGeopyTry\")\n\nWe write a function to do the reverse geocoding given lattitude and longitude.\n\n# Function to fill missing zip_code\ndef get_zip_code(latitude, longitude):\n    try:\n        location = geolocator.reverse((latitude, longitude), timeout=10)\n        if location:\n            address = location.raw['address']\n            zip_code = address.get('postcode', None)\n            return zip_code\n        else:\n            return None\n    except Exception as e:\n        print(f\"Error: {e} for coordinates {latitude}, {longitude}\")\n        return None\n    finally:\n        time.sleep(1)  # Delay to avoid overwhelming the service\n\nLet’s try it out:\n\n# Example usage\nlatitude = 40.730610\nlongitude = -73.935242\nget_zip_code(latitude, longitude)\n\n'11101'\n\n\nThe function get_zip_code can then be applied to rows where zip code is missing but geocodes are not to fill the missing zip code.\nOnce zip code is known, figuring out burough is simple because valid zip codes from each borough are known.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Manipulation</span>"
    ]
  },
  {
    "objectID": "manipulation.html#accessing-census-data",
    "href": "manipulation.html#accessing-census-data",
    "title": "5  Data Manipulation",
    "section": "5.4 Accessing Census Data",
    "text": "5.4 Accessing Census Data\nThe U.S. Census Bureau provides extensive demographic, economic, and social data through multiple surveys, including the decennial Census, the American Community Survey (ACS), and the Economic Census. These datasets offer valuable insights into population trends, economic conditions, and community characteristics at multiple geographic levels.\nThere are several ways to access Census data:\n\nCensus API: The Census API allows programmatic access to various datasets. It supports queries for different geographic levels and time periods.\ndata.census.gov: The official web interface for searching and downloading Census data.\nIPUMS USA: Provides harmonized microdata for longitudinal research. Available at IPUMS USA.\nNHGIS: Offers historical Census data with geographic information. Visit NHGIS.\n\nIn addition, Python tools simplify API access and data retrieval.\n\n5.4.1 Python Tools for Accessing Census Data\nSeveral Python libraries facilitate Census data retrieval:\n\ncensusapi: The official API wrapper for direct access to Census datasets.\ncensus: A high-level interface to the Census API, supporting ACS and decennial Census queries. See census on PyPI.\ncensusdata: A package for downloading and processing Census data directly in Python. Available at censusdata documentation.\nuszipcode: A library for retrieving Census and geographic information by ZIP code. See uszipcode on PyPI.\n\n\n\n5.4.2 Zip-Code Level for NYC Crash Data\nNow that we have NYC crash data, we might want to analyze patterns at the zip-code level to understand whether certain demographic or economic factors correlate with traffic incidents. While the crash dataset provides details about individual accidents, such as location, time, and severity, it does not contain contextual information about the neighborhoods where these crashes occur.\nTo perform meaningful zip-code-level analysis, we need additional data sources that provide relevant demographic, economic, and geographic variables. For example, understanding whether high-income areas experience fewer accidents, or whether population density influences crash frequency, requires integrating Census data. Key variables such as population size, median household income, employment rate, and population density can provide valuable context for interpreting crash trends across different zip codes.\nSince the Census Bureau provides detailed estimates for these variables at the zip-code level, we can use the Census API or other tools to retrieve relevant data and merge it with the NYC crash dataset. To access the Census API, you need an API key, which is free and easy to obtain. Visit the Census API Request page and submit your email address to receive a key. Once you have the key, you must include it in your API requests to access Census data. The following demonstration assumes that you have registered, obtained your API key, and saved it in a file called censusAPIkey.txt.\n\n# Import modules\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport geopandas as gpd\nfrom census import Census\nfrom us import states\nimport os\nimport io\n\napi_key = open(\"censusAPIkey.txt\").read().strip()\nc = Census(api_key)\n\nSuppose that we want to get some basic info from ACS data of the year of 2023 for all the NYC zip codes. The variable names can be found in the ACS variable documentation.\n\nACS_YEAR = 2023\nACS_DATASET = \"acs/acs5\"\n\n# Important ACS variables (including land area for density calculation)\nACS_VARIABLES = {\n    \"B01003_001E\": \"Total Population\",\n    \"B19013_001E\": \"Median Household Income\",\n    \"B02001_002E\": \"White Population\",\n    \"B02001_003E\": \"Black Population\",\n    \"B02001_005E\": \"Asian Population\",\n    \"B15003_022E\": \"Bachelor’s Degree Holders\",\n    \"B15003_025E\": \"Graduate Degree Holders\",\n    \"B23025_002E\": \"Labor Force\",\n    \"B23025_005E\": \"Unemployed\",\n    \"B25077_001E\": \"Median Home Value\"\n}\n\n# Convert set to list of strings\nmerged_zips = list(map(str, merged_zips))\n\nLet’s set up the query to request the ACS data, and process the returned data.\n\nacs_data = c.acs5.get(\n    list(ACS_VARIABLES.keys()), \n    {'for': f'zip code tabulation area:{\",\".join(merged_zips)}'}\n    )\n\n# Convert to DataFrame\ndf_acs = pd.DataFrame(acs_data)\n\n# Rename columns\ndf_acs.rename(columns=ACS_VARIABLES, inplace=True)\ndf_acs.rename(columns={\"zip code tabulation area\": \"ZIP Code\"}, inplace=True)\n\nWe could save the ACS data df_acs in feather format (see next Section).\n#| eval: false\ndf_acs.to_feather(\"data/acs2023.feather\")\nThe population density could be an important factor for crash likelihood. To obtain the population densities, we need the areas of the zip codes. The shape files can be obtained from NYC Open Data.\n\nimport requests\nimport zipfile\nimport geopandas as gpd\n\n# Define the NYC MODZCTA shapefile URL and extraction directory\nshapefile_url = \"https://data.cityofnewyork.us/api/geospatial/pri4-ifjk?method=export&format=Shapefile\"\nextract_dir = \"MODZCTA_Shapefile\"\n\n# Create the directory if it doesn't exist\nos.makedirs(extract_dir, exist_ok=True)\n\n# Step 1: Download and extract the shapefile\nprint(\"Downloading MODZCTA shapefile...\")\nresponse = requests.get(shapefile_url)\nwith zipfile.ZipFile(io.BytesIO(response.content), \"r\") as z:\n    z.extractall(extract_dir)\n\nprint(f\"Shapefile extracted to: {extract_dir}\")\n\nDownloading MODZCTA shapefile...\nShapefile extracted to: MODZCTA_Shapefile\n\n\nNow we process the shape file to calculate the areas of the polygons.\n\n# Step 2: Automatically detect the correct .shp file\nshapefile_path = None\nfor file in os.listdir(extract_dir):\n    if file.endswith(\".shp\"):\n        shapefile_path = os.path.join(extract_dir, file)\n        break  # Use the first .shp file found\n\nif not shapefile_path:\n    raise FileNotFoundError(\"No .shp file found in extracted directory.\")\n\nprint(f\"Using shapefile: {shapefile_path}\")\n\n# Step 3: Load the shapefile into GeoPandas\ngdf = gpd.read_file(shapefile_path)\n\n# Step 4: Convert to CRS with meters for accurate area calculation\ngdf = gdf.to_crs(epsg=3857)\n\n# Step 5: Compute land area in square miles\ngdf['land_area_sq_miles'] = gdf['geometry'].area / 2_589_988.11\n# 1 square mile = 2,589,988.11 square meters\n\nprint(gdf[['modzcta', 'land_area_sq_miles']].head())\n\nUsing shapefile: MODZCTA_Shapefile/geo_export_1daca795-0288-4cf1-be6c-e69d6ffefeee.shp\n  modzcta  land_area_sq_miles\n0   10001            1.153516\n1   10002            1.534509\n2   10003            1.008318\n3   10026            0.581848\n4   10004            0.256876\n\n\nLet’s export this data frame for future usage in feather format (see next Section).\n\ngdf[['modzcta', 'land_area_sq_miles']].to_feather('data/nyc_zip_areas.feather')\n\nNow we are ready to merge the two data frames.\n\n# Merge ACS data (`df_acs`) directly with MODZCTA land area (`gdf`)\ngdf = gdf.merge(df_acs, left_on='modzcta', right_on='ZIP Code', how='left')\n\n# Calculate Population Density (people per square mile)\ngdf['popdensity_per_sq_mile'] = (\n    gdf['Total Population'] / gdf['land_area_sq_miles']\n    )\n\n# Display first few rows\nprint(gdf[['modzcta', 'Total Population', 'land_area_sq_miles',\n    'popdensity_per_sq_mile']].head())\n\n  modzcta  Total Population  land_area_sq_miles  popdensity_per_sq_mile\n0   10001           27004.0            1.153516            23410.171200\n1   10002           76518.0            1.534509            49864.797219\n2   10003           53877.0            1.008318            53432.563117\n3   10026           38265.0            0.581848            65764.650082\n4   10004            4579.0            0.256876            17825.700993\n\n\nSome visualization of population density.\n\nimport matplotlib.pyplot as plt\nimport geopandas as gpd\n\n# Set up figure and axis\nfig, ax = plt.subplots(figsize=(10, 12))\n\n# Plot the choropleth map\ngdf.plot(column='popdensity_per_sq_mile', \n         cmap='viridis',  # Use a visually appealing color map\n         linewidth=0.8, \n         edgecolor='black',\n         legend=True,\n         legend_kwds={'label': \"Population Density (per sq mile)\",\n             'orientation': \"horizontal\"},\n         ax=ax)\n\n# Add a title\nax.set_title(\"Population Density by ZCTA in NYC\", fontsize=14)\n\n# Remove axes\nax.set_xticks([])\nax.set_yticks([])\nax.set_frame_on(False)\n\n# Show the plot\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Manipulation</span>"
    ]
  },
  {
    "objectID": "manipulation.html#cross-platform-data-format-arrow",
    "href": "manipulation.html#cross-platform-data-format-arrow",
    "title": "5  Data Manipulation",
    "section": "5.5 Cross-platform Data Format Arrow",
    "text": "5.5 Cross-platform Data Format Arrow\nThe CSV format (and related formats like TSV - tab-separated values) for data tables is ubiquitous, convenient, and can be read or written by many different data analysis environments, including spreadsheets. An advantage of the textual representation of the data in a CSV file is that the entire data table, or portions of it, can be previewed in a text editor. However, the textual representation can be ambiguous and inconsistent. The format of a particular column: Boolean, integer, floating-point, text, factor, etc. must be inferred from text representation, often at the expense of reading the entire file before these inferences can be made. Experienced data scientists are aware that a substantial part of an analysis or report generation is often the “data cleaning” involved in preparing the data for analysis. This can be an open-ended task — it required numerous trial-and-error iterations to create the list of different missing data representations we use for the sample CSV file and even now we are not sure we have them all.\nTo read and export data efficiently, leveraging the Apache Arrow library can significantly improve performance and storage efficiency, especially with large datasets. The IPC (Inter-Process Communication) file format in the context of Apache Arrow is a key component for efficiently sharing data between different processes, potentially written in different programming languages. Arrow’s IPC mechanism is designed around two main file formats:\n\nStream Format: For sending an arbitrary length sequence of Arrow record batches (tables). The stream format is useful for real-time data exchange where the size of the data is not known upfront and can grow indefinitely.\nFile (or Feather) Format: Optimized for storage and memory-mapped access, allowing for fast random access to different sections of the data. This format is ideal for scenarios where the entire dataset is available upfront and can be stored in a file system for repeated reads and writes.\n\nApache Arrow provides a columnar memory format for flat and hierarchical data, optimized for efficient data analytics. It can be used in Python through the pyarrow package. Here’s how you can use Arrow to read, manipulate, and export data, including a demonstration of storage savings.\nFirst, ensure you have pyarrow installed on your computer (and preferrably, in your current virtual environment):\npip install pyarrow\nFeather is a fast, lightweight, and easy-to-use binary file format for storing data frames, optimized for speed and efficiency, particularly for IPC and data sharing between Python and R or Julia.\nThe following code processes the cleaned data in CSV format from Mohammad Mundiwala and write out in Arrow format.\n\nimport pandas as pd\n\n# Read CSV, ensuring 'zip_code' is string and 'crash_datetime' is parsed as datetime\ndf = pd.read_csv('data/nyc_crashes_cleaned_mm.csv',\n                 dtype={'zip_code': str},\n                 parse_dates=['crash_datetime'])\n\n# Drop the 'date' and 'time' columns\ndf = df.drop(columns=['crash_date', 'crash_time'])\n\n# Move 'crash_datetime' to the first column\ndf = df[['crash_datetime'] + df.drop(columns=['crash_datetime']).columns.tolist()]\n\ndf['zip_code'] = df['zip_code'].astype(str).str.rstrip('.0')\n\ndf = df.sort_values(by='crash_datetime')\n\ndf.to_feather('nyccrashes_cleaned.feather')\n\nLet’s compare the file sizes of the feather format and the CSV format.\n\nimport os\n\n# File paths\ncsv_file = 'data/nyccrashes_2024w0630_by20250212.csv'\nfeather_file = 'data/nyccrashes_cleaned.feather'\n\n# Get file sizes in bytes\ncsv_size = os.path.getsize(csv_file)\nfeather_size = os.path.getsize(feather_file)\n\n# Convert bytes to a more readable format (e.g., MB)\ncsv_size_mb = csv_size / (1024 * 1024)\nfeather_size_mb = feather_size / (1024 * 1024)\n\n# Print the file sizes\nprint(f\"CSV file size: {csv_size_mb:.2f} MB\")\nprint(f\"Feather file size: {feather_size_mb:.2f} MB\")\n\nCSV file size: 0.34 MB\nFeather file size: 0.19 MB\n\n\nRead the feather file back in:\n#| eval: false\ndff = pd.read_feather(\"data/nyccrashes_cleaned.feather\")\ndff.shape",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Manipulation</span>"
    ]
  },
  {
    "objectID": "manipulation.html#database-with-sql",
    "href": "manipulation.html#database-with-sql",
    "title": "5  Data Manipulation",
    "section": "5.6 Database with SQL",
    "text": "5.6 Database with SQL\n\nThis section was prepared by Alyssa Horn, a junior Applied Data Analysis major with a domain in Public Policy/Management.\nThis section explores database operations using SQL in Python, focusing on the contributing_factor_vehicle_1 column of the NYC crash dataset. We’ll use Python’s sqlite3 and pandas libraries for database interaction.\n\n\n5.6.1 What is a Database?\n\nA collection of related data.\nOrganized in a way that allows computers to efficiently store, retrieve, and manipulate information.\n“Filing system” for large amounts of data that can be easily accessed and analyzed\n\n\n5.6.1.1 Non-relational Databases\n\nIdeal for handling large volumes of unstructured or semi-structured data.\nDoes not store data in a traditional tabular format with rows and columns like a relational database.\nAllows for more flexible data structures like documents, key-value pairs, graphs, or columns.\n\n\n\n5.6.1.2 Relational Databases\n\nStores data in tables with rows (records) and columns (attributes).\nEach table has a primary key to uniquely identify records.\nAllows you to easily access and understand how different pieces of data are connected to each other.\nExample: In a phone book, each contact has a unique ID, name, phone number, and address.\n\n\n\n\n5.6.2 What is SQL?\n\nStructured Query Language for managing and querying relational databases.\nHelps you store, retrieve, update, and delete data easily using simple commands in Python.\nUsing sqlite3, you can run SQL queries directly from your Python code to interact with your database seamlessly.\n\n\n5.6.2.1 CRUD Model\n\nThe four most basic operations that can be performed with most traditional database systems and they are the backbone for interacting with any database.\n\nCreate: Insert new records.\nRead: Retrieve data.\nUpdate: Modify existing records.\nDelete: Remove records.\n\n\n\n\n5.6.2.2 What can SQL do?\n\nExecute queries against a database\nRetrieve data from a database\nInsert records in a database\nUpdate records in a database\nDelete records from a database\nCreate new databases\nCreate new tables in a database\nCreate stored procedures in a database\nCreate views in a database\nSet permissions on tables, procedures, and views\n\n\n\n5.6.2.3 Key Statements\n\nCreate a cursor object to interact with the database\ncursor.execute executes a single SQL statement\nconn.commit saves all changes made\nquery = requests specific information from database\n\n\n\n\n5.6.3 Setting up the Database\n\n5.6.3.1 Read in the datatset and store dataframe as SQL table\nWe start by importing necessary packages and reading in the cleaned nyccrashes feather.\n\nUse data.to_sql to store dataframe as an SQL table.\n\n\nimport sqlite3\nimport pandas as pd\n\n# Create a database and load the NYC crash data\ndb_path = 'nyc_crash.db'\nconn = sqlite3.connect(db_path)\n#The conn object acts as a bridge between Python and the database,\n#allowing you to execute SQL queries and manage data.\n\n# Load Feather\ndata = pd.read_feather(\"data/nyccrashes_cleaned.feather\")\n\n# create crash_date and crash_time columns\ndata[\"crash_date\"] = pd.to_datetime(data[\"crash_datetime\"]).dt.date\ndata[\"crash_time\"] = pd.to_datetime(data[\"crash_datetime\"]).dt.strftime(\"%H:%M:%S\")\n\n# Drop the original datetime column (optional)\ndata.drop(columns=[\"crash_datetime\"], inplace=True)\n\n# Store DataFrame as a SQL table\ndata.to_sql('nyc_crashes', conn, if_exists='replace', index=False)\n\n1875\n\n\n\n\n5.6.3.2 Display the Table\nWe can display the table by querying all (or some) of the data and using the .head() command\n\n# Query to select all data (or limit rows to avoid overload)\nquery = \"SELECT * FROM nyc_crashes LIMIT 10;\"\n\n# Load the data into a pandas DataFrame\nnyc_crashes_data = pd.read_sql_query(query, conn)\n\n# Display the DataFrame\nnyc_crashes_data.head(5)\n\n\n\n\n\n\n\n\nborough\nzip_code\nlatitude\nlongitude\nlocation\non_street_name\ncross_street_name\noff_street_name\nnumber_of_persons_injured\nnumber_of_persons_killed\n...\ncontributing_factor_vehicle_4\ncontributing_factor_vehicle_5\ncollision_id\nvehicle_type_code_1\nvehicle_type_code_2\nvehicle_type_code_3\nvehicle_type_code_4\nvehicle_type_code_5\ncrash_date\ncrash_time\n\n\n\n\n0\nNone\nNaN\nNaN\nNaN\n(0.0, 0.0)\nNone\nNone\nGOLD STREET\n0\n0\n...\nNone\nNone\n4736746\nSedan\nSedan\nNone\nNone\nNone\n2024-06-30\n17:30:00\n\n\n1\nNone\nNaN\nNaN\nNaN\nNone\nBELT PARKWAY RAMP\nNone\nNone\n0\n0\n...\nNone\nNone\n4736768\nStation Wagon/Sport Utility Vehicle\nStation Wagon/Sport Utility Vehicle\nNone\nNone\nNone\n2024-06-30\n00:32:00\n\n\n2\nBROOKLYN\n11235.0\n40.58106\n-73.96744\n(40.58106, -73.96744)\nNone\nNone\n2797 OCEAN PARKWAY\n0\n0\n...\nNone\nNone\n4737060\nStation Wagon/Sport Utility Vehicle\nNone\nNone\nNone\nNone\n2024-06-30\n07:05:00\n\n\n3\nMANHATTAN\n10021.0\n40.76363\n-73.95330\n(40.76363, -73.9533)\nFDR DRIVE\nNone\nNone\n0\n0\n...\nNone\nNone\n4737510\nSedan\nNone\nNone\nNone\nNone\n2024-06-30\n20:47:00\n\n\n4\nBROOKLYN\n11222.0\n40.73046\n-73.95149\n(40.73046, -73.95149)\nGREENPOINT AVENUE\nMC GUINNESS BOULEVARD\nNone\n0\n0\n...\nNone\nNone\n4736759\nBus\nBox Truck\nNone\nNone\nNone\n2024-06-30\n10:14:00\n\n\n\n\n5 rows × 29 columns\n\n\n\n\n\n\n5.6.4 Normalizing the Database with a Lookup Table\n\n5.6.4.1 Create the lookup table\nCreate the lookup table using the create table command with the corresponding column names.\n\n# Connect to the SQLite database\ncursor = conn.cursor()\n\ncursor.execute('''\nCREATE TABLE IF NOT EXISTS contributing_factor_lookup (\n    factor_id INTEGER PRIMARY KEY AUTOINCREMENT,\n    factor_description TEXT UNIQUE\n)\n''')\nprint(\"Lookup table created successfully.\")\n\nLookup table created successfully.\n\n\n\n\n5.6.4.2 Populate the Lookup Table with Distinct Values\nPopulate the lookup table with the values contained in contributing_factor_vehicle_1.\n\ncursor.execute('''\nINSERT OR IGNORE INTO contributing_factor_lookup (factor_description)\nSELECT DISTINCT contributing_factor_vehicle_1\nFROM nyc_crashes\nWHERE contributing_factor_vehicle_1 IS NOT NULL;\n''')\nprint(\"Lookup table populated with distinct contributing factors.\")\n\nLookup table populated with distinct contributing factors.\n\n\n\n\n5.6.4.3 Update the Original Table to Include factor_id\nUse ALTER TABLE to add factor_id column into original table.\n\ncursor.execute('''\nALTER TABLE nyc_crashes ADD COLUMN factor_id INTEGER;\n''')\nprint(\"Added 'factor_id' column to nyc_crashes table.\")\n\nAdded 'factor_id' column to nyc_crashes table.\n\n\n\n\n5.6.4.4 Update the Original Table with Corresponding Codes\nUse UPDATE command to update table with factor descriptions.\n\ncursor.execute('''\nUPDATE nyc_crashes\nSET factor_id = (\n    SELECT factor_id \n    FROM contributing_factor_lookup \n    WHERE contributing_factor_vehicle_1 = factor_description\n)\nWHERE contributing_factor_vehicle_1 IS NOT NULL;\n''')\nprint(\"Updated nyc_crashes with corresponding factor IDs.\")\n\nUpdated nyc_crashes with corresponding factor IDs.\n\n\n\n\n5.6.4.5 Query with a Join to Retrieve Full Descriptions\nUse JOIN command to recieve contributing factor descriptions from factor_id.\n\nquery = '''\nSELECT n.*, l.factor_description\nFROM nyc_crashes n\nJOIN contributing_factor_lookup l ON n.factor_id = l.factor_id\nLIMIT 10;\n'''\n\n# Load the data into a pandas DataFrame\nresult_df = pd.read_sql_query(query, conn)\n\n# Commit changes\nconn.commit()\n\nresult_df.head()\n\n\n\n\n\n\n\n\nborough\nzip_code\nlatitude\nlongitude\nlocation\non_street_name\ncross_street_name\noff_street_name\nnumber_of_persons_injured\nnumber_of_persons_killed\n...\ncollision_id\nvehicle_type_code_1\nvehicle_type_code_2\nvehicle_type_code_3\nvehicle_type_code_4\nvehicle_type_code_5\ncrash_date\ncrash_time\nfactor_id\nfactor_description\n\n\n\n\n0\nNone\nNaN\nNaN\nNaN\n(0.0, 0.0)\nNone\nNone\nGOLD STREET\n0\n0\n...\n4736746\nSedan\nSedan\nNone\nNone\nNone\n2024-06-30\n17:30:00\n1\nPassing Too Closely\n\n\n1\nNone\nNaN\nNaN\nNaN\nNone\nBELT PARKWAY RAMP\nNone\nNone\n0\n0\n...\n4736768\nStation Wagon/Sport Utility Vehicle\nStation Wagon/Sport Utility Vehicle\nNone\nNone\nNone\n2024-06-30\n00:32:00\n2\nUnspecified\n\n\n2\nBROOKLYN\n11235.0\n40.58106\n-73.96744\n(40.58106, -73.96744)\nNone\nNone\n2797 OCEAN PARKWAY\n0\n0\n...\n4737060\nStation Wagon/Sport Utility Vehicle\nNone\nNone\nNone\nNone\n2024-06-30\n07:05:00\n2\nUnspecified\n\n\n3\nMANHATTAN\n10021.0\n40.76363\n-73.95330\n(40.76363, -73.9533)\nFDR DRIVE\nNone\nNone\n0\n0\n...\n4737510\nSedan\nNone\nNone\nNone\nNone\n2024-06-30\n20:47:00\n2\nUnspecified\n\n\n4\nBROOKLYN\n11222.0\n40.73046\n-73.95149\n(40.73046, -73.95149)\nGREENPOINT AVENUE\nMC GUINNESS BOULEVARD\nNone\n0\n0\n...\n4736759\nBus\nBox Truck\nNone\nNone\nNone\n2024-06-30\n10:14:00\n1\nPassing Too Closely\n\n\n\n\n5 rows × 31 columns\n\n\n\n\n\n5.6.4.6 Display Table With factor.id\nSince we added factor_id column to dataframe, we can now display the table including the factor_id column using a query.\n\n# Query to select all data (or limit rows to avoid overload)\nquery = \"SELECT * FROM nyc_crashes LIMIT 10;\"\n\n# Load the data into a pandas DataFrame\nnyc_crashes_data = pd.read_sql_query(query, conn)\n\n# Display the DataFrame\nnyc_crashes_data.head()\n\n\n\n\n\n\n\n\nborough\nzip_code\nlatitude\nlongitude\nlocation\non_street_name\ncross_street_name\noff_street_name\nnumber_of_persons_injured\nnumber_of_persons_killed\n...\ncontributing_factor_vehicle_5\ncollision_id\nvehicle_type_code_1\nvehicle_type_code_2\nvehicle_type_code_3\nvehicle_type_code_4\nvehicle_type_code_5\ncrash_date\ncrash_time\nfactor_id\n\n\n\n\n0\nNone\nNaN\nNaN\nNaN\n(0.0, 0.0)\nNone\nNone\nGOLD STREET\n0\n0\n...\nNone\n4736746\nSedan\nSedan\nNone\nNone\nNone\n2024-06-30\n17:30:00\n1\n\n\n1\nNone\nNaN\nNaN\nNaN\nNone\nBELT PARKWAY RAMP\nNone\nNone\n0\n0\n...\nNone\n4736768\nStation Wagon/Sport Utility Vehicle\nStation Wagon/Sport Utility Vehicle\nNone\nNone\nNone\n2024-06-30\n00:32:00\n2\n\n\n2\nBROOKLYN\n11235.0\n40.58106\n-73.96744\n(40.58106, -73.96744)\nNone\nNone\n2797 OCEAN PARKWAY\n0\n0\n...\nNone\n4737060\nStation Wagon/Sport Utility Vehicle\nNone\nNone\nNone\nNone\n2024-06-30\n07:05:00\n2\n\n\n3\nMANHATTAN\n10021.0\n40.76363\n-73.95330\n(40.76363, -73.9533)\nFDR DRIVE\nNone\nNone\n0\n0\n...\nNone\n4737510\nSedan\nNone\nNone\nNone\nNone\n2024-06-30\n20:47:00\n2\n\n\n4\nBROOKLYN\n11222.0\n40.73046\n-73.95149\n(40.73046, -73.95149)\nGREENPOINT AVENUE\nMC GUINNESS BOULEVARD\nNone\n0\n0\n...\nNone\n4736759\nBus\nBox Truck\nNone\nNone\nNone\n2024-06-30\n10:14:00\n1\n\n\n\n\n5 rows × 30 columns\n\n\n\n\n\n\n5.6.5 Inserting Data\nNew records (rows) can be added into a database table. The INSERT INTO statement is used to accomplish this task. When you insert data, you provide values for one or more columns in the table.\n\nINSERT INTO table_name (columns) VALUES (values\n\nInsert a new crash record into the nyc_crashes table with the date 06/30/2024, time 10:15, location BROOKLYN, and contributing factor “Driver Inattention/Distraction”.\n\ncursor = conn.cursor()\n\n# Adds a crash on 06/30/2024 at 10:15 in\n# Brooklyn due to Inattention/Distraction\ncursor.execute(\"\"\"\nINSERT INTO nyc_crashes (crash_date, crash_time, \nborough, contributing_factor_vehicle_1)\nVALUES ('2024-06-30', '10:15:00', 'BROOKLYN',\n'Driver Inattention/Distraction')\n\"\"\")\n\nconn.commit()\n\n\n5.6.5.1 Verify the record exists\nWe can use a query for a specific data point to verify if addition was successful.\n\nquery_before = \"\"\"\nSELECT * FROM nyc_crashes \nWHERE crash_date = '2024-06-30' \nAND crash_time = '10:15:00' \nAND borough = 'BROOKLYN';\n\"\"\"\n\nbefore_deletion = pd.read_sql_query(query_before, conn)\nprint(\"Before Deletion:\")\nbefore_deletion\n\nBefore Deletion:\n\n\n\n\n\n\n\n\n\nborough\nzip_code\nlatitude\nlongitude\nlocation\non_street_name\ncross_street_name\noff_street_name\nnumber_of_persons_injured\nnumber_of_persons_killed\n...\ncontributing_factor_vehicle_5\ncollision_id\nvehicle_type_code_1\nvehicle_type_code_2\nvehicle_type_code_3\nvehicle_type_code_4\nvehicle_type_code_5\ncrash_date\ncrash_time\nfactor_id\n\n\n\n\n0\nBROOKLYN\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\n...\nNone\nNone\nNone\nNone\nNone\nNone\nNone\n2024-06-30\n10:15:00\nNone\n\n\n\n\n1 rows × 30 columns\n\n\n\n\n\n\n5.6.6 Deleting Data\nUse DELETE FROM statement to delete data\n\ndelete_query = \"\"\"\nDELETE FROM nyc_crashes\nWHERE crash_date = '2024-06-30' \nAND crash_time = '10:15:00' \nAND borough = 'BROOKLYN'\nAND contributing_factor_vehicle_1 = 'Driver Inattention/Distraction';\n\"\"\"\n\ncursor.execute(delete_query)\nconn.commit()\n\n\n5.6.6.1 Verify the deletion\nWe can use a query for a specific data point to verify if deletion was successful.\n\nquery_after = \"\"\"\nSELECT * FROM nyc_crashes \nWHERE crash_date = '2024-06-30' \nAND crash_time = '10:15:00' \nAND borough = 'BROOKLYN';\n\"\"\"\n\nafter_deletion = pd.read_sql_query(query_after, conn)\nprint(\"After Deletion:\")\nafter_deletion\n\nAfter Deletion:\n\n\n\n\n\n\n\n\n\nborough\nzip_code\nlatitude\nlongitude\nlocation\non_street_name\ncross_street_name\noff_street_name\nnumber_of_persons_injured\nnumber_of_persons_killed\n...\ncontributing_factor_vehicle_5\ncollision_id\nvehicle_type_code_1\nvehicle_type_code_2\nvehicle_type_code_3\nvehicle_type_code_4\nvehicle_type_code_5\ncrash_date\ncrash_time\nfactor_id\n\n\n\n\n\n\n0 rows × 30 columns\n\n\n\n\n\n\n5.6.7 Querying the data\nQuerying the data means requesting specific information from a database. In SQL, queries are written as commands to retrieve, filter, group, or sort data based on certain conditions. The goal of querying is to extract meaningful insights or specific subsets of data from a larger dataset.\n\nSELECT DISTINCT retrieves unique values from a column\npd.read_sql_query() executes the SQL query and returns the result as a DataFrame\n\n\n\n5.6.7.1 Query to find distinct contributing factors\nThis query selects the distinct contributing factors from the contributing_factor_vehicle_1 column.\n\nquery = \"SELECT DISTINCT contributing_factor_vehicle_1 FROM nyc_crashes;\"\n\nfactors = pd.read_sql_query(query, conn)\n\nfactors.head(5)\n\n\n\n\n\n\n\n\ncontributing_factor_vehicle_1\n\n\n\n\n0\nPassing Too Closely\n\n\n1\nUnspecified\n\n\n2\nDriver Inattention/Distraction\n\n\n3\nFailure to Yield Right-of-Way\n\n\n4\nOther Vehicular\n\n\n\n\n\n\n\n\n\n\n5.6.7.2 Can query using factor.id\nThis query selects the distinct contributing factors using the factor_id column.\n\nquery = \"\"\"\nSELECT DISTINCT n.factor_id, l.factor_description \nFROM nyc_crashes n\nJOIN contributing_factor_lookup l ON n.factor_id = l.factor_id\nWHERE n.factor_id IS NOT NULL;\n\"\"\"\n\nfactors = pd.read_sql_query(query, conn)\nfactors.head(5)\n\n\n\n\n\n\n\n\nfactor_id\nfactor_description\n\n\n\n\n0\n1\nPassing Too Closely\n\n\n1\n2\nUnspecified\n\n\n2\n3\nDriver Inattention/Distraction\n\n\n3\n4\nFailure to Yield Right-of-Way\n\n\n4\n5\nOther Vehicular\n\n\n\n\n\n\n\n\n\n\n5.6.8 Analyzing contributing_factor_vehicle_1\n\nSELECT Choose columns to retrieve\nCOUNT Count rows for each group\nGROUP BY Group rows that have the same values in specific columns\nORDER BY Sort results by the count in descending order\n\n\n5.6.8.1 Insights into contributing_factor_vehicle_1\n\nIdentify the most common contributing factors.\nUnderstand trends related to vehicle crash causes.\n\n\nfactor_count = pd.read_sql_query(\"\"\"\nSELECT contributing_factor_vehicle_1, COUNT(*) AS count \nFROM nyc_crashes \nGROUP BY contributing_factor_vehicle_1\nORDER BY count DESC;\n\"\"\", conn)\n\nfactor_count.head(10)\n\n\n\n\n\n\n\n\ncontributing_factor_vehicle_1\ncount\n\n\n\n\n0\nUnspecified\n473\n\n\n1\nDriver Inattention/Distraction\n447\n\n\n2\nFailure to Yield Right-of-Way\n116\n\n\n3\nFollowing Too Closely\n104\n\n\n4\nUnsafe Speed\n82\n\n\n5\nPassing or Lane Usage Improper\n74\n\n\n6\nTraffic Control Disregarded\n66\n\n\n7\nOther Vehicular\n63\n\n\n8\nPassing Too Closely\n57\n\n\n9\nAlcohol Involvement\n57\n\n\n\n\n\n\n\n\n\n\n5.6.9 Visualizing Analysis\nCan use Plotnine to visualize our Analysis for contributing_factor_vehicle_1 in a chart.\n\nfrom plotnine import ggplot, aes, geom_bar, theme_minimal, coord_flip, labs\n\ndb_path = 'nyc_crash.db'\nconn = sqlite3.connect(db_path)\n\n# Query to get the contributing factor counts\nfactor_count = pd.read_sql_query(\"\"\"\nSELECT contributing_factor_vehicle_1, COUNT(*) AS count\nFROM nyc_crashes\nGROUP BY contributing_factor_vehicle_1\nORDER BY count DESC;\n\"\"\", conn)\n\n# Create a bar chart using plotnine\nchart = (\n    ggplot(factor_count, aes(x='reorder(contributing_factor_vehicle_1, count)'\n                             , y='count')) +\n    geom_bar(stat='identity', fill='steelblue') +\n    coord_flip() +  # Flip for better readability\n    theme_minimal() +\n    labs(title='Top Contributing Factors to NYC Crashes',\n         x='Contributing Factor',\n         y='Number of Incidents')\n)\n\nchart\n\n\n\n\n\n\n\n\n\n\n5.6.10 Conclusion\n\nSQL in Python is powerful for handling structured data.\nsqlite3 and pandas simplify database interactions.\nAnalyzing crash data helps understand key contributing factors for traffic incidents.\n\n\n\n\n5.6.11 Further Readings:\n\n[How to use SQL in Python] (Przybyla (2024))\n[Python MySQL] (W3Schools (2025))\n[SQL using Python] (Bansal (2024))\n[Master Using SQL with Python - Using SQL with Pandas] (Cafferky (2019))",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Manipulation</span>"
    ]
  },
  {
    "objectID": "manipulation.html#imputation-methods-for-missing-data",
    "href": "manipulation.html#imputation-methods-for-missing-data",
    "title": "5  Data Manipulation",
    "section": "5.7 Imputation Methods for Missing Data",
    "text": "5.7 Imputation Methods for Missing Data\n\nThis section was prepared by Sebastian Symula, a Junior Statistical Data Science major with a domain in statistics and a minor in math graduating in the fall.\nThis section will serve as an introduction into what data imputation is, when to do it, as well as comparing and contrasting a few different methods of imputation.\n\n\n5.7.1 What is Data Imputation?\nIn practice, most datasets will have missing values. This can be due to things like faulty equipment, nonresponse, or the data not existing in the first place.\nWhen data is missing, you have two options:\n\nIgnore it (Complete Case Analysis): We only use complete rows without missing data for our analysis.\nImpute it: We fill in missing values using some statistical method.\n\nTypes of Missing Data:\n\nMissing Completely at Random (MCAR):This is the best case scenario. There is no pattern to be observed with missing and observed data. The full rows can be treated as an independent subset of the full dataset. This does not introduce bias and is usually not a realistic assumption. Each row has same chance of being missing.\nMissing at Random (MAR): Missing data can be explained by observed data. missingness of data is related to observed variables, but not to the missing values themselves. Therefore, the mechanism of missingness can be modeled with observed data.\nMissing Not at Random (MNAR): This is when the missingness may say something about the true value, due to some unobserved data. So the fact that the value is missing has an impact on the true value. Do NOT impute in this case.\n\nTypes of Missing Data Example:\nLet’s imagine we have a dataset where one column is gender and the other is the respondent’s happiness score from a survey.\n\nMCAR: If we see a relatively even spread of missing values between men and women. In this case the complete cases would be an independent subset of the dataset and we could run complete case analysis without any bias provided the missing data isn’t a large portion of our set.\nMAR: If we see a pattern of more missing values for men, but we assume this is due to men not finishing the survey and does not correlate with their happiness score. In this case we can use the observed cases for men to impute the missing ones.\nMNAR: If we think someone who’s less happy might be less likely to complete the survey. Can’t use observed values to predict missing ones.\n\n\n\n5.7.2 Inspecting Missingness\nLet’s create a complete dataset, then introduce MAR data:\n\nimport pandas as pd\nimport numpy as np\n\nnp.random.seed(10)\ndata = {\n    'ID': np.arange(1, 3000 + 1),\n    'Age': np.random.randint(18, 60,3000),\n    'Salary': np.random.randint(30000, 100000, 3000),\n    'Department': np.random.choice(['Sales', 'HR', 'IT', 'Finance'], 3000),\n    'Location': np.random.choice(['Remote', 'Hybrid'], 3000),\n    'Tenure': np.random.randint(0,35, 3000)\n}\ndf = pd.DataFrame(data)\n\nDataset has 6 columns and 3000 rows.\nIntroducing Missing Values:\n\nsal_prob = np.clip((df['Age'] - 30) / 40, 0, 1)\nsal_mask = np.random.uniform(0, 1, size=3000) &lt; sal_prob\ndep_prob = np.clip((35 - df['Tenure']) / 35, 0, 1)\ndep_mask = np.random.uniform(0, 1, size=3000) &lt; dep_prob\nten_prob = np.where(df['Location'] == 'Hybrid', 0.6, 0.1) \nten_mask = np.random.uniform(0, 1, size=3000) &lt; ten_prob\n# Create dataset with missing values\ndf_missing = df.copy()\ndf_missing.loc[sal_mask, 'Salary'] = np.nan\ndf_missing.loc[dep_mask, 'Department'] = np.nan\ndf_missing.loc[ten_mask, 'Tenure'] = np.nan\nmissing_vals = df_missing.isnull()\n\nProbability of Salary being missing increases as Age increases\nProbability of Department being missing increases as Tenure decreases\nProbability of Tenure being missing is greater if Location is Hybrid\n\ndf_missing.isna().sum()\n\nID               0\nAge              0\nSalary         785\nDepartment    1573\nLocation         0\nTenure        1052\ndtype: int64\n\n\nisna().sum() is a good place to start when looking at missing data. It provides the sum of missing data by column.\nInspecting Missingness: Age vs Salary\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(10, 6))\nsns.scatterplot(data=df_missing, x='Age', y='Salary', alpha=0.6, color='blue')\nplt.title(\"MAR Missing Pattern: Income Missing More for Older Individuals\")\nplt.show()\n\n\n\n\n\n\n\n\nWe can see that there is a trend of more missing values for Salary as Age increases.\nInspecting Missingness: Tenure vs Department\n\ndf_missing['dep_missing'] = df_missing['Department'].isnull().astype(int)\n\n# Bin Tenure for easier visualization\ndf_missing['tenure_binned'] = pd.cut(df['Tenure'], bins=[0, 5, 10, 20, 30, 35], \n                             labels=['0-5', '6-10', '11-20', '21-30', '30+'], \n                             include_lowest=True)\n# Bar plot to show % of missingness per tenure bin\nplt.figure(figsize=(8, 5))\nsns.barplot(x=df_missing['tenure_binned'], y=df_missing['dep_missing'], estimator=np.mean, palette=\"coolwarm\")\nplt.xlabel(\"Tenure (Binned)\")\nplt.ylabel(\"Proportion of Department Missing\")\nplt.title(\"Proportion of Missing Department Values by Tenure\")\nplt.show()\ndf_missing.drop(columns=['dep_missing','tenure_binned'],inplace= True)\n\n/var/folders/cq/5ysgnwfn7c3g0h46xyzvpj800000gn/T/ipykernel_90324/955409734.py:9: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n\n\n\n\n\n\n\n\n\nThere are more missing values for Department as Tenure decreases.\nInspecting Missingness: Location vs Tenure\n\nplt.figure(figsize=(8, 5))\ndf_missing['missingflag_ten'] = df_missing['Tenure'].isnull()\nfrom sklearn.impute import MissingIndicator\nindicator = MissingIndicator(missing_values=np.nan, features='all')\nmask_all = indicator.fit_transform(df_missing)\n# Plot scatter with missingness indicated by color\nplt.figure(figsize=(8, 5))\nsns.countplot(data=df_missing, x='Location', hue=df_missing['missingflag_ten'], palette={True: 'red', False: 'blue'})\ndf_missing.drop(columns = ['missingflag_ten'], inplace = True)\n# Customize the plot\nplt.legend()\nplt.title(\"Missing Tenure by Location: Hybrid vs Remote\")\nplt.xlabel(\"Location\")\nplt.ylabel(\"Count\")\nplt.show()\n\n&lt;Figure size 768x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\nMore missing values for Tenure when Location is Hybrid.\nWhen Is Imputing a Good Idea?\nImputing might be the right decision if:\n\nIt makes sense in the context of the data. Sometimes missing values are expected.\nIt wouldn’t introduce extra bias. Depends on the kind of missing data.\nDeleting would result in losing important data.\nThe computational cost isn’t too great.\nThe missing data is greater than 5 percent of the dataset.\n\n\n\n5.7.3 Methods of Imputation\n\n5.7.3.1 Simple Methods\n\nMean\nMedian\nMode\n\nThese can be a good initial option but will underestimate error.\nFast and easy computationally.\nWill mess up visuals. There will be large clumps around the mean/median/mode.\n\n\nMean/Mode Imputation\nFor simplicity, we will exclusively be using functions from sklearn throughout. Specifically sklearn.impute.\n\n# importing and initialize imputations\nfrom sklearn.impute import SimpleImputer\nimp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\nimp_mode = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n\n## making a copy dataframe with missing values for imputation use\nmean_df = df_missing.copy()\n\n# imputing numeric and categorical features separately\nmean_df[['Salary', 'Tenure']] = imp_mean.fit_transform(mean_df[['Salary', 'Tenure']])\nmean_df[['Department']] = imp_mode.fit_transform(mean_df[['Department']])\n\nRealistically, we don’t need to use these functions. We could manually impute the mean or mode.\nEvaluating Mean/Mode Imputation:\n\n# calculating rmse\nfrom sklearn.metrics import mean_squared_error\nrmse_salary_simple = np.sqrt(mean_squared_error(df['Salary'], mean_df['Salary']))\nrmse_tenure_simple = np.sqrt(mean_squared_error(df['Tenure'], mean_df['Tenure']))\n\n# calculating accuracy\ncorrect_imputations = (df[missing_vals]['Department'] == mean_df[missing_vals]['Department']).sum()\ntotal_imputed = missing_vals['Department'].sum()\ncategorical_accuracy = correct_imputations / total_imputed\n\nprint(f'Tenure Error: {rmse_tenure_simple} years')\nprint(f'Salary Error: ${rmse_salary_simple}')\nprint(f'Department Accuracy: {categorical_accuracy}')\n\nTenure Error: 6.0706196465053335 years\nSalary Error: $10658.00027447137\nDepartment Accuracy: 0.24284806102987921\n\n\nFor evaluating accuracy of imputations, we will be using mean squared error for continuous variables and proportion of correct imputations for categorical data.\nKNN Imputation\nOne of the most popular methods of imputation. Finds \\(k\\) nearest neighbors that don’t have missing values for the given column. Uses the mean of these values to impute missing value.\n\nUses Euclidean distance to find nearest neighbors\nBecomes computationally expensive for higher values of \\(k\\)\nCan be used for MCAR and MAR\n\nKNN Imputation Example:\n\n# importing and initializing imputer\nfrom sklearn.impute import KNNImputer\nfrom sklearn.preprocessing import OrdinalEncoder\nknn_df = df_missing.copy()\n\n# encoding the categorical column (doesn't work on strings)\nencoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=np.nan)\nknn_df[['Department','Location']] = encoder.fit_transform(knn_df[['Department','Location']])\n\n# imputing the dataFrame using k = 8\nimputer = KNNImputer(n_neighbors=8, weights=\"uniform\")\nknn_df_imputed = imputer.fit_transform(knn_df)\n# converting back to DataFrame\nknn_df = pd.DataFrame(knn_df_imputed, columns=knn_df.columns)\n# rounding the encoded column to integers (sometimes knn doesn't return # ints)\nknn_df['Department'] = np.round(knn_df['Department']).astype(int)\n\n# reverse encoding back to strings for accuracy evaluation\nknn_df[['Department','Location']] = encoder.inverse_transform(knn_df[['Department','Location']])\n\n\nThis KNN Imputer only works with numeric data, so we encoded the categorical features.\nThe number of neighbors, \\(k\\) can be tuned. In this case we choose \\(k\\) = 8. Typically 5-10 is a good number.\n\nKNN Accuracy:\n\n# finding rmse for Salary and Tenure\nrmse_salary_knn = np.sqrt(mean_squared_error(df['Salary'], knn_df['Salary']))\nrmse_tenure_knn = np.sqrt(mean_squared_error(df['Tenure'], knn_df['Tenure']))\n\ncorrect_imputations = (df[missing_vals]['Department'] == knn_df[missing_vals]['Department']).sum()\ntotal_imputed = missing_vals['Department'].sum()\ncategorical_accuracy = correct_imputations / total_imputed\n\nprint(f'Tenure Error: {rmse_tenure_knn} years')\nprint(f'Salary Error: ${rmse_salary_knn}')\nprint(f'Department Accuracy: {categorical_accuracy}')\n\nTenure Error: 6.2255413084593165 years\nSalary Error: $11300.874134353271\nDepartment Accuracy: 0.2663699936427209\n\n\n\nThe root MSE for both Tenure and Salary increased\nDepartment accuracy increased slightly over the 1 in 4 random guessing that was used with mode imputation.\n\nProblems with Single Imputation\n\nAll these examples have been methods of single imputation, in which one value is imputed for each missing value.\nOnly having one value is understating the uncertainty of the true value and when we use single imputation, the imputed value will almost always be incorrect.\nSince we’re very uncertain about the true values, we should impute multiple times to correctly account for uncertainty.\n\n\n\n5.7.3.2 Multiple Imputation\nExecute another imputation technique (like KNN) \\(m\\) times on randomly sampled subsets that don’t have missing values for the given column. This creates a set of possible values instead of just one. We then perform statistical analysis on each of the \\(m\\) complete datasets and pool the results of the analysis.\n\nCan become computationally expensive for large datasets or large values of \\(m\\).\n\\(m\\) can take on any value. Common range is 20-100, but generally bigger is better.\n\nMultiple Imputation by Chained Equations (MICE)\n\nStep 1: A simple imputation, such as imputing the mean, is performed for every missing value in the dataset. These mean imputations can be thought of as “place holders.”\nStep 2: The “place holder” mean imputations for one variable (“var”) are set back to missing.\nStep 3: The observed values from the variable “var” in Step 2 are regressed on the other variables in the imputation model.\nStep 4: This process is repeated for each feature with missing values, then the entire process is repeated for a given number of iterations until the algorithm reaches convergence (imputed values yield similar results after each iteration).\nThis forms 1 imputed dataset. We repeat this process \\(m\\) times.\n\nUsing statsmodels.imputation.mice\n\nWe will be using the MICEData function from statsmodels.imputation. mice for multiple imputation.\nk_pmm: This is the size of the sample used to perform predictive mean matching. Predictive mean matching is similar to KNN in that it uses similar rows to impute.\nperturbation_method: Adds random noise to imputations to account for uncertainty. We use ‘gaussian’ for this because we are adding noise generated from a normal distribution.\n\nMI Example\n\nfrom statsmodels.imputation.mice import MICEData\nimport statsmodels.api as sm\nimport numpy as np\nfrom sklearn.preprocessing import OrdinalEncoder\n\n# copying data and encode categorical variables\ndf_mi = df_missing.copy()\nencoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=np.nan)\ndf_mi[['Department', 'Location']] = encoder.fit_transform(df_mi[['Department', 'Location']]).astype(int)\n\n# performing Multiple Imputation\nmice_data = MICEData(df_mi,k_pmm = 30, perturbation_method='gaussian')\nm = 20  # Number of imputed datasets\nimputed_datasets = []\nfor _ in range(m):\n    mice_data.update_all()  # Perform one imputation step\n    imputed_datasets.append(mice_data.data.copy())  # Store a copy of the imputed dataset\n\n/var/folders/cq/5ysgnwfn7c3g0h46xyzvpj800000gn/T/ipykernel_90324/3889701920.py:9: RuntimeWarning: invalid value encountered in cast\n\n\nPooling MI Results\n\nwe have multiple regression models formed from multiple imputed datasets, but we need one final model.\nTo achieve this, we pool the results using Rubin’s combination rules.\nFor parameter estimates, we simply take the average.\nFor within imputation variance, we take the average of variances.\nFor between imputation variance, we use the equation given by:\n\n\\(b = \\frac{1}{m - 1} \\sum_{i = 1}^{m} (\\theta_i - \\hat{\\theta})^2\\).\n\nFor pooled total variance of imputed datasets, we use the equation:\n\n\\(t = w + (1 + 1/m)*b\\),\nwhere \\(w\\) is the within imputation variance, \\(b\\) is the between imputation variance, \\(m\\) is the total number of imputed datasets, \\(\\theta\\) is a parameter estimate, and \\(\\hat{theta}\\) is our pooled parameter estimate.\nMI Results\nHere we fit a linear regression model to each of the 20 imputed dataframes.\n\n# fitting regression models on imputed datasets one at a time\nresults = [sm.OLS(df['Salary'], sm.add_constant(df[['Department', 'Location','Tenure']])).fit() for df in imputed_datasets]\n# pooling estimates using Rubin's Rules (shown above)\nparams = np.array([res.params for res in results])\nbse = np.array([res.bse for res in results])\n\npooled_means = params.mean(axis=0)\npooled_se = np.sqrt(bse.mean(axis=0) + ((1 + 1/m) * params.var(axis=0, ddof=1)))\nprint(\"Pooled Coefficients:\\n\", pooled_means)\nprint(\"\\nPooled Standard Errors:\\n\", pooled_se)\n\nPooled Coefficients:\n [ 1.48366491e-33 -7.17518361e-15  7.38865124e-34  2.46007132e-32]\n\nPooled Standard Errors:\n [5.26793854e-18 1.15848202e-08 3.71753761e-18 2.14508949e-17]\n\n\nThe first coefficient/se is for the intercept.\n\n\n\n5.7.4 Conclusions\n\nNot everything needs to be imputed, we need to consider the context of the data. If we suspect that the missing data is MNAR, we do not impute.\nNeed to weigh computational cost with accuracy. Multiple imputation can be difficult on large datasets.\nOverall, multiple imputation is a safe bet.\n\nImputing in R:\nThese examples were all done in python, because that is the language we’ve used for this class. However, R has much more sophisticated packages and functions for imputing. Specifically the mice package in R has plenty of options for multiple imputation, with a lot of documentation available.\nI would recommend checking this out!\n\n\n5.7.5 Further Readings\n\nCombining Multiple Imputations\nMultiple Imputation by Chained Equations\nstatsmodels.imputation.mice\n\n\n\n\n\nBansal, R. (2024). SQL using python.\n\n\nCafferky, B. (2019). Master using SQL with python: Lesson 1 - using SQL with pandas.\n\n\nPrzybyla, M. (2024). How to use SQL in python.\n\n\nW3Schools. (2025). Python MySQL.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Manipulation</span>"
    ]
  },
  {
    "objectID": "visualization.html",
    "href": "visualization.html",
    "title": "6  Data Visualization",
    "section": "",
    "text": "6.1 Handling Spatial Data with GeoPandas and gmplot\nThe following section was written by Thomas Schittina, a senior majoring in statistics and minoring in mathematics at the University of Connecticut.\nThis section focuses on how to manipulate and visualize spatial data in Python, with a particular focus on the packages GeoPandas and gmplot. We’ll start with GeoPandas and do the following:\nFor gmplot we will:",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "visualization.html#handling-spatial-data-with-geopandas-and-gmplot",
    "href": "visualization.html#handling-spatial-data-with-geopandas-and-gmplot",
    "title": "6  Data Visualization",
    "section": "",
    "text": "Cover the core concepts and functionalities\nWalkthrough an example using NYC shape data\n\n\n\nTalk about why you’ll need a Google Maps API key\nSee some of the different plotting functionalities\nWalkthrough an example using NYC shape data\n\n\n6.1.1 GeoPandas\n\n6.1.1.1 Introducing GeoPandas\nFounded in 2013, GeoPandas is an open-source extension of Pandas that adds support for geospatial data. GeoPandas is built around the GeoSeries and GeoDataFrame objects. Both are subclasses of the corresponding Pandas objects, so they should feel familiar to those who have used Pandas before.\n\n\n6.1.1.2 A Remark about Shapely\nThe package Shapely is a core dependency of GeoPandas that handles geometric operations. Each geometry (point, polygon, etc.) stored in a GeoDataFrame is a Shapely object, and GeoPandas internally calls Shapely methods to perform spatial analysis. You won’t often need to interact directly with Shapely when using GeoPandas. Still, you may want to familiarize yourself with its basic concepts.\nShapely Documentation can be found here.\n\n\n6.1.1.3 GeoSeries and GeoDataFrame\nGeoSeries:\n\nSimilar to Series, but should exclusively contain geometries\nGeoSeries.crs stores the Coordinate Reference System information\n\nGeoDataFrame:\n\nMay consist of both Series and GeoSeries\nMay contain several GeoSeries, but only one active geometry column\n\nGeometric operations will only apply to the active column\nAccessed and manipulated with GeoDataFrame.geometry\n\nOtherwise similar to a normal DataFrame\n\n\n\n\n6.1.2 Example with NYC MODZCTA Shapefile\nGiven a file containing geospatial data, geopandas.read_file() will detect the filetype and create a GeoDataFrame.\n\nimport geopandas as gpd\nimport os\n\n# get .shp from MODZCTA_Shapefile folder\nshapefile_path = None\nfor file in os.listdir('MODZCTA_Shapefile'):\n    if file.endswith(\".shp\"):\n        shapefile_path = os.path.join('MODZCTA_Shapefile', file)\n        break  # Use the first .shp file found\n\n# read in data\ngdf = gpd.read_file(shapefile_path)\n\ngdf.drop(columns=['label', 'zcta'], inplace=True)\n\ngdf.head()\n\n\n\n\n\n\n\n\nmodzcta\npop_est\ngeometry\n\n\n\n\n0\n10001\n23072.0\nPOLYGON ((-73.98774 40.74407, -73.98819 40.743...\n\n\n1\n10002\n74993.0\nPOLYGON ((-73.9975 40.71407, -73.99709 40.7146...\n\n\n2\n10003\n54682.0\nPOLYGON ((-73.98864 40.72293, -73.98876 40.722...\n\n\n3\n10026\n39363.0\nMULTIPOLYGON (((-73.96201 40.80551, -73.96007 ...\n\n\n4\n10004\n3028.0\nMULTIPOLYGON (((-74.00827 40.70772, -74.00937 ...\n\n\n\n\n\n\n\nIt’s very important to know which CRS your geospatial data is in. Operations involving distance or area require a projected CRS (using feet, meters, etc.). If a geographic CRS is used (degrees), the calculations will likely be wrong.\n\nprint(gdf.crs)\n\n# convert to projected CRS\ngdf = gdf.to_crs(epsg=3857)\n\nprint(gdf.crs)\n\nEPSG:4326\nEPSG:3857\n\n\nOriginally, the geometries were in EPSG 4326, which is measured by latitude and longitude. In order to work with the shape data, the CRS was converted to EPSG 3857, which uses meters.\nNow we can start working with the spatial data. First, let’s compute the area of each zip code and store it as a new column.\n\n# create column of areas\ngdf['area'] = gdf.area\ngdf.head(3)\n\n\n\n\n\n\n\n\nmodzcta\npop_est\ngeometry\narea\n\n\n\n\n0\n10001\n23072.0\nPOLYGON ((-8236278.03 4974664.364, -8236327.85...\n2.987592e+06\n\n\n1\n10002\n74993.0\nPOLYGON ((-8237364.444 4970258.308, -8237318.6...\n3.974361e+06\n\n\n2\n10003\n54682.0\nPOLYGON ((-8236377.258 4971559.548, -8236390.9...\n2.611531e+06\n\n\n\n\n\n\n\nOur active geometry column is the shape data for each zip code, so gdf.area() only acts on that column and ignores the others.\nLet’s also find the boundary of each zip code, as well as its geographic center.\n\n# create columns for boundary and centorid info\ngdf['boundary'] = gdf.boundary\ngdf['centroid'] = gdf.centroid\n\ngdf[['modzcta', 'boundary', 'centroid']].head(3)\n\n\n\n\n\n\n\n\nmodzcta\nboundary\ncentroid\n\n\n\n\n0\n10001\nLINESTRING (-8236278.03 4974664.364, -8236327....\nPOINT (-8237323.727 4975637.524)\n\n\n1\n10002\nLINESTRING (-8237364.444 4970258.308, -8237318...\nPOINT (-8236103.249 4970509.323)\n\n\n2\n10003\nLINESTRING (-8236377.258 4971559.548, -8236390...\nPOINT (-8236435.551 4972866.281)\n\n\n\n\n\n\n\nSuppose we want to find the distance between two centroids. The current active geometry column is the shape data. Run gdf.geometry = gdf['centroid'] to switch the active geometry.\n\n# switch active geometry to centroid info\ngdf.geometry = gdf['centroid']\n\nThen we can calculate the distance between the first two centroids with distance().\n\n# find distance between first two centroids\ngdf.geometry[0].distance(gdf.geometry[1])\n\n5271.432980923517\n\n\n\n6.1.2.1 Plotting with GeoPandas\nGeoPandas also includes some basic plotting functionality. Similar to Pandas, plot() will generate visuals using matplotlib.\n\n# plot NYC zip codes with color mapping by area\ngdf.geometry = gdf['geometry'] # must switch active geometry back first\ngdf.plot('area', legend=True)\n\n\n\n\n\n\n\n\nInteractive maps can also be generated using explore, but you will need to install optional dependencies. An alternative approach is the package gmplot, which we’ll discuss next. First though, here is a list of common GeoPandas methods we’ve not yet covered.\n\nto_file(): save GeoDataFrame to a geospatial file (.shp, .GEOjson, etc.)\nlength(): calculate the length of a geometry, useful for linestrings\ninstersects(): check if one geometry intersects with another\ncontains(): check if one geometry contains another\nbuffer(): create a buffer of specified size around a geometry\nequals(): check if the CRS of two objects is the same\nis_valid(): check for invalid geometries\n\n\n\n\n6.1.3 gmplot\n\n6.1.3.1 Google Maps API\nAn API key is not necessary to create visuals with gmplot, but it is highly recommended. Without a key, any generated output will be dimmed and have a watermark.\n\n\n\nExample with no API key\n\n\nThe process to create an API key is very simple. Go here and click on Get Started. It requires some credit card information, but you start on a free trial with $300 of credit. You will not be charged unless you select activate full account.\nThere are some configuration options you can set for your key. Google has many different APIs, but gmplot only requires the Maps Javascript API.\n\n\n6.1.3.2 Creating Plots with gmplot\ngmplot is designed to mimic matplotlib, so the syntax should feel similar. The class GoogleMapPlotter provides the core functionality of the package.\n\nimport gmplot\n\napikey = open('gmapKey.txt').read().strip() # read in API key\n\n# plot map centered at NYC with zoom = 11\ngmap = gmplot.GoogleMapPlotter(40.5665, -74.1697, 11, apikey=apikey)\n\nNote: To render the classnotes on your computer, you will need to create the text file gmapKey.txt and store your Google Maps API key there.\nThe arguments include:\n\nThe latitude and longitude of NYC\nThe level of zoom\nAPI key (even if it’s not used directly)\nmore optional arguments for further customization\n\n\n\n\n6.1.4 Making Maps with NYC Zip Code Data\nLet’s display the largest zip code by area in NYC.\n\ngdf = gdf.to_crs(epsg=4326) # convert CRS to plot by latitude and longitude\nlargest_zip = gdf['geometry'][gdf['area'].idxmax()] # returns Shapely POLYGON\n\ncoords = list(largest_zip.exterior.coords) # unpack boundary coordinates\nlats = [lat for lon, lat in coords]\nlons = [lon for lon, lat in coords]\n\n# plot shape of zip code\ngmap.polygon(lats, lons, face_color='green', edge_color='blue', edge_width=3)\n\n# gmap.draw('largest_zip.html')\n\nAfter creating the plot, gmap.draw('filename') saves it as an HTML file in the current working directory, unless another location is specified. In the classnotes, all outputs will be shown as a PNG image.\n\n\n\nLargest NYC Zip Code by area\n\n\nLet’s also plot the centriod of this zip code, and include a link to gmplot’s documentation (in the classnotes this link won’t work because the PNG is used).\n\ngdf.geometry = gdf['centroid'] # now working with new geometry column\ngdf = gdf.to_crs(epsg=4326) # convert CRS to plot by latitude and longitude\n\ncentroid = gdf['centroid'][gdf['area'].idxmax()] # returns Shapely POINT\n\n# plot the point with info window\ngmap.marker(centroid.y, centroid.x, title='Center of Zip Code',\n            info_window=\"&lt;a href='https://github.com/gmplot/gmplot/wiki'&gt;gmplot docs&lt;/a&gt;\")\n\n# plot the polygon\ngmap.polygon(lats, lons, face_color='green', edge_color='blue', edge_width=3)\n\n# gmap.draw('zip_w_marker.html')\n\nHere’s the output:\n\n\n\nCenter of largest NYC Zip Code\n\n\n\n6.1.4.1 Other Features of gmplot\n\ndirections(): draw directions from one point to another\nscatter(): plot a collection of points\nheatmap(): plot a heatmap\nenable_marker_dropping(): click on map to create/remove markers\nfrom_geocode(): use name of location instead of coordinates\nsee docs for more\n\nYou can also change the map type when you create an instance of GoogleMapPlotter.\n\n# create hybrid type map\ngmap = gmplot.GoogleMapPlotter(40.776676, -73.971321, 11.5, apikey=apikey,\n                               map_type='hybrid')\n\n# gmap.draw('nyc_hybrid.html')\n\n\n\n\nHybrid map of NYC\n\n\n\n\n\n6.1.5 Summary\nGeopandas is a powerful tool for handling spatial data and operations. It builds on regular Pandas by introducing two new data structures, the GeoSeries and GeoDataFrame. Under the hood, Shapely handles geometric operations.\nThe package gmplot is a simple yet dynamic tool that overlays spatial data onto interactive Google maps. It does so through the class GoogleMapPlotter, which offers an alternative to Geopandas’ built in graphing methods for simple plots.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "visualization.html#google-maps-visualizations-with-folium",
    "href": "visualization.html#google-maps-visualizations-with-folium",
    "title": "6  Data Visualization",
    "section": "6.2 Google Maps visualizations with Folium",
    "text": "6.2 Google Maps visualizations with Folium\nThis section was created by Vlad Lagutin. I am a sophomore majoring in Statistical Data Science at the University of Connecticut.\nHere I introduce one more library for geospatial visualizations, in addition to GeoPandas and gmplot libraries described in the previous section.\n\n6.2.1 Folium and its features\n\nFolium is a Python library used to create interactive maps\nIt is built on top of Leaflet.js, an open-source JavaScript library for interactive maps\nManipulate your data in Python, visualize it in a Leaflet map with Folium\nEasily compatible with Pandas and Geopandas in Python\nSupports interactive features such as popups, zoom and tooltips\nAble to export maps to HTML\n\n\n\n6.2.2 Initializing Maps and Tile Layers\nThis is how simple map is created. It is often useful to provide arguments like location and zoom_start for convenience:\nlocation - location where map is initialized\nzoom_start - specifies starting zoom\n\nimport folium\n\nm = folium.Map(location=[38.8, -106.54], \n               zoom_start=4)\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nWe can add various Tile Layers to modify how our base map looks like: built-in folium styles, as well as many other tiles provided by Leaflet can be found here.\n\nm = folium.Map(location=[50, -100], zoom_start=4)\n\n# not built-in layer; add the link here\nfolium.TileLayer('https://{s}.tile.opentopomap.org/{z}/{x}/{y}.png', \n                 name='OpenTopoMap', attr='OpenTopoMap').add_to(m)\n\n# built-in layers\nfolium.TileLayer('CartoDB Positron', name='Positron', \nattr='cartodb positron').add_to(m)\n\nfolium.TileLayer('CartoDB Voyager', name='Voyager', \nattr='Voyager').add_to(m)\n\n# to be able to use them, add Layer Control\nfolium.LayerControl().add_to(m)\n\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nAfter adding LayerControl, we can switch these layers using control in the top right corner of the produced map.\n\n6.2.2.1 Geojson files\nWith Geojson files, we can visualize the borders of counties or states inside of them. These GeoJson files can be found online.\n\nm = folium.Map(location=[50, -100], zoom_start=4)\n\nfolium.GeoJson('data/us_states.json', name=\"USA\").add_to(m)\n\nfolium.GeoJson('data/canada_provinces.json', name=\"Canada\").add_to(m)\n\nfolium.LayerControl().add_to(m)\n\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\n6.2.2.2 Styling\nWe can style these geojson objects. Useful parameters:\n\ncolor - color of line stroke\nweight - line stroke width\nopacity - opacity of line strokes\nfillcolor - filling color of regions\nfillOpacity - opacity of regions\n\n\n# initialize styling dictionary\nstyle = {'color': 'black', 'weight': 1,\n                'fillColor': 'purple'}  \n\n\nm = folium.Map(location=[50, -100], zoom_start=4)\n\n# pass styling dictinary to a special argument \"style_function\"\nfolium.GeoJson('data/us_states.json', name=\"USA\",\n               style_function= lambda x: style).add_to(m)\n\nfolium.GeoJson('data/canada_provinces.json', name=\"Canada\",\n               style_function= lambda x: style).add_to(m)\n\n\nfolium.LayerControl().add_to(m)\n\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\n\n6.2.3 Markers\nIt is possible for user to label certain locations on the map with various types of markers. Folium provides several types of them.\n\n6.2.3.1 Circle Markers\nAs one can understand from the title, these are just circles.\nThere are two types of circle markers:\nfolium.Circle - has radius in meters\nfolium.CircleMarker - has radius in pixels\n\nm = folium.Map(location=[38.8974579,-77.0376094], \n               zoom_start=13.7)\n\n# Radius in meters\nfolium.Circle(location=[38.89766472658641, -77.03654034831065],\nradius=100).add_to(m)\n\n\n# Circle marker has radius in pixels\nfolium.CircleMarker(location=[38.88946075081255, -77.03528690318743],\nradius=50).add_to(m)\n\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nAs you can see, the marker around the Washington monument increases while zooming out, and the marker around White House remains the same.\n\n\n6.2.3.2 Styling for Circles\nWe can style circles as well, here are some important parameters:\n\nstroke - set to True to enable line stroke, default is True\nweight - line stroke width in pixels, default is 5\ncolor - line stroke color\nopacity - line stroke opacity\nfill - set to True to enable filling with color, default is False\nfill_color - fill Color\nfill_opacity - ranges between 0 to 1. 0 means transparent, 1 means opaque\n\nMoreover, we can also add\n\ntooltip - a label that appears when you put your cursor over an element\npopup - a box with info that appears when you click on element\n\n\nm = folium.Map(location=[38.8974579,-77.0376094], \n               zoom_start=13.7)\n\n\n\n# Radius in meters\nfolium.Circle(radius=100, location=[38.89766472658641, -77.03654034831065],\n              color='black', \n              fill=True,\n              fill_opacity=0.7,\n              tooltip=\"White House\",\n              # can also just write string popup; use html\n              popup=folium.Popup(\"\"\"&lt;h2&gt;The White House&lt;/h2&gt;&lt;br/&gt;  \n              &lt;img src=\"https://cdn.britannica.com/43/93843-050-A1F1B668/White-House-Washington-DC.jpg\" \n                                 alt=\"Trulli\" style=\"max-width:100%;max-height:100%\"&gt;\"\"\", max_width=500)\n              ).add_to(m)\n\n\n\n\n\n# Circle marker has radius in pixels\nfolium.CircleMarker(radius=50, location=[38.88946075081255, -77.03528690318743],\n              color='purple', \n              fill=True,\n              tooltip=\"Washington monument\",\n              popup=folium.Popup(\"\"\"&lt;h2&gt;The Washington monument&lt;/h2&gt;&lt;br/&gt;  \n              &lt;img src=\"https://www.trolleytours.com/wp-content/uploads/2016/06/washington-monument.jpg\" \n                                 alt=\"Trulli\" style=\"max-width:100%;max-height:100%\"&gt;\"\"\", max_width=500)\n                                ).add_to(m)\n\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\n6.2.3.3 Markers\nIn addition to circles, we can add just Markers:\n\nm = folium.Map(location=[39.8584824090568, -99.63735509074904],\n               zoom_start= 4)\n\n\nfolium.Marker(location=[43.88284841471961, -85.43121849839345]\n              ).add_to(m)\n\nfolium.Marker(location=[42.97269745752499, -98.88739407603738]\n              ).add_to(m)\n\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\n6.2.3.4 Styling for Markers\nHere we can use icon parameter to change the icon of a marker.\nIcon names for glyphicons by bootstrapcan be found here\nIcon names by fontawesome can be found here, need to add prefix='fa'\n\nm = folium.Map(location=[39.8584824090568, -99.63735509074904],\n               zoom_start= 4)\n\n\nfolium.Marker(\n    location=[43.88284841471961, -85.43121849839345],\n    tooltip='See location',\n    popup='*location*',\n    icon=folium.Icon(icon='glyphicon-volume-off', color='red')\n).add_to(m)\n\n\nfolium.Marker(\n    location=[42.97269745752499, -98.88739407603738],\n    tooltip=\"See location\",\n    popup=\"*location*\",\n    icon=folium.Icon(icon='fa-cube', prefix='fa', color='green')\n).add_to(m)\n\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\n\n6.2.4 Grouping\nWe can create groups of Markers, choosing whether we want to show them or not\n\nm = folium.Map(location=[39.8584824090568, -99.63735509074904],\n               zoom_start= 4)\n\n\n# adding group 1\ngroup_1 = folium.FeatureGroup(\"first group\").add_to(m)\n\nfolium.Marker(location=(37.17403654771468, -96.90854476924225), \n              icon=folium.Icon(\"red\")\n              ).add_to(group_1)\n\n\nfolium.Marker(location=[43.88284841471961, -85.43121849839345]\n              ).add_to(m)\n\n\n\n# adding group 2\ngroup_2 = folium.FeatureGroup(\"second group\").add_to(m)\n\n\nfolium.Marker(location=(42.53679960949629, -110.16683522968691), \n              icon=folium.Icon(\"green\")\n              ).add_to(group_2)\n\nfolium.Marker(location=[42.97269745752499, -98.88739407603738]\n              ).add_to(m)\n\n\n\nfolium.LayerControl().add_to(m)\n\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nUsing Layer Control on the top right, we can turn on and off these groups of Markers.\nHowever, two blue markers were not added to any of the groups but to the map directly, so we cannot hide them.\n\n\n6.2.5 Drawing different shapes on a map\nWe can draw different shapes like rectangles, lines, and polygons. Styling works the same as it does for circles\n\n6.2.5.1 Rectangle\nFor a rectangle, we just need two diagonal points.\nWe can draw it, for example, around a Wyoming state, since it has a rectangular form:\n\nm = folium.Map(location=[39.8584824090568, -99.63735509074904],\n               zoom_start=4)\n\n\n# for rectangle, we need only 2 diagonal points\nfolium.Rectangle([(45.0378, -111.0328), \n                  (41.0734, -104.0689)],\n                  color='purple',\n                  fill=True,\n                  tooltip='see the name',\n                  popup=\"Wyoming state\",\n                  fill_color='blue').add_to(m)\n\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\n6.2.5.2 Polygon\nHowever, for states like Nevada, which are not of a rectangular form, we can use Polygon:\n\npolygon_coords = [(42.0247, -120.0016),\n                (42.0106, -114.0776),\n                (36.1581, -114.0157),\n                (36.1220, -114.6994),\n                (35.0721, -114.7066),\n                (39.0379, -120.0695)\n]\n\n\nfolium.Polygon(polygon_coords,\n                color='purple',\n                fill=True,\n                tooltip='see the name',\n                popup=\"Nevada state\",\n                fill_color='blue').add_to(m)\n\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\n6.2.5.3 PolyLine\nIt is also possible to just create lines;\nThe only diffence between Polygon and PolyLine is that Polygon connects the first point to the last and PolyLine does not\n\npolyline_coords = [(34.9614, -108.2743),\n                 (38.5229, -112.7751),\n                 (42.9696, -112.9947),\n                 (45.9843, -118.5384)\n]\n\n\n\nfolium.PolyLine(polyline_coords,\n                 color='red',\n                 ).add_to(m)\n\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\n6.2.5.4 Draw it yourself\nIs it possible to draw these shapes ourselves, we just need to import Draw plugin:\n\nfrom folium.plugins import Draw\n\n# add export button, allowing to save as geojson file\nDraw(export=True).add_to(m)\n\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nTo draw, use tools on a panel on the left.\n\n\n\n6.2.6 Heatmap\nWe can also create simple HeatMaps:\nArguments:\n\ndata (list of points of the form [lat, lng] or [lat, lng, weight]) – The points you want to plot. You can also provide a numpy.array of shape (n,2) or (n,3). Ideally, the weight should be between 0 and 1.\nname (default None) – The name of the Layer, as it will appear in LayerControls\nmin_opacity (default 1.) – The minimum opacity the heat will start at\nradius (default 25) – Radius of each “point” of the heatmap\nblur (default 15) – Amount of blur\n\n\nimport numpy as np\nfrom folium.plugins import HeatMap\n\n\nm = folium.Map(location=[40.71, -74],\n               zoom_start=10)\n\n\ndata = (\n    np.random.normal(size=(100, 2)) * np.array([[0.1, 0.1]]) + \n    np.array([[40.7128, -74.0060]])\n    ).tolist()\n\nHeatMap(data).add_to(m)\n\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\n6.2.7 Demonstration\nHere is the demonstration with flood data from Midterm project:\n\nimport pandas as pd\n\nflood_df = pd.read_csv(\"data/nycflood2024.csv\", \n                       parse_dates=[\"Created Date\", \"Closed Date\"])\n\nflood_df.columns = flood_df.columns.str.replace(\" \", \"_\").str.lower()\n\n\n# some manipulation/cleaning\nflood_df = flood_df[~flood_df['location'].isna()]\n\nflood_df[\"response_time\"] = (flood_df[\"closed_date\"] - \n        flood_df[\"created_date\"]).dt.total_seconds() / 3600\n\n\nflood_df = flood_df[~flood_df['response_time'].isna()]\n\n\nmean = flood_df['response_time'].mean()\nstd = flood_df['response_time'].std()\n\n# standardize response time\nflood_df['z_score'] = (flood_df['response_time'] - mean) / std\n\n\n\n\nm = folium.Map(location=[40.7128, -74], zoom_start=12,\n               tiles='cartodb dark_matter',\n               world_copy_jump=True)\n\n\n# styling dictionary\nstyle = {'color': 'orange', 'weight': 1,\n                'fillColor': 'white'}  \n\nfolium.GeoJson(\"data/new-york-city-boroughs.json\", name='Borough borders',\nstyle_function=lambda x: style).add_to(m)\n\n\n\n# create 4 groups: 2 for usual points, 2 for outliers\noutlier_flood = folium.FeatureGroup('Flooding outliers').add_to(m)\nnormal_flood = folium.FeatureGroup('Flooding complaints').add_to(m)\noutlier_CB = folium.FeatureGroup('Catch Basin outliers').add_to(m)\nnormal_CB = folium.FeatureGroup('Catch Basin complaints').add_to(m)\n\n\n# reducing number of points, so the code cell can run\nflood_df = flood_df[:3500]\n\n\nfor _, row in flood_df.iterrows():\n    \n    if row[\"descriptor\"] == 'Street Flooding (SJ)':\n        if row['z_score'] &gt; 3 or row['z_score'] &lt; -3:\n            # visualize SF outliers\n            folium.Marker(location=[row.latitude, row.longitude],\n                          popup=\"Unsusually large response time\", \n                          tooltip='SF outlier',\n                          icon=folium.Icon(icon='glyphicon-minus-sign',\n                                           color='red')).add_to(outlier_flood)\n        else:\n            # ordinary SF locations\n            folium.Circle(radius=1, \n                location=[row.latitude, row.longitude],\n                color=\"red\",\n                tooltip='SF complaint',\n                popup=\"Normal SF complaint\",\n                ).add_to(normal_flood)\n            \n    else:\n        # visualize CB outliers\n        if row['z_score'] &gt; 3 or row['z_score'] &lt; -3:\n            folium.Marker(location=[row.latitude, row.longitude],\n                          tooltip='CB outlier',\n                          popup=\"Unsusually large response time\", \n                          icon=folium.Icon(icon='fa-exclamation-triangle',\n                                           prefix='fa', \n                                           color='orange')).add_to(outlier_CB)\n        else:\n            # ordinary CB locations\n            folium.Circle(radius=1,\n                location=[row.latitude, row.longitude],\n                color=\"blue\",\n                tooltip='CB complaint',\n                popup=\"Normal CB complaint\", \n                ).add_to(normal_CB)\n        \n\n\n\nfolium.LayerControl().add_to(m)\n\nm\n\n/var/folders/cq/5ysgnwfn7c3g0h46xyzvpj800000gn/T/ipykernel_10379/2407777587.py:3: UserWarning:\n\nCould not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\n6.2.8 Further Reading\n\nFolium documentation\nLeaflet",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "visualization.html#math-animation",
    "href": "visualization.html#math-animation",
    "title": "6  Data Visualization",
    "section": "6.3 Math Animation",
    "text": "6.3 Math Animation\nThis section was contributed by Mohammad Mundiwala. I am a first year graduate student in Mechanical Engineering. My research interests are in sustainable design and machine learning. I enjoy to make short videos on youtube which is partly why I chose this topic.\n\n6.3.1 A Simple Example\n\n\n\n\n\n6.3.2 3blue1Brown \nOne of my favorite youtubers is actually the creator of the manim package that we are going to explore. He is one of the largest STEM educators on the internet and at the time of writing this has amassed 650 million views on YouTube alone. What sets his videos apart are his captivating, precise, and dynamic animations. Because there were no available tools that could help visualize the abstract concepts he wanted to share, he specifically curated manim to be able to do just that.\n\n\n6.3.3 manim vs matplotlib.animation\nA first choice for many animation tasks is and very well should be matplotlib as it is familiar and powerful. It is useful then to clarify when an even more powerful package such as manim should be used.\nScene-direction approach vs Frame-by-frame approach\nPrecise control of objects vs Quick iterative plotting\nElegant mathematical animations vs Straightforward integration\nWhen using manim, you become the director of a movie. You have control of every single detail in the frame which is exciting and daunting. If your goal is as simple as making many static plots (frames) and stitching them together to create a GIF or animation, then manim is overkill. Even the creator of manim would reccommend you use Desmos or GeoGebra instead. I love these options for their clean UI and speed. If you cannot get it done with these tools, then you may be in manim territory.\nIn manim, everything you see in a rendered output video is called an object. In my simple example video above, the circle, square, triangle, etc are objects with properties like color, size, position, etc. The camera itself is also an object which you can control preciseley.\nTo make the elegant animations with manim on your computer, simply follow the Manim Setup Guide. There are a few steps that may take some time to download but it is otherwise a pretty simple process - especially since we are all very familiar with the command line interface.\n\n\n6.3.4 Manim Workflow\nEach animation that I made using manim can be boiled down to the following five simple steps.\n\nDefine a scene class (e.g., Scene, ThreeDScene, ZoomedScene)\nCreate shapes or objects (circles, squares, axis)\nApply transformations or animations (.animate, MoveTo)\nCustomize text, labels, colors, to clarify whats going on\nRender the scene with “manim -plq script.py Scene_Name”\n\nManim is great example of Object Oriented Programming (OOP) because almost everything you might want to do is done by calling a method on some object that you made an instance of. A “scene” is just a class that you write in python where you can specify the type of scene in the argument. The way you interact with the different scene options (presented above: Scene or ThreeDScene) will be the same. There are different methods (actions) that you might want to do in a 3D scene vs a 2D scene. For example, in 3D, you can control the angle of the lighting, the angle and position of the camera, and the orientation of objects in 3D space. It would not make sense to do this in 2D, for example.\nEvery basic shape or symbol you want to use in your animation in likely an existing object you can simply call. If you want to animate your own symbol, or custom icon then you can simply read it in as an image, and treat it like an object (where applicable). Note that all objects are automatically placed at the center of the screen (the origin). If you want to place a circle, for instance, to the left or right of the page, you can use circle.move_to(LEFT). Similarly, if you want to go up, down, or right, use UP, DOWN, and RIGHT, respectively. These act kind of like unit vectors in their directions. Multiply a number by them to move further in that direction.\nNext is the fun part. You can apply the movements and transformations that you would like to which ever object you want. If you want to transform a square to a cirlce, then use Transform(square, circle). To make the transformation actually happen within your scene, you have to call the scene object that you defined. That is as simple as writing self.play(&lt;insert animation here&gt;). One useful argument to the .play() method which you use for every single animation is run_time which helps you define exactly how long, in seconds, the animation you defined will last. The default is 1 second which may be too quick for some ideas.\nYou can add labels, text, Latex math, etc. to make your animations more complete. Each text block you write, with MathTex() for the latex fans, is also treated like an object within the scene. You can control its font, font size, color, position; you may also link its position relative to another object.\nFinally you will use the command line or terminal of your choice to render your animation. I must admit that there is some upfront effort required to make your system and virtual environment ready to run a manim script. Again, there is nothing else you should need between the Manim Community Documentation and ChatGPT, but still, it took me many hours to get all of the dependencies and packages to be installed correctly. If you already have a latex rendering package and video rendering package installed that is added to your PATH, then it may take less time. I want to simply clarify that manim is NOT a single pip install command away, but that is not to discourage anyone. I did it and feel it is worth the extra effort.\n\n\n6.3.5 Code for intro\nThe code for the intro animation is presented below. It is simple and aims at just getting the primary functionality and commands of manim across. It follows the manim workflow described in the previous section.\n\nfrom manim import *\nimport numpy as np\nclass TransformCycle(Scene):\n    def construct(self):\n        # circle -&gt; square -&gt; triangle -&gt; star -&gt; circle\n        circle1 = Circle()\n        square = Square()\n        triangle = Triangle()\n        star = Star()\n        circle2 = Circle()\n\n        shapes = [circle1, square, triangle, star, circle2]\n        current = shapes[0]\n        self.add(current)\n        for shape in shapes[1:]:\n            self.play(Transform(current, shape), run_time=1.25)\n\n\n\n6.3.6 Using \\(\\LaTeX\\)\nPart of what makes manim so useful for math is its natural use of Latex when writing text. Other animation softwares do not support precise math notation using Latex which often used by experts. In the following example, I show a very simple demonstration on how text using Latex and related objects (ellipse) can be positioned in the frame. First notice the different animations that are happening; then we can explore the code!\n\n\n\nThe code used to generate the video is presented below. It should be easy(ish) to follow since we use the same format as the previous animation. This time I included more arguements to the objects (position, color, width, height, etc).\n\nclass ProbabilityEvents(Scene):\n    def construct(self):\n        main_group = VGroup()\n        ellipse1 = Ellipse(\n          width=3, height=2.5, color=BLUE, fill_opacity=0.3\n                          ).move_to(RIGHT*2.5 + UP)\n        ellipse2 = ellipse1.copy().set_color(color=RED).move_to(RIGHT*2.5+DOWN)\n        intersection = Intersection(\n            ellipse1, ellipse2, fill_color=PURPLE, fill_opacity=0.7)\n\n        prE1 = MathTex(r\"\\Pr(E_1)\").scale(0.8).move_to(2* LEFT + UP)\n        prE2 = MathTex(r\"\\Pr(E_2)\").scale(0.8).next_to(\n          prE1, DOWN, buff=0.4).move_to(2*LEFT + DOWN) \n        prUnion = MathTex(r\"\\Pr(E_1 \\cup E_2)\").scale(0.8).next_to(\n          prE2, UP*0.5, buff=0.6).move_to(2 *LEFT) \n        prFormula = MathTex(\n          r\"\\Pr(E_1) + \\Pr(E_2) - \\Pr(E_1 \\cap E_2)\").scale(0.8) \n        prFormula.next_to(prUnion, buff=0.4).move_to(1.5*LEFT)   \n\nUsing the defined objects from above, below are the commands that actually animate. This is where you can be creative! The animation occur sequentially. When multiple animations are listed in the self.play() method, they begin at the same time.\n\n        self.play(FadeIn(prE1), FadeIn(ellipse1), run_time = 2)\n        self.play(FadeIn(prE2), FadeIn(ellipse2))\n        self.play(FadeIn(intersection))\n        self.play(FadeOut(prE1), FadeOut(prE2), FadeIn(prUnion))\n        self.wait(1)\n        self.play(Transform(prUnion, prFormula),\n                  intersection.animate.set_fill(color=WHITE, opacity=1),\n                  ellipse1.animate.set_fill(opacity=0.9),\n                    ellipse2.animate.set_fill(opacity=0.9),\n                    run_time=3) \n        self.wait()\n\nHow cool. Try and edit this animation I made slightly to see how different parameters effect the final result. I found that it was the best way to learn.\n\n\n6.3.7 Visualizing Support Vector Machine\nSome data is distributed in a 2 dimensional plane. Some samples are blue and some are white. When trying to seperate the white samples from blue (called binary classification), we see it is not possible when they are shown in 2D. We can draw a linear boundary where blue is on one side and white is on the other. My animation shows the utility of the Kernel Trick used by SVM. We apply a nonlinear transformation to increase the dimensionality of the feature space. Then, in the higher dimension, we search for a hyperplane that can split the two classes. Once that hyper plane is found, we can map that plane back into the feature space using the inverse of the non-linear transformation. For this animation, I used a parabaloid transformation to keep it simple and clear. The hyper plane intersects the parabaloid as a conic section - ellipse in 2D. The camera pans to top down view to show the feature space now segmented using the kernel trick. SVM is a highly abstract mathemtical tool that can be hard to imagine. I feel that in this low dimensional case (2D and 3D), we can convey the most important ideas without getting into the math.\n\n\n\nI split the movie shown above into sections or scenes below. Every single animation or motion presented above is written in these relatively short blocks of code.\n\n6.3.7.1 Axis\n\nclass Scatter(ThreeDScene):\n    def construct(self):\n        # camera\n        self.set_camera_orientation(phi=70*DEGREES, theta=-35*DEGREES)\n        X, y = make_dataset(50, 50, seed=0)\n        #   class‑0 → WHITE, class‑1 → BLUE\n        dot_rad = 0.02\n        colors   = {0: WHITE, 1: BLUE}\n        # axes\n        axes = ThreeDAxes(\n            x_range=[-4, 8, 1],\n            y_range=[-4, 8, 1],\n            z_range=[-0.5, 5, 1],\n            x_length=4,\n            y_length=4,\n            z_length=3,\n        )\n        self.add(axes)\n        self.wait(1)\n\n\n\n6.3.7.2 Kernel trick\nThe primary animation that shows the points in 2D being ‘transformed’ into 3D is a simple Replacement Transform in manim. The samples, represented by spheres, were given a starting position and ending position. Manim, automatically smoothly interpolates the transition from start to end, which renders as the points slowly rising upward. Pretty cool!\n\nbase_pts, lifted_pts = VGroup(), VGroup()\n  for (x, y), cls in zip(X, y):\n      p = axes.c2p(x, y, 0)\n      base_pts.add(Dot3D(p, radius=dot_rad, color=colors[int(cls)]))\n      z = (x - 2)**2 + (y - 2)**2 + 1\n      p = axes.c2p(x, y, z)          # or axes.coords_to_point\n      lifted_pts.add(Dot3D(p, radius=dot_rad, color=colors[int(cls)]))\n  self.play(FadeIn(base_pts), run_time=1)\n  self.wait(1)\n  step2_tex = MarkupText(\"2nd: Kernal trick with nonlinear mapping\", \n                          color=WHITE, font_size=24).move_to(UP*2 + RIGHT*2)\n  self.play(ReplacementTransform(base_pts, lifted_pts), \n              run_time=3)\n  self.wait()\n\n\n\n6.3.7.3 Hyper-plane\nThe hyperplane “searching” for the optimum boundary is performed by creating a square in 3D space. I then set its color and position. The show the rotation, I determined the final orienation of the plane in terms of its normal vector. I sweep a small range of nearby normal vectors to ultimately ‘animate’ the search or convergance of an SVM model. Note that there are many different ways I could have gone about accomplishing this. This was one way where I did the math by hand.\n\n  plane_size = 0.6\n  plane_mobj = Square(side_length=plane_size)\n  plane_mobj.set_fill(BLUE_A, opacity=0.5)\n  plane_mobj.set_stroke(width=0)\n  plane_mobj.move_to(axes.c2p(2, 2, 1.3))\n\n  # default Square is parallel to XY\n  self.add(plane_mobj)\n  self.wait(1)\n  # final orientation:\n  normal = np.array([A, B, C], dtype=float)\n  z_anchor = -(A*2 + B*2 + D) / C\n  anchor_pt = np.array([2, 2, z_anchor])\n  anchor_3d = axes.c2p(*anchor_pt)\n  z_hat = np.array([0, 0, 1])\n  n_hat = normal / np.linalg.norm(normal)\n  final_angle = angle_between_vectors(z_hat, n_hat)\n  # Rotation axis is their cross product:\n  rot_axis = np.cross(z_hat, n_hat)\n  if np.allclose(rot_axis, 0):\n      rot_axis = OUT \n  self.play(\n      plane_mobj.animate.rotate(\n        30*DEGREES, axis=RIGHT, about_point=plane_mobj.get_center()),\n      run_time=2\n  )\n  self.play(\n      plane_mobj.animate.rotate(\n        20*DEGREES, axis=UP, about_point=plane_mobj.get_center()),\n      run_time=2\n  )\n  # move & rotate to final plane\n  self.play(\n      plane_mobj.animate.move_to(anchor_3d).rotate(\n        final_angle, axis=rot_axis, about_point=anchor_3d),\n      run_time=3\n  )\n  self.wait()\n  self.play(FadeOut(plane_mobj), run_time=1)\n\n\n\n6.3.7.4 Camera & Ellipse\nThe camera in manim is operated using spherical coordinates. If you remember from Calc 3, \\(\\phi\\) describes the angle formed by the positive \\(z\\)-axis and the line segment connecting the origin to the point. Meanwhile, \\(\\theta\\) is the angle in the \\(x-y\\) plane, in reference to the \\(+x\\) axis. Manim may help you to brush up on your math skills..it certainly helped me!\n\nself.move_camera(phi=0*DEGREES, theta=-90*DEGREES, run_time=3)\nself.wait()\n\n#   ((x - 2)^2 / 0.55^2) + ((y - 2)^2 / 0.65^2) = 1.\ncenter_x, center_y = 2, 2.1\na, b = 0.55, 0.65  # ellipse semi-axes\ndef ellipse_param(t):\n    x = center_x + a * np.cos(t)\n    y = center_y + b * np.sin(t)\n    return axes.c2p(x, y, 0)  # z=0 =&gt; in the XY plane\n\nellipse = ParametricFunction(\n    ellipse_param, t_range=[0, TAU], color=YELLOW, stroke_width=2)\nself.play(Create(ellipse), run_time=2)\nself.wait(2)\n\n\n\n\n6.3.8 Learnings\n\nTakes time upfront, but allows you to convey abstract concepts quickly.\nYou can impress sponsors or your boss.\nIt is cool. I made this for the lab I work in using just an image (.svg) icon!\n\n\n\n\n\n\n6.3.9 Further readings\nYou should be able to find everything you need from the community documentation and there is no better gallery than 3Blue1Brown’s free online videos. With these (relatively) simple tools, he has made incredible animations for math. He also posts all of the code for each animation he makes; for all of the videos he has posted, the source code is in his GitHub repository.\nManim Community Documentation\n3Blue1Brown YouTube",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "stats.html",
    "href": "stats.html",
    "title": "7  Statistical Tests and Models",
    "section": "",
    "text": "7.1 An Overview of Statistical Testing in Python",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Statistical Tests and Models</span>"
    ]
  },
  {
    "objectID": "stats.html#an-overview-of-statistical-testing-in-python",
    "href": "stats.html#an-overview-of-statistical-testing-in-python",
    "title": "7  Statistical Tests and Models",
    "section": "",
    "text": "7.1.1 Introduction\nThis section was written by John Ackerman, a sophomore at the University of Connecticut double majoring in mathematics and statistical data science.\nBy the end of this section, you’ll understand how to approach problems of the following forms using Python:\n\nDoes the average height of male and female university students differ significantly?\n\nIs there a relationship between the type of movie a person sees and whether or not they buy snacks at the theater?\n\nCan we predict whether a patient has a disease based on their temperature and gender?\n\nHow can a company estimate customer satisfaction with a small sample of feedback?\n\n\n\n7.1.2 What’s Statistical Testing?\nStatistical testing is a method of making probabilistic decisions about a population parameter based on sample data. It involves formulating hypotheses and using data to assess the evidence against a null hypothesis.\n\n\n7.1.3 Null and Alternate Hypotheses\nThe null hypothesis (\\(H_o\\)) is the default assumption that there is no effect, no difference, or no relationship in the data. The alternative hypothesis (\\(H_1\\) or \\(H_a\\)) is what we seek to provide evidence for-it suggests that there is an effect, a difference, or a relationship. Hypothesis testing evaluates whether the observed data provides enough evidence to reject the null hypothesis in favor of the alternative.\nThe following table contains the null and alternative hypotheses for some common tests:\n\n\n\n\n\n\n\n\nTest\nNull Hypothesis (H₀)\nAlternative Hypothesis (H₁ or Hₐ)\n\n\n\n\nt-test (one-sample or two-sample)\nThe population mean(s) are equal.\nThe population mean(s) are different.\n\n\nChi-square test for independence\nTwo categorical variables are independent.\nTwo categorical variables are dependent.\n\n\nANOVA (Analysis of Variance)\nAll group means are equal.\nAt least one group mean is different.\n\n\nLinear Regression (F-test)\nThe independent variable(s) have no effect on the dependent variable.\nAt least one independent variable significantly affects the dependent variable.\n\n\nMann-Whitney U test\nThe distributions of the two groups are the same.\nThe distributions of the two groups are different.\n\n\n\n\n\n7.1.4 Assumptions\nStatistical tests are based on certain assumptions about the data, such as the distribution, variance, or independence of observations. Violating assumptions increases your risk of Type I errors (falsely rejecting a true null hypothesis) and Type II errors (failing to reject a false null hypothesis).\n\n\n\n\n\n\n\n\nAssumption\nMethod to Test\nPackage:Function\n\n\n\n\nNormality: (data is normally distributed)\nShapiro-Wilk test or Q-Q plot\nscipy.stats.shapiro (Shapiro-Wilk Test), statsmodels.api.qqplot (Q-Q Plot)\n\n\nIndependence: (observations are independent)\nDurbin-Watson test (for time series)\nstatsmodels.stats.stattools.durbin_watson (Durbin-Watson Test)\n\n\nHomoscedasticity: (equal variance across groups)\nLevene’s test\nscipy.stats.levene (Levene’s Test)\n\n\nHeteroscedasticity: (unequal variance)\nBreusch-Pagan test\nstatsmodels.stats.diagnostic.het_breuschpagan (Breusch-Pagan Test)\n\n\nNo Outliers: (no extreme values)\nIQR method or Z-score\nscipy.stats.iqr (Interquartile Range), scipy.stats.zscore (Z-score)\n\n\nNo Multicollinearity: (independent predictors in regression)\nVariance Inflation Factor (VIF)\nstatsmodels.stats.outliers_influence.variance_inflation_factor (VIF)\n\n\n\n\n\n7.1.5 Parametric vs. Non-Parametric\nParametric tests assume that the data follows a specific distribution (typically normal), and rely on parameters such as the mean and standard deviation. Non-parametric tests don’t assume any distribution and as such are used when parametric assumptions are violated. Both are important because parametric tests offer greater precision if assumptions hold, while non-parametric tests provide a robust alternative when they don’t.\nLet’s run a series of simulations to compare the accuracy of the independent t-test (parametric) vs the Wilcoxon test (non-parametric) with right-skewed samples with a known difference between groups:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\n# Set random seed\nnp.random.seed(124)\n\n# Simulation parameters\nn_simulations = 1000\nsample_size = 30\n\n# Arrays to store p-values\nt_test_pvalues = []\nwilcoxon_pvalues = []\n\n# Run simulations\nfor _ in range(n_simulations):\n    # Generate skewed data with a true difference\n    group1 = np.random.lognormal(mean=0, sigma=0.8, size=sample_size)\n    group2 = np.random.lognormal(mean=0.3, sigma=0.8, size=sample_size)  \n    \n    # Run both tests\n    _, t_pvalue = stats.ttest_ind(group1, group2)\n    _, w_pvalue = stats.mannwhitneyu(group1, group2)\n    \n    # Storing p-values\n    t_test_pvalues.append(t_pvalue)\n    wilcoxon_pvalues.append(w_pvalue)\n\n# Find prop of results w/ (p &lt; 0.05)\nt_test_significant = (\n  np.sum(np.array(t_test_pvalues) &lt; 0.05) / n_simulations\n)\nwilcoxon_significant = (\n  np.sum(np.array(wilcoxon_pvalues) &lt; 0.05) / n_simulations\n)\n\n# Plot histograms\nplt.figure(figsize=(8, 5))\n\nplt.subplot(1, 2, 1)\nplt.hist(t_test_pvalues, bins=20, color='blue')\nplt.axvline(x=0.05, color='red', linestyle='--')\nplt.ylim(None, 310)\nplt.title(f't-test\\nDetection Rate: {t_test_significant:.2f}')\nplt.xlabel('p-value')\nplt.ylabel('Frequency')\n\nplt.subplot(1, 2, 2)\nplt.hist(wilcoxon_pvalues, bins=20, color='green')\nplt.axvline(x=0.05, color='red', linestyle='--')\nplt.ylim(None, 310)\nplt.title(f'Wilcoxon Test\\nDetection Rate: {wilcoxon_significant:.2f}')\nplt.xlabel('p-value')\n\nplt.tight_layout()\nplt.show()\n\nprint(\n  f\"Proportion of significant t-test results: \"\n  f\"{t_test_significant:.2f}\"\n)\nprint(\n  f\"Proportion of significant Wilcoxon results: \"\n  f\"{wilcoxon_significant:.2f}\"\n)\nprint(\n  f\"The non-parametric Wilcoxon test correctly identifies the difference \"\n  f\"{wilcoxon_significant/t_test_significant:.2f}x more often\"\n)\n\n\n\n\n\n\n\n\nProportion of significant t-test results: 0.23\nProportion of significant Wilcoxon results: 0.30\nThe non-parametric Wilcoxon test correctly identifies the difference 1.28x more often\n\n\nThe Wilcoxon test correctly detected the difference more often because it didn’t assume the samples followed a normal distribution.\n\n\n7.1.6 Types of Statistical Testing\n\n7.1.6.1 Is There a Significant Difference? (A/B Testing)\nThese tests help determine whether the observed differences between groups are significant or just due to chance.\nEx: Does our new drug significantly lower blood pressure compared to placebo?\n\n7.1.6.1.1 Overview of Relevant Tests:\n\n\n\n\n\n\n\n\n\nTest\nUse Case\nKey Assumptions\nPackage:Function\n\n\n\n\nt-test\nCompare two means\nNormality, equal variance (for independent test)\nscipy.stats:ttest_ind\n\n\nMann-Whitney U\nCompare two groups (non-parametric)\nContinuous data, different distributions\nscipy.stats:mannwhitneyu\n\n\nANOVA\nCompare 3+ groups\nNormality, equal variance\nscipy.stats:f_oneway\n\n\nKruskal-Wallis\nCompare 3+ groups (non-parametric)\nSame shape distributions\nscipy.stats:kruskal\n\n\n\n\n\n7.1.6.1.2 t-test & Mann-Whitney U Python Implementation:\n\nfrom scipy.stats import ttest_ind, mannwhitneyu\n\ngroup_a = [5, 7, 8, 6, 9]\ngroup_b = [6, 8, 7, 10, 12]\n\n# If data is normally distributed:\nt_stat, p_value = ttest_ind(group_a, group_b)\nprint(f\"T-test: T={t_stat:.3f}, p={p_value:.3f}\")\n\n# If data is non-normal:\nu_stat, p_value = mannwhitneyu(group_a, group_b)\nprint(f\"Mann-Whitney U: U={u_stat:.3f}, p={p_value:.3f}\")\n\nT-test: T=-1.242, p=0.249\nMann-Whitney U: U=7.500, p=0.343\n\n\n\n\n7.1.6.1.3 Key Points:\nWhen choosing statistical tests, follow these guidelines:\n\nUse a t-test if the data is normal; otherwise, use the Mann-Whitney U test.\n\nFor three or more groups, use ANOVA (or Kruskal-Wallis if non-parametric).\n\nFor paired and one-sample t-tests, use SciPy’s ttest_rel and ttest_1samp.\n\n\n\n\n7.1.6.2 Are Two Variables Related?\nThese tests assess whether two variables are correlated or if their relationship is just due to random variation.\nEx: Is there a correlation between hours studied and test performance?\n\n7.1.6.2.1 Overview of Relevant Tests:\n\n\n\n\n\n\n\n\n\nTest\nUse Case\nKey Assumptions\nPackage:Function\n\n\n\n\nPearson correlation\nLinear relationship between variables\nBoth variables are continuous and normally distributed\nscipy.stats:pearsonr\n\n\nSpearman correlation\nMonotonic relationship between variables\nNo normality assumption\nscipy.stats:spearmanr\n\n\nChi-square test\nRelationship between two categorical variables\nExpected frequency &gt;5 in each category\nscipy.stats:chi2_contingency\n\n\n\n\n\n7.1.6.2.2 Pearson & Spearman Python Implementation\n\nfrom scipy.stats import pearsonr, spearmanr\n\nx = [10, 20, 30, 40, 50]\ny = [15, 25, 35, 45, 60]\n\n# Pearson correlation\ncorr, p_value = pearsonr(x, y)\nprint(f\"Pearson correlation: {corr:.3f}, p={p_value:.3f}\")\n\n# Spearman correlation (if data is non-normal)\ncorr, p_value = spearmanr(x, y)\nprint(f\"Spearman correlation: {corr:.3f}, p={p_value:.3f}\")\n\nPearson correlation: 0.996, p=0.000\nSpearman correlation: 1.000, p=0.000\n\n\n\n\n7.1.6.2.3 Key Points\nFor correlation analysis:\n\nUse Pearson’s correlation for linear relationships and Spearman’s correlation for monotonic ones.\n\nUse the chi-square test when dealing with categorical data.\n\nRemember that correlation \\(\\neq\\) causation.\n\n\n\n\n7.1.6.3 Can We Predict One Variable from Another?\nThese methods reveal how well one variable can predict another and quantify the strength of that relationship.\nEx: Can we predict a car’s fuel efficiency with its weight and horsepower?\n\n7.1.6.3.1 Overview of Relevant Tests:\n\n\n\n\n\n\n\n\n\nTest\nUse Case\nKey Assumptions\nPackage:Function\n\n\n\n\nLinear Regression\nPredict a continuous outcome\nLinearity, normal residuals\nsklearn.linear_model:LinearRegression\n\n\nLogistic Regression\nPredict a binary outcome\nLinear relationship between predictors and log-odds\nsklearn.linear_model:LogisticRegression\n\n\nChi-square goodness-of-fit\nCompare observed vs. expected frequencies\nCategories should have expected counts &gt;5\nscipy.stats:chisquare\n\n\n\n\n\n7.1.6.3.2 Linear Regression Python Implementation:\n\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\n\n# Sample data\nX = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)\ny = np.array([2, 4, 5, 4, 5])\n\n# Fit a simple linear regression model\nmodel = LinearRegression()\nmodel.fit(X, y)\npredictions = model.predict(X)\n\nprint(f\"Predicted values: {predictions}\")\n\nPredicted values: [2.8 3.4 4.  4.6 5.2]\n\n\n\n\n7.1.6.3.3 Key Points:\nFor regression modeling:\n\nUse linear regression for continuous outcomes and logistic regression for binary outcomes.\n\nConsider feature scaling and transformations when using regression models.\n\n\n\n\n7.1.6.4 Is This Result Significant or Noise? (Alternative Inference)\nThese methods help distinguish meaningful patterns from random noise while providing estimates of the existence and magnitude of effects. They offer robust alternatives to traditional approaches by using simulation and resampling techniques, making them valuable when standard assumptions are violated.\nEx: Amazon is considering a new design for their checkout page and wants to know if the new design increases conversion rate and by how much.\n\n7.1.6.4.1 Overview of Relevant Tests\n\n\n\n\n\n\n\n\n\nMethod\nUse Case\nKey Idea\nPackage:Function\n\n\n\n\nBootstrap CI\nEstimate confidence intervals from data\nResample data many times to see possible variations\nnumpy.random:choice (custom implementation)\n\n\nPermutation Test\nTest if two groups are meaningfully different\nShuffle labels and see how extreme the observed difference is\nscipy.stats:permutation_test\n\n\nBayesian Inference\nQuantify belief in different hypotheses\nTreat parameters as probabilities and update based on data\npymc3 / scipy.stats:beta (for simple cases)\n\n\n\n\n\n7.1.6.4.2 Python Implementation of all methods:\n\nimport numpy as np\nfrom scipy.stats import permutation_test, beta\n\n# Simulated data: time spent on old vs. new layout\nold_layout = np.array([40, 50, 45, 55, 42, 48])\nnew_layout = np.array([52, 60, 58, 65, 55, 57])\n\n# Bootstrap CI\nbootstrap_samples = np.random.choice(\n  new_layout - old_layout,\n  size=(10000, len(old_layout)),\n  replace=True\n)\nci_lower, ci_upper = np.percentile(\n  bootstrap_samples.mean(axis=1), [2.5, 97.5]\n)\nprint(f\"Bootstrap 95% CI: [{ci_lower:.3f}, {ci_upper:.3f}]\")\n\n# Permutation test\nperm_result = permutation_test(\n  (old_layout, new_layout),\n  statistic=lambda x, y: np.mean(x) - np.mean(y),\n  n_resamples=10000\n)\nprint(f\"Permutation test p-value: {perm_result.pvalue:.3f}\")\n\n# Bayesian inference estimates probability that new layout increases time spent\nalpha, beta_params = 1 + new_layout.sum(), 1 + old_layout.sum()\nbayes_prob = 1 - beta.cdf(0, alpha, beta_params)\nprint(f\"Bayesian probability that new layout is better: {bayes_prob:.3f}\")\n\nBootstrap 95% CI: [10.000, 12.500]\nPermutation test p-value: 0.006\nBayesian probability that new layout is better: 1.000\n\n\n\n\n7.1.6.4.3 Key Points:\nPrinciple ideas for the three methods:\n\nFocus on effect sizes and uncertainty rather than just p-values.\n\nEffect sizes often provide more actionable insights.\n\nBootstrap confidence intervals provide robust interval estimates without normality assumptions.\n\nPermutation tests allow inference without relying on parametric models.\n\nBayesian methods offer probability-based conclusions rather than binary p-values.\n\n\n\n\n\n7.1.7 Statistical vs. Practical Significance\n\n7.1.7.1 Multiple Testing\nMultiple testing refers to performing several statistical tests on the same data or research question simultaneously, which increases the risk of false positives (Type I errors). For example, suppose the null hypothesis is true, and we use a significance level 0.05. With a single test, there’s a 5% chance of a Type I error. However, if we conduct 10 tests, the probability of encountering at least one Type I error rises to approximately 40%.\nA common corrective measure is the Bonferroni correction, which adjusts the significance level as follows:\n\\[\n\\alpha_{\\text{adjusted}} = \\frac{\\alpha}{m},\n\\]\nwhere \\(\\alpha\\) is the significance level and \\(m\\) is the number of analyses.\n\n\n7.1.7.2 Effect Size\nP-values only inform us whether an effect exists. Consider that large sample sizes can make small effects significant, and small samples might overlook large effects. Effect size quantifies the magnitude of a difference or relationship, providing context beyond statistical significance. One such measure for mean difference is Cohen’s d, which is defined as:\n\\[\nd = \\frac{\\bar{X}_1 - \\bar{X}_2}{s_p},\n\\]\nwhere \\(\\bar{X}_1\\) and \\(\\bar{X}_2\\) are the sample means, and \\(s_p\\) is the pooled standard deviation. The pooled standard deviation is calculated as:\n\\[\ns_p = \\sqrt{\\frac{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}{n_1 + n_2 - 2}},\n\\]\nwhere \\(s_1\\) and \\(s_2\\) are the standard deviations, and \\(n_1\\) and \\(n_2\\) are the sample sizes.\nOther measures include:\n\nCorrelation Coefficient: Measures how two continuous variables relate.\n\nEta-Squared: Used for ANOVA; quantifies the proportion of variance explained by a categorical factor.\n\nPhi and Cramer’s V: Used for categorical data in chi-square tests.\n\n\n\n\n7.1.8 Best Practices\n\n7.1.8.1 Assumption Violations\n\nCheck assumptions before choosing a test rather than after obtaining results.\n\nUse non-parametric alternatives when assumptions are violated and connot be easily corrected.\n\nTry transforming the data to better meet assumptions.\n\n\n\n7.1.8.2 Interpretation Guidelines\n\nUse p-values alongside effect sizes and confidence intervals for a more complete analysis.\nBe catious of multiple comparisons and adjust significance levels when necessary.\n\n\n\n7.1.8.3 Documentation\n\nEnsure code and data preprocessing steps are well-documented for reproducibility.\nReport all relevant statistics.\n\nInclude visualizations when possible.\n\n\n\n\n7.1.9 Further Reading\n\nscipy.stats\nstatsmodels.stats\nsklearn.userguide",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Statistical Tests and Models</span>"
    ]
  },
  {
    "objectID": "stats.html#tests-for-exploratory-data-analysis",
    "href": "stats.html#tests-for-exploratory-data-analysis",
    "title": "7  Statistical Tests and Models",
    "section": "7.2 Tests for Exploratory Data Analysis",
    "text": "7.2 Tests for Exploratory Data Analysis\nA collection of functions are available from scipy.stats.\n\nComparing the locations of two samples\n\nttest_ind: t-test for two independent samples\nttest_rel: t-test for paired samples\nranksums: Wilcoxon rank-sum test for two independent samples\nwilcoxon: Wilcoxon signed-rank test for paired samples\n\nComparing the locations of multiple samples\n\nf_oneway: one-way ANOVA\nkruskal: Kruskal-Wallis H-test\n\nTests for associations in contigency tables\n\nchi2_contingency: Chi-square test of independence of variables\nfisher_exact: Fisher exact test on a 2x2 contingency table\n\nGoodness of fit\n\ngoodness_of_fit: distribution could contain unspecified parameters\nanderson: Anderson-Darling test\nkstest: Kolmogorov-Smirnov test\nchisquare: one-way chi-square test\nnormaltest: test for normality\n\n\nSince R has a richer collections of statistical functions, we can call R function from Python with rpy2. See, for example, a blog on this subject.\nFor example, fisher_exact can only handle 2x2 contingency tables. For contingency tables larger than 2x2, we can call fisher.test() from R through rpy2. See this StackOverflow post. Note that the . in function names and arguments are replaced with _.\n\nimport pandas as pd\nimport numpy as np\nimport rpy2.robjects.numpy2ri\nfrom rpy2.robjects.packages import importr\nrpy2.robjects.numpy2ri.activate()\n\nstats = importr('stats')\n\nw0630 = pd.read_feather(\"data/nyccrashes_cleaned.feather\")\nw0630[\"injury\"] = np.where(w0630[\"number_of_persons_injured\"] &gt; 0, 1, 0)\nm = pd.crosstab(w0630[\"injury\"], w0630[\"borough\"])\nprint(m)\n\nres = stats.fisher_test(m.to_numpy(), simulate_p_value = True)\nprint(res)\n\nLoading custom .Rprofileborough  BRONX  BROOKLYN  MANHATTAN  QUEENS  STATEN ISLAND\ninjury                                                    \n0          149       345        164     249             65\n1          129       266        127     227             28\n\n    Fisher's Exact Test for Count Data with simulated p-value (based on\n    2000 replicates)\n\ndata:  structure(c(149L, 129L, 345L, 266L, 164L, 127L, 249L, 227L, 65L, 28L), dim = c(2L, 5L))\np-value = 0.02949\nalternative hypothesis: two.sided",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Statistical Tests and Models</span>"
    ]
  },
  {
    "objectID": "stats.html#statistical-modeling",
    "href": "stats.html#statistical-modeling",
    "title": "7  Statistical Tests and Models",
    "section": "7.3 Statistical Modeling",
    "text": "7.3 Statistical Modeling\nStatistical modeling is a cornerstone of data science, offering tools to understand complex relationships within data and to make predictions. Python, with its rich ecosystem for data analysis, features the statsmodels package— a comprehensive library designed for statistical modeling, tests, and data exploration. statsmodels stands out for its focus on classical statistical models and compatibility with the Python scientific stack (numpy, scipy, pandas).\n\n7.3.1 Installation of statsmodels\nTo start with statistical modeling, ensure statsmodels is installed:\nUsing pip:\npip install statsmodels\n\n\n7.3.2 Linear Model\nLet’s simulate some data for illustrations.\n\nimport numpy as np\n\nnobs = 200\nncov = 5\nnp.random.seed(123)\nx = np.random.random((nobs, ncov)) # Uniform over [0, 1)\nbeta = np.repeat(1, ncov)\ny = 2 + np.dot(x, beta) + np.random.normal(size = nobs)\n\nCheck the shape of y:\n\ny.shape\n\n(200,)\n\n\nCheck the shape of x:\n\nx.shape\n\n(200, 5)\n\n\nThat is, the true linear regression model is \\[\ny = 2 + x_1 + x_2 + x_3 + x_4 + x_5 + \\epsilon.\n\\]\nA regression model for the observed data can be fitted as\n\nimport statsmodels.api as sma\nxmat = sma.add_constant(x)\nmymod = sma.OLS(y, xmat)\nmyfit = mymod.fit()\nmyfit.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\ny\nR-squared:\n0.309\n\n\nModel:\nOLS\nAdj. R-squared:\n0.292\n\n\nMethod:\nLeast Squares\nF-statistic:\n17.38\n\n\nDate:\nWed, 16 Apr 2025\nProb (F-statistic):\n3.31e-14\n\n\nTime:\n09:26:43\nLog-Likelihood:\n-272.91\n\n\nNo. Observations:\n200\nAIC:\n557.8\n\n\nDf Residuals:\n194\nBIC:\n577.6\n\n\nDf Model:\n5\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nconst\n1.8754\n0.282\n6.656\n0.000\n1.320\n2.431\n\n\nx1\n1.1703\n0.248\n4.723\n0.000\n0.682\n1.659\n\n\nx2\n0.8988\n0.235\n3.825\n0.000\n0.435\n1.362\n\n\nx3\n0.9784\n0.238\n4.114\n0.000\n0.509\n1.448\n\n\nx4\n1.3418\n0.250\n5.367\n0.000\n0.849\n1.835\n\n\nx5\n0.6027\n0.239\n2.519\n0.013\n0.131\n1.075\n\n\n\n\n\n\n\n\nOmnibus:\n0.810\nDurbin-Watson:\n1.978\n\n\nProb(Omnibus):\n0.667\nJarque-Bera (JB):\n0.903\n\n\nSkew:\n-0.144\nProb(JB):\n0.637\n\n\nKurtosis:\n2.839\nCond. No.\n8.31\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nQuestions to review:\n\nHow are the regression coefficients interpreted? Intercept?\nWhy does it make sense to center the covariates?\n\nNow we form a data frame with the variables\n\nimport pandas as pd\ndf = np.concatenate((y.reshape((nobs, 1)), x), axis = 1)\ndf = pd.DataFrame(data = df,\n                  columns = [\"y\"] + [\"x\" + str(i) for i in range(1,\n                  ncov + 1)])\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 200 entries, 0 to 199\nData columns (total 6 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   y       200 non-null    float64\n 1   x1      200 non-null    float64\n 2   x2      200 non-null    float64\n 3   x3      200 non-null    float64\n 4   x4      200 non-null    float64\n 5   x5      200 non-null    float64\ndtypes: float64(6)\nmemory usage: 9.5 KB\n\n\nLet’s use a formula to specify the regression model as in R, and fit a robust linear model (rlm) instead of OLS. Note that the model specification and the function interface is similar to R.\n\nimport statsmodels.formula.api as smf\nmymod = smf.rlm(formula = \"y ~ x1 + x2 + x3 + x4 + x5\", data = df)\nmyfit = mymod.fit()\nmyfit.summary()\n\n\nRobust linear Model Regression Results\n\n\nDep. Variable:\ny\nNo. Observations:\n200\n\n\nModel:\nRLM\nDf Residuals:\n194\n\n\nMethod:\nIRLS\nDf Model:\n5\n\n\nNorm:\nHuberT\n\n\n\n\nScale Est.:\nmad\n\n\n\n\nCov Type:\nH1\n\n\n\n\nDate:\nWed, 16 Apr 2025\n\n\n\n\nTime:\n09:26:43\n\n\n\n\nNo. Iterations:\n16\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n1.8353\n0.294\n6.246\n0.000\n1.259\n2.411\n\n\nx1\n1.1254\n0.258\n4.355\n0.000\n0.619\n1.632\n\n\nx2\n0.9664\n0.245\n3.944\n0.000\n0.486\n1.447\n\n\nx3\n0.9995\n0.248\n4.029\n0.000\n0.513\n1.486\n\n\nx4\n1.3275\n0.261\n5.091\n0.000\n0.816\n1.839\n\n\nx5\n0.6768\n0.250\n2.712\n0.007\n0.188\n1.166\n\n\n\nIf the model instance has been used for another fit with different fit parameters, then the fit options might not be the correct ones anymore .\n\n\nFor model diagnostics, one can check residual plots.\n\nimport matplotlib.pyplot as plt\n\nmyOlsFit = smf.ols(formula = \"y ~ x1 + x2 + x3 + x4 + x5\", data = df).fit()\nfig = plt.figure(figsize = (6, 6))\n## residual versus x1; can do the same for other covariates\nfig = sma.graphics.plot_regress_exog(myOlsFit, 'x1', fig=fig)\n\n\n\n\n\n\n\n\nSee more on residual diagnostics and specification tests.\n\n\n7.3.3 Generalized Linear Regression\nA linear regression model cannot be applied to presence/absence or count data. Generalized Linear Models (GLM) extend the classical linear regression to accommodate such response variables, that follow distributions other than the normal distribution. GLMs consist of three main components:\n\nRandom Component: This specifies the distribution of the response variable \\(Y\\). It is assumed to be from the exponential family of distributions, such as Binomial for binary data and Poisson for count data.\nSystematic Component: This consists of the linear predictor, a linear combination of unknown parameters and explanatory variables. It is denoted as \\(\\eta = X\\beta\\), where \\(X\\) represents the explanatory variables, and \\(\\beta\\) represents the coefficients.\nLink Function: The link function, \\(g\\), provides the relationship between the linear predictor and the mean of the distribution function. For a GLM, the mean of \\(Y\\) is related to the linear predictor through the link function as \\(\\mu = g^{-1}(\\eta)\\).\n\nGLMs adapt to various data types through the selection of appropriate link functions and probability distributions. Here, we outline four special cases of GLM: normal regression, logistic regression, Poisson regression, and gamma regression.\n\nNormal Regression (Linear Regression). In normal regression, the response variable has a normal distribution. The identity link function is typically used, making this case equivalent to classical linear regression.\n\nUse Case: Modeling continuous data where residuals are normally distributed.\nLink Function: Identity, \\(g(\\mu) = \\mu\\).\nDistribution: Normal.\n\nLogistic Regression. Logistic regression is used for binary response variables. It employs the logit link function to model the probability that an observation falls into one of two categories.\n\nUse Case: Binary outcomes (e.g., success/failure).\nLink Function: Logit, \\(g(\\mu) = \\log\\frac{\\mu}{1-\\mu}\\).\nDistribution: Binomial.\n\nPoisson Regression. Poisson regression models count data using the Poisson distribution. It’s ideal for modeling the rate at which events occur.\n\nUse Case: Count data, such as the number of occurrences of an event.\nLink Function: Log, \\(g(\\mu) = \\log(\\mu)\\)\nDistribution: Poisson.\n\nGamma Regression. Gamma regression is suited for modeling positive continuous variables, especially when data are skewed and variance increases with the mean.\n\nUse Case: Positive continuous outcomes with non-constant variance.\nLink Function: Inverse \\(g(\\mu) = \\frac{1}{\\mu}\\).\nDistribution: Gamma.\n\n\nEach GLM variant addresses specific types of data and research questions, enabling precise modeling and inference based on the underlying data distribution. Prediction will need the inverse link function which transforms the linear predictor to the expectation of the outcome.\nTo demonstrate the validation of logistic regression models, we first create a simulated dataset with binary outcomes. This setup involves generating logistic probabilities and then drawing binary outcomes based on these probabilities.\n\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\n\n# Set seed for reproducibility\nnp.random.seed(42)\n\n# Create a DataFrame with random features named `simdat`\nsimdat = pd.DataFrame(np.random.randn(1000, 5), columns=['x1', 'x2', 'x3', 'x4', 'x5'])\n\n# Calculating the linear combination of inputs plus an intercept\neta = simdat.dot([2, 2, 2, 0, 0]) - 5\n\n# Applying the logistic function to get probabilities using statsmodels' logit link\np = sm.families.links.Logit().inverse(eta)\n\n# Generating binary outcomes based on these probabilities and adding them to `simdat`\nsimdat['yb'] = np.random.binomial(1, p, p.size)\n\n# Display the first few rows of the dataframe\nprint(simdat.head())\n\n         x1        x2        x3        x4        x5  yb\n0  0.496714 -0.138264  0.647689  1.523030 -0.234153   0\n1 -0.234137  1.579213  0.767435 -0.469474  0.542560   0\n2 -0.463418 -0.465730  0.241962 -1.913280 -1.724918   0\n3 -0.562288 -1.012831  0.314247 -0.908024 -1.412304   0\n4  1.465649 -0.225776  0.067528 -1.424748 -0.544383   0\n\n\nFit a logistic regression for y1b with the formula interface.\n\nimport statsmodels.formula.api as smf\n\n# Specify the model formula\nformula = 'yb ~ x1 + x2 + x3 + x4 + x5'\n\n# Fit the logistic regression model using glm and a formula\nfit = smf.glm(formula=formula, data=simdat, family=sm.families.Binomial()).fit()\n\n# Print the summary of the model\nprint(fit.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                     yb   No. Observations:                 1000\nModel:                            GLM   Df Residuals:                      994\nModel Family:                Binomial   Df Model:                            5\nLink Function:                  Logit   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -136.44\nDate:                Wed, 16 Apr 2025   Deviance:                       272.89\nTime:                        09:26:45   Pearson chi2:                 1.09e+03\nNo. Iterations:                     8   Pseudo R-squ. (CS):             0.2793\nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     -5.4564      0.453    -12.049      0.000      -6.344      -4.569\nx1             2.1544      0.244      8.822      0.000       1.676       2.633\nx2             2.0781      0.225      9.234      0.000       1.637       2.519\nx3             1.9260      0.237      8.125      0.000       1.461       2.391\nx4            -0.1085      0.166     -0.652      0.514      -0.434       0.217\nx5             0.2672      0.158      1.695      0.090      -0.042       0.576\n==============================================================================",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Statistical Tests and Models</span>"
    ]
  },
  {
    "objectID": "stats.html#logistic-regression",
    "href": "stats.html#logistic-regression",
    "title": "7  Statistical Tests and Models",
    "section": "7.4 Logistic Regression",
    "text": "7.4 Logistic Regression\nOnce a logistic regression model is fitted, interpreting its results is crucial for understanding how predictor variables influence the probability of the outcome. Logistic regression models the log-odds of the response variable as a linear function of the predictor variables. To ease the intrepretation, consider a logistic model with a single binary predictor (e.g., treatment indicator):\n\\[\n\\log\\left(\\frac{\\mu}{1 - \\mu}\\right) = \\beta_0 + \\beta_1 X\n\\]\nwhere \\(\\mu = E(Y \\mid X)\\) represents the probability of the positive class, and \\(\\beta_1\\) is the estimated coefficient for the binary predictor \\(X\\).\n\n7.4.1 Interpreting Coefficients\nIf \\(X\\) is a binary variable (e.g., 0 for “No” and 1 for “Yes”), \\(\\beta_1\\) represents the difference in log-odds between the two groups. Exponentiating \\(\\beta_1\\) gives the odds ratio:\n\\[\n\\text{Odds Ratio} = \\frac{\\exp(\\beta_0 + \\beta_1)}{\\exp(\\beta_0)} = e^{\\beta_1}.\n\\]\n\nIf \\(e^{\\beta_1} &gt; 1\\), the outcome is more likely when \\(X = 1\\) than when \\(X = 0\\).\nIf \\(e^{\\beta_1} &lt; 1\\), the outcome is less likely when \\(X = 1\\).\nIf \\(e^{\\beta_1} = 1\\), there is no effect of \\(X\\) on the odds of the outcome.\n\nEquivalently, \\(\\beta_1\\) is the log odds ratio between the two groups.\nWhen there are multiple predictors, the intrepretation needs to state that all the other predictors are unchanged.\nHow would you intreprete the coefficient of a continuous predictor?\n\n\n7.4.2 Probabilistic Interpretation\nWe can transform the linear predictor into a probability estimate using the inverse logit function:\n\\[\n\\Pr(Y=1 | X) = \\frac{e^{\\beta_0 + \\beta_1 X}}{1 + e^{\\beta_0 + \\beta_1 X}}.\n\\]\nThis allows for a direct interpretation of how being in one category of \\(X\\) influences the predicted probability of the outcome. By construction, this value is always in \\((0, 1)\\).\n\n\n7.4.3 Evaluating Statistical Significance\nThe significance of \\(\\beta_1\\) is assessed using standard errors and p-values:\n\nA small p-value (e.g., &lt; 0.05) suggests that \\(X\\) has a statistically significant effect on the outcome.\nConfidence intervals for \\(e^{\\beta_1}\\) help understand the precision of odds ratio estimates.\n\n\n\n7.4.4 Confusion Matrix\nValidating the performance of logistic regression models is crucial to assess their effectiveness and reliability. This section explores key metrics used to evaluate the performance of logistic regression models, starting with the confusion matrix, then moving on to accuracy, precision, recall, F1 score, and the area under the ROC curve (AUC). Using simulated data, we will demonstrate how to calculate and interpret these metrics using Python.\nThe confusion matrix is a fundamental tool used for calculating several other classification metrics. It is a table used to describe the performance of a classification model on a set of data for which the true values are known. The matrix displays the actual values against the predicted values, providing insight into the number of correct and incorrect predictions.\n\n\n\nActual\nPredicted Positive\nPredicted Negative\n\n\n\n\nActual Positive\nTrue Positive (TP)\nFalse Negative (FN)\n\n\nActual Negative\nFalse Positive (FP)\nTrue Negative (TN)\n\n\n\nFour entries in the confusion matrix:\n\nTrue Positive (TP): The cases in which the model correctly predicted the positive class.\nFalse Positive (FP): The cases in which the model incorrectly predicted the positive class (i.e., the model predicted positive, but the actual class was negative).\nTrue Negative (TN): The cases in which the model correctly predicted the negative class.\nFalse Negative (FN): The cases in which the model incorrectly predicted the negative class (i.e., the model predicted negative, but the actual class was positive).\n\nFour rates from the confusion matrix with actual (row) margins:\n\nTrue positive rate (TPR): TP / (TP + FN). Also known as sensitivity.\nFalse negative rate (FNR): FN / (TP + FN). Also known as miss rate.\nFalse positive rate (FPR): FP / (FP + TN). Also known as false alarm, fall-out.\nTrue negative rate (TNR): TN / (FP + TN). Also known as specificity.\n\nNote that TPR and FPR do not add up to one. Neither do FNR and FPR.\n\nPositive predictive value (PPV): TP / (TP + FP). Also known as precision.\nFalse discovery rate (FDR): FP / (TP + FP).\nFalse omission rate (FOR): FN / (FN + TN).\nNegative predictive value (NPV): TN / (FN + TN).\n\nNote that PPV and NP do not add up to one.\n\n\n7.4.5 Accuracy\nAccuracy measures the overall correctness of the model and is defined as the ratio of correct predictions (both positive and negative) to the total number of cases examined.\n  Accuracy = (TP + TN) / (TP + TN + FP + FN)\n\nImbalanced Classes: Accuracy can be misleading if there is a significant imbalance between the classes. For instance, in a dataset where 95% of the samples are of one class, a model that naively predicts the majority class for all instances will still achieve 95% accuracy, which does not reflect true predictive performance.\nMisleading Interpretations: High overall accuracy might hide the fact that the model is performing poorly on a smaller, yet important, segment of the data.\n\n\n\n7.4.6 Precision\nPrecision (or PPV) measures the accuracy of positive predictions. It quantifies the number of correct positive predictions made.\n  Precision = TP / (TP + FP)\n\nNeglect of False Negatives: Precision focuses solely on the positive class predictions. It does not take into account false negatives (instances where the actual class is positive but predicted as negative). This can be problematic in cases like disease screening where missing a positive case (disease present) could be dangerous.\nNot a Standalone Metric: High precision alone does not indicate good model performance, especially if recall is low. This situation could mean the model is too conservative in predicting positives, thus missing out on a significant number of true positive instances.\n\n\n\n7.4.7 Recall\nRecall (Sensitivity or TPR) measures the ability of a model to find all relevant cases (all actual positives).\n  Recall = TP / (TP + FN)\n\nNeglect of False Positives: Recall does not consider false positives (instances where the actual class is negative but predicted as positive). High recall can be achieved at the expense of precision, leading to a large number of false positives which can be costly or undesirable in certain contexts, such as in spam detection.\nTrade-off with Precision: Often, increasing recall decreases precision. This trade-off needs to be managed carefully, especially in contexts where both false positives and false negatives carry significant costs or risks.\n\n\n\n7.4.8 F-beta Score\nThe F-beta score is a weighted harmonic mean of precision and recall, taking into account a \\(\\beta\\) parameter such that recall is considered \\(\\beta\\) times as important as precision: \\[\n(1 + \\beta^2) \\frac{\\text{precision} \\cdot \\text{recall}}\n{\\beta^2 \\text{precision} + \\text{recall}}.\n\\]\nSee stackexchange post for the motivation of \\(\\beta^2\\) instead of just \\(\\beta\\).\nThe F-beta score reaches its best value at 1 (perfect precision and recall) and worst at 0.\nIf reducing false negatives is more important (as might be the case in medical diagnostics where missing a positive diagnosis could be critical), you might choose a beta value greater than 1. If reducing false positives is more important (as in spam detection, where incorrectly classifying an email as spam could be inconvenient), a beta value less than 1 might be appropriate.\nThe F1 Score is a specific case of the F-beta score where beta is 1, giving equal weight to precision and recall. It is the harmonic mean of Precision and Recall and is a useful measure when you seek a balance between Precision and Recall and there is an uneven class distribution (large number of actual negatives).\n\n\n7.4.9 Receiver Operating Characteristic (ROC) Curve\nThe Receiver Operating Characteristic (ROC) curve is a plot that illustrates the diagnostic ability of a binary classifier as its discrimination threshold is varied. It shows the trade-off between the TPR and FPR. The ROC plots TPR against FPR as the decision threshold is varied. It can be particularly useful in evaluating the performance of classifiers when the class distribution is imbalanced,\n\nIncreasing from \\((0, 0)\\) to \\((1, 1)\\).\nBest classification passes \\((0, 1)\\).\nClassification by random guess gives the 45-degree line.\nArea between the ROC and the 45-degree line is the Gini coefficient, a measure of inequality.\nArea under the curve (AUC) of ROC thus provides an important metric of classification results.\n\nThe Area Under the ROC Curve (AUC) is a scalar value that summarizes the performance of a classifier. It measures the total area underneath the ROC curve, providing a single metric to compare models. The value of AUC ranges from 0 to 1:\n\nAUC = 1: A perfect classifier, which perfectly separates positive and negative classes.\nAUC = 0.5: A classifier that performs no better than random chance.\nAUC &lt; 0.5: A classifier performing worse than random.\n\nThe AUC value provides insight into the model’s ability to discriminate between positive and negative classes across all possible threshold values.\n\n\n7.4.10 Demonstration\nLet’s apply these metrics to the simdat dataset to understand their practical implications. We will fit a logistic regression model, make predictions, and then compute accuracy, precision, and recall.\n\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, confusion_matrix,\n    f1_score, roc_curve, auc\n)\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_classification\n\n# Generate synthetic data\nX, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n\n# Fit the logistic regression model\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\n\n# Predict labels on the test set\ny_pred = model.predict(X_test)\n\n# Get predicted probabilities for ROC curve and AUC\ny_scores = model.predict_proba(X_test)[:, 1]  # Probability for the positive class\n\n# Compute confusion matrix\ncm = confusion_matrix(y_test, y_pred)\n\n# Calculate accuracy, precision, and recall\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\n\n# Print confusion matrix and metrics\nprint(\"Confusion Matrix:\\n\", cm)\nprint(f\"Accuracy: {accuracy:.2f}\")\nprint(f\"Precision: {precision:.2f}\")\nprint(f\"Recall: {recall:.2f}\")\n\nConfusion Matrix:\n [[104  11]\n [ 26 109]]\nAccuracy: 0.85\nPrecision: 0.91\nRecall: 0.81\n\n\nBy varying threshold, one can plot the whole ROC curve.\n\n# Compute ROC curve and AUC\nfpr, tpr, thresholds = roc_curve(y_test, y_scores)\nroc_auc = auc(fpr, tpr)\n\n# Print AUC\nprint(f\"AUC: {roc_auc:.2f}\")\n\n# Plot ROC curve\nplt.figure()\nplt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')  # Diagonal line (random classifier)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC) Curve')\nplt.legend(loc=\"lower right\")\nplt.show()\n\nAUC: 0.92\n\n\n\n\n\n\n\n\n\nWe could pick the best threshold that optmizes F1-score/\n\n# Compute F1 score for each threshold\nf1_scores = []\nfor thresh in thresholds:\n    y_pred_thresh = (y_scores &gt;= thresh).astype(int)  # Apply threshold to get binary predictions\n    f1 = f1_score(y_test, y_pred_thresh)\n    f1_scores.append(f1)\n\n# Find the best threshold (the one that maximizes F1 score)\nbest_thresh = thresholds[np.argmax(f1_scores)]\nbest_f1 = max(f1_scores)\n\n# Print the best threshold and corresponding F1 score\nprint(f\"Best threshold: {best_thresh:.4f}\")\nprint(f\"Best F1 score: {best_f1:.2f}\")\n\nBest threshold: 0.3960\nBest F1 score: 0.89",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Statistical Tests and Models</span>"
    ]
  },
  {
    "objectID": "stats.html#lasso-logistic-models",
    "href": "stats.html#lasso-logistic-models",
    "title": "7  Statistical Tests and Models",
    "section": "7.5 LASSO Logistic Models",
    "text": "7.5 LASSO Logistic Models\nThe Least Absolute Shrinkage and Selection Operator (LASSO) (Tibshirani, 1996), is a regression method that performs both variable selection and regularization. LASSO imposes an L1 penalty on the regression coefficients, which has the effect of shrinking some coefficients exactly to zero. This results in simpler, more interpretable models, especially in situations where the number of predictors exceeds the number of observations.\n\n7.5.1 Theoretical Formulation of the Problem\nThe objective function for LASSO logistic regression can be expressed as,\n\\[\n\\min_{\\beta}\n\\left\\{ -\\frac{1}{n} \\sum_{i=1}^n \\left[ y_i \\log(\\hat{p}_i) + (1 - y_i) \\log(1 - \\hat{p}_i) \\right] + \\lambda \\sum_{j=1}^p |\\beta_j| \\right\\}\n\\]\nwhere:\n\n\\(\\hat{p}_i = \\frac{1}{1 + e^{-X_i\\beta}}\\) is the predicted probability for the \\(i\\)-th sample.\n\\(y_i\\) represents the actual class label (binary: 0 or 1).\n\\(X_i\\) is the feature vector for the \\(i\\)-th observation.\n\\(\\beta\\) is the vector of model coefficients (including the intercept).\n\\(\\lambda\\) is the regularization parameter that controls the trade-off between model fit and sparsity (higher \\(\\lambda\\)) encourages sparsity by shrinking more coefficients to zero).\n\nThe lasso penalty encourages the sum of the absolute values of the coefficients to be small, effectively shrinking some coefficients to zero. This results in sparser solutions, simplifying the model and reducing variance without substantial increase in bias.\nPractical benefits of LASSO:\n\nDimensionality Reduction: LASSO is particularly useful when the number of features \\(p\\) is large, potentially even larger than the number of observations \\(n\\), as it automatically reduces the number of features.\nPreventing Overfitting: The L1 penalty helps prevent overfitting by constraining the model, especially when \\(p\\) is large or there is multicollinearity among features.\nInterpretability: By selecting only the most important features, LASSO makes the resulting model more interpretable, which is valuable in fields like bioinformatics, economics, and social sciences.\n\n\n\n7.5.2 Solution Path\nTo illustrate the effect of the lasso penalty in logistic regression, we can plot the solution path of the coefficients as a function of the regularization parameter \\(\\lambda\\). This demonstration will use a simulated dataset to show how increasing \\(\\lambda\\) leads to more coefficients being set to zero.\n\nimport numpy as np\nfrom sklearn.datasets import make_classification\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\n\n# Step 1: Generate a classification dataset\nX, y = make_classification(n_samples=100, n_features=20, n_informative=2,\n                               random_state=42)\n\n# Step 2: Get a lambda grid given length of lambda and min_ratio of lambda_max\ndef get_lambda_l1(xs: np.ndarray, y: np.ndarray, nlambda: int, min_ratio: float):\n    ybar = np.mean(y)\n    xbar = np.mean(xs, axis=0)\n    xs_centered = xs - xbar\n    xty = np.dot(xs_centered.T, (y - ybar))\n    lmax = np.max(np.abs(xty))\n    lambdas = np.logspace(np.log10(lmax), np.log10(min_ratio * lmax),\n                              num=nlambda)\n    return lambdas\n\n# Step 3: Calculate lambda values\nnlambda = 100\nmin_ratio = 0.01\nlambda_values = get_lambda_l1(X, y, nlambda, min_ratio)\n\n# Step 4: Standardize the features\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Step 5: Initialize arrays to store the coefficients for each lambda value\ncoefficients = []\n\n# Step 6: Fit logistic regression with L1 regularization (Lasso) for each lambda value\nfor lam in lambda_values:\n    model = LogisticRegression(penalty='l1', solver='liblinear', C=1/lam, max_iter=1000)\n    model.fit(X_scaled, y)\n    coefficients.append(model.coef_.flatten())\n\n# Convert coefficients list to a NumPy array for plotting\ncoefficients = np.array(coefficients)\n\n# Step 7: Plot the solution path for each feature\nplt.figure(figsize=(10, 6))\nfor i in range(coefficients.shape[1]):\n    plt.plot(lambda_values, coefficients[:, i], label=f'Feature {i + 1}')\n    \nplt.xscale('log')\nplt.xlabel('Lambda values (log scale)')\nplt.ylabel('Coefficient value')\nplt.title('Solution Path of Logistic Lasso Regression')\nplt.grid(True)\nplt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n7.5.3 Selection the Tuning Parameter\nIn logistic regression with LASSO regularization, selecting the optimal value of the regularization parameter \\(C\\) (the inverse of \\(\\lambda\\)) is crucial to balancing the model’s bias and variance. A small \\(C\\) value (large \\(\\lambda\\)) increases the regularization effect, shrinking more coefficients to zero and simplifying the model. Conversely, a large \\(C\\) (small \\(\\lambda\\)) allows the model to fit the data more closely.\nThe best way to select the optimal \\(C\\) is through cross-validation. In cross-validation, the dataset is split into several folds, and the model is trained on some folds while evaluated on the remaining fold. This process is repeated for each fold, and the results are averaged to ensure the model generalizes well to unseen data. The \\(C\\) value that results in the best performance is selected.\nThe performance metric used in cross-validation can vary based on the task. Common metrics include:\n\nLog-loss: Measures how well the predicted probabilities match the actual outcomes.\nAccuracy: Measures the proportion of correctly classified instances.\nF1-Score: Balances precision and recall, especially useful for imbalanced classes.\nAUC-ROC: Evaluates how well the model discriminates between the positive and negative classes.\n\nIn Python, the LogisticRegressionCV class from scikit-learn automates cross-validation for logistic regression. It evaluates the model’s performance for a range of \\(C\\) values and selects the best one.\n\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import make_classification\nfrom sklearn.metrics import accuracy_score\n\n# Generate synthetic data\nX, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n\n# Initialize LogisticRegressionCV with L1 penalty for Lasso and cross-validation\nlog_reg_cv = LogisticRegressionCV(\n    Cs=np.logspace(-4, 4, 20),  # Range of C values (inverse of lambda)\n    cv=5,                       # 5-fold cross-validation\n    penalty='l1',               # Lasso regularization (L1 penalty)\n    solver='liblinear',         # Solver for L1 regularization\n    scoring='accuracy',         # Optimize for accuracy\n    max_iter=10000              # Ensure convergence\n)\n\n# Train the model with cross-validation\nlog_reg_cv.fit(X_train, y_train)\n\n# Best C value (inverse of lambda)\nprint(f\"Best C value: {log_reg_cv.C_[0]}\")\n\n# Evaluate the model on the test set\ny_pred = log_reg_cv.predict(X_test)\ntest_accuracy = accuracy_score(y_test, y_pred)\nprint(f\"Test Accuracy: {test_accuracy:.2f}\")\n\n# Display the coefficients of the best model\nprint(\"Model Coefficients:\\n\", log_reg_cv.coef_)\n\nBest C value: 0.08858667904100823\nTest Accuracy: 0.86\nModel Coefficients:\n [[ 0.          0.          0.05552448  0.          0.          1.90889734\n   0.          0.          0.          0.          0.0096863   0.23541942\n   0.          0.         -0.0268928   0.          0.          0.\n   0.          0.        ]]\n\n\n\n\n7.5.4 Preparing for Logistic Regression Fitting\nThe LogisticRegression() function in scikit.learn takes the design matrix of the regression as input, which needs to be prepared with care from the covariates or features that we have.\n\n7.5.4.1 Continuous Variables\nFor continuous variables, it is often desirable to standardized them so that they have mean zero and standard deviation one. There are multiple advantages of doing so. It improves numerical stability in algorithms like logistic regression that rely on gradient descent, ensuring faster convergence and preventing features with large scales from dominating the optimization process. Standardization also enhances the interpretability of model coefficients by allowing for direct comparison of the effects of different features, as coefficients then represent the change in outcome for a one standard deviation increase in each variable. Additionally, it ensures that regularization techniques like Lasso and Ridge treat all features equally, allowing the model to select the most relevant ones without being biased by feature magnitude.\nMoreover, standardization is essential for distance-based models such as k-Nearest Neighbors (k-NN) and Support Vector Machines (SVMs), where differences in feature scale can distort the calculations. It also prevents models from being sensitive to arbitrary changes in the units of measurement, improving robustness and consistency. Finally, standardization facilitates better visualizations and diagnostics by putting all variables on a comparable scale, making patterns and residuals easier to interpret. Overall, it is a simple yet powerful preprocessing step that leads to better model performance and interpretability.\nWe have already seen this with StandardScaler.\n\n\n7.5.4.2 Categorical Variables\nCategorical variables can be classified into two types: nominal and ordinal. Nominal variables represent categories with no inherent order or ranking between them. Examples include variables like “gender” (male, female) or “color” (red, blue, green), where the categories are simply labels and one category does not carry more significance than another. Ordinal variables, on the other hand, represent categories with a meaningful order or ranking. For example, education levels such as “high school,” “bachelor,” “master,” and “PhD” have a clear hierarchy, where each level is ranked higher than the previous one. However, the differences between the ranks are not necessarily uniform or quantifiable, making ordinal variables distinct from numerical variables. Understanding the distinction between nominal and ordinal variables is important when deciding how to encode and interpret them in statistical models.\nCategorical variables needs to be coded into numerical values before further processing. In Python, nominal and ordinal variables are typically encoded differently to account for their unique properties. Nominal variables, which have no inherent order, are often encoded using One-Hot Encoding, where each category is transformed into a binary column (0 or 1). For example, the OneHotEncoder from scikit-learn can be used to convert a “color” variable with categories like “red,” “blue,” and “green” into separate columns color_red, color_blue, and color_green, with only one column being 1 for each observation. On the other hand, ordinal variables, which have a meaningful order, are best encoded using Ordinal Encoding. This method assigns an integer to each category based on their rank. For example, an “education” variable with categories “high school,” “bachelor,” “master,” and “PhD” can be encoded as 0, 1, 2, and 3, respectively. The OrdinalEncoder from scikit-learn can be used to implement this encoding, which ensures that the model respects the order of the categories during analysis.\n\n\n7.5.4.3 An Example\nHere is a demo with pipeline using a simulated dataset.\nFirst we generate data with sample size 1000 from a logistic model with both categorical and numerical covariates.\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nimport numpy as np\nfrom scipy.special import expit  # Sigmoid function\n\n# Generate a dataset with the specified size\ndataset_size = 1000\nnp.random.seed(20241014)\n\n# Simulate categorical and numerical features\ngender = np.random.choice(\n    ['male', 'female'], size=dataset_size)  # Nominal variable\neducation = np.random.choice(\n    ['high_school', 'bachelor', 'master', 'phd'], size=dataset_size)  # Ordinal variable\nage = np.random.randint(18, 65, size=dataset_size)\nincome = np.random.randint(30000, 120000, size=dataset_size)\n\n# Create a logistic relationship between the features and the outcome\ngender_num = np.where(gender == 'male', 0, 1)\n\n# Define the linear predictor with regression coefficients\nlinear_combination = (\n    0.3 * gender_num - 0.02 * age + 0.00002 * income\n)\n\n# Apply sigmoid function to get probabilities\nprobabilities = expit(linear_combination)\n\n# Generate binary outcome based on the probabilities\noutcome = np.random.binomial(1, probabilities)\n\n# Create a DataFrame\ndata = pd.DataFrame({\n    'gender': gender,\n    'education': education,\n    'age': age,\n    'income': income,\n    'outcome': outcome\n})\n\nNext we split the data into features and target and define transformers for each types of feature columns.\n\n# Split the dataset into features (X) and target (y)\nX = data[['gender', 'education', 'age', 'income']]\ny = data['outcome']\n\n# Define categorical and numerical columns\ncategorical_cols = ['gender', 'education']  \nnumerical_cols = ['age', 'income']\n\n# Define transformations for categorical variable\ncategorical_transformer = OneHotEncoder(\n    categories=[['male', 'female'], ['high_school', 'bachelor', 'master', 'phd']],\n    drop='first')\n\n# Define transformations for continuous variables\nnumerical_transformer = StandardScaler()\n\n# Use ColumnTransformer to transform the columns\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('cat', categorical_transformer, categorical_cols),\n        ('num', numerical_transformer, numerical_cols)\n    ]\n)\n\nDefine a pipeline, which preprocess the data and then fits a logistic model.\n\npipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('classifier', LogisticRegression(penalty='l1', solver='liblinear',\n    max_iter=1000))\n])\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=2024)\n\n# Fit the pipeline to the training data\npipeline.fit(X_train, y_train)\n\nPipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('cat',\n                                                  OneHotEncoder(categories=[['male',\n                                                                             'female'],\n                                                                            ['high_school',\n                                                                             'bachelor',\n                                                                             'master',\n                                                                             'phd']],\n                                                                drop='first'),\n                                                  ['gender', 'education']),\n                                                 ('num', StandardScaler(),\n                                                  ['age', 'income'])])),\n                ('classifier',\n                 LogisticRegression(max_iter=1000, penalty='l1',\n                                    solver='liblinear'))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFittedPipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('cat',\n                                                  OneHotEncoder(categories=[['male',\n                                                                             'female'],\n                                                                            ['high_school',\n                                                                             'bachelor',\n                                                                             'master',\n                                                                             'phd']],\n                                                                drop='first'),\n                                                  ['gender', 'education']),\n                                                 ('num', StandardScaler(),\n                                                  ['age', 'income'])])),\n                ('classifier',\n                 LogisticRegression(max_iter=1000, penalty='l1',\n                                    solver='liblinear'))]) preprocessor: ColumnTransformer?Documentation for preprocessor: ColumnTransformerColumnTransformer(transformers=[('cat',\n                                 OneHotEncoder(categories=[['male', 'female'],\n                                                           ['high_school',\n                                                            'bachelor',\n                                                            'master', 'phd']],\n                                               drop='first'),\n                                 ['gender', 'education']),\n                                ('num', StandardScaler(), ['age', 'income'])]) cat['gender', 'education'] OneHotEncoder?Documentation for OneHotEncoderOneHotEncoder(categories=[['male', 'female'],\n                          ['high_school', 'bachelor', 'master', 'phd']],\n              drop='first') num['age', 'income'] StandardScaler?Documentation for StandardScalerStandardScaler() LogisticRegression?Documentation for LogisticRegressionLogisticRegression(max_iter=1000, penalty='l1', solver='liblinear') \n\n\nCheck the coefficients of the fitted logistic regression model.\n\nmodel = pipeline.named_steps['classifier']\nintercept = model.intercept_\ncoefficients = model.coef_\n\n# Check the preprocessor's encoding\nencoded_columns = pipeline.named_steps['preprocessor']\\\n.transformers_[0][1].get_feature_names_out(categorical_cols)\n\n# Show intercept, coefficients, and encoded feature names\nintercept, coefficients, list(encoded_columns)\n\n(array([0.66748582]),\n array([[ 0.30568894,  0.10069842,  0.12087311,  0.22576774, -0.24749201,\n          0.55828424]]),\n ['gender_female', 'education_bachelor', 'education_master', 'education_phd'])\n\n\nNote that the encoded columns has one for gender and three for education, with male and high_school as reference levels, respectively. The reference level was determined when calling oneHotEncoder() with drop = 'first'. If categories were not specified, the first level in alphabetical order would be dropped. With the default drop = 'none', the estimated coefficients will have two columns that are not estimable and were set to zero. Obviously, if no level were dropped in forming the model matrix, the columns of the one hot encoding for each categorical variable would be perfectly linearly dependent because they would sum to one.\nThe regression coefficients returned by the logistic regression model in this case should be interpreted on the standardized scale of the numerical covariates (e.g., age and income). This is because we applied standardization to the numerical features using StandardScaler in the pipeline before fitting the model. For example, the coefficient for age would reflect the change in the log-odds of the outcome for a 1 standard deviation increase in age, rather than a 1-unit increase in years. The coefficients for the one-hot encoded categorical variables (gender and education) are on the original scale because one-hot encoding does not change the scale of the variables. For instance, the coefficient for gender_female tells us how much the log-odds of the outcome changes when the observation is male versus the reference category (male).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Statistical Tests and Models</span>"
    ]
  },
  {
    "objectID": "stats.html#count-data-modeling",
    "href": "stats.html#count-data-modeling",
    "title": "7  Statistical Tests and Models",
    "section": "7.6 Count Data Modeling",
    "text": "7.6 Count Data Modeling\nCount data consists of non-negative integers representing event occurrences over a fixed unit of time or space. These data often exhibit skewness, overdispersion, and a prevalence of zeros, requiring specialized statistical models. Count data is common in applied fields such as urban planning and environmental studies. This section introduces statistical models for count data, focusing on the Poisson and Negative Binomial (NB) distributions. Their probability mass functions (pmfs) are linked to Generalized Linear Model (GLM) parameters.\n\n7.6.1 Poisson Regression\nThe Poisson model is a member of the GLM. It assumes that the count variable \\(Y\\) follows a Poisson distribution:\n\\[\n\\Pr(Y = y) = \\frac{\\lambda^y e^{-\\lambda}}{y!}, \\quad y = 0,1,2,\\dots\n\\] where \\(\\lambda\\) is the expected count, linked to predictor variables through a log link function:\n\\[\n\\log \\lambda = X^{\\top} \\beta.\n\\]\nHere, $X represents the vector of covariates, and \\(\\beta\\) denotes the regression coefficients. The model assumes equidispersion (i.e., \\(E[Y] = \\text{Var}(Y)\\)), which is often violated in practice.\n\nThe coefficient \\(\\beta_j\\) represents the log change in the expected count per unit increase in \\(X_j\\).\nExponentiating \\(\\beta_j\\) provides the multiplicative effect on the mean count.\nIf \\(\\beta_j &gt; 0\\), increasing \\(X_j\\) leads to higher counts, while \\(\\beta_j &lt; 0\\) suggests a negative association\n\n\n\n7.6.2 Negative Binomial Regression\nWhen overdispersion (variance exceeding the mean) is present, the Negative Binomial (NB) regression provides a more flexible alternative. The NB model introduces an overdispersion parameter \\(\\theta\\), modifying the variance structure:\n\\[\n\\Pr(Y = y) = \\frac{\\Gamma(y + \\theta^{-1})}{y! \\Gamma(\\theta^{-1})}\n\\left( \\frac{\\theta \\lambda}{1 + \\theta \\lambda} \\right)^y\n\\left( \\frac{1}{1 + \\theta \\lambda} \\right)^{\\theta^{-1}},\n\\]\nwhere \\(\\theta\\) controls the degree of dispersion. The mean remains \\(\\lambda\\), but the variance expands to:\n\\[\n\\text{Var}(Y) = \\lambda + \\frac{\\lambda^2}{\\theta}.\n\\]\nThe log link function remains to be commonly used.\n\nCoefficients in NB regression are interpreted similarly to Poisson regression.\nThe dispersion parameter \\(\\theta\\) quantifies the degree of overdispersion; larger values suggest the Poisson model may still be appropriate.\n\n\n\n7.6.3 Model Diagnosis\nAssessing model fit is crucial in count data modeling. Common diagnostic methods include:\n\nOverdispersion Check: If the variance significantly exceeds the mean, NB regression is preferred.\nGoodness-of-Fit: Comparing Akaike Information Criterion (AIC) values for Poisson and NB models; lower AIC suggests a better fit.\nResidual Analysis: Examining Pearson and deviance residuals for systematic patterns.\nZero-Inflation Check: If excess zeros exist, zero-inflated models may be required.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Statistical Tests and Models</span>"
    ]
  },
  {
    "objectID": "stats.html#an-example-with-nyc-street-flood",
    "href": "stats.html#an-example-with-nyc-street-flood",
    "title": "7  Statistical Tests and Models",
    "section": "7.7 An Example with NYC Street Flood",
    "text": "7.7 An Example with NYC Street Flood\nWe use the NYC street flood data from the mid-term project. This analysis focuses on two sewer-related complaints in 2024: Street Flooding (SF) and Catch Basin (CB). SF complaints serve as a practical indicator of street flooding, while CB complaints provide insights into a key infrastructural factor—when catch basins fail to drain rainwater properly due to blockages or structural issues, water accumulates on the streets\nLet’s reformat the data to create count times series of SF and CB complaints by zip code.\n\nimport pandas as pd\n# Reload the dataset, ensuring 'Incident Zip' is read as a string from the start\ndf = pd.read_csv(\"data/nycflood2024.csv\",\n                 dtype={\"Incident Zip\": str}, parse_dates=[\"Created Date\"])\n\n# Filter for incidents in 2024, but also include 2023-12-31 for lag calculation\ndf_2024 = df[(df[\"Created Date\"] &gt;= \"2023-12-31\") &\n                           (df[\"Created Date\"] &lt;= \"2024-12-31\")].copy()\n\n# Extract date and ensure proper formatting\ndf_2024[\"Date\"] = df_2024[\"Created Date\"].dt.date\ndf_2024[\"Zipcode\"] = df_2024[\"Incident Zip\"].astype(str)\n\n# Identify complaint types\ndf_2024[\"SFcount\"] = df_2024[\"Descriptor\"].\\\nstr.contains(\"Street Flooding\", na=False).astype(int)\ndf_2024[\"CBcount\"] = df_2024[\"Descriptor\"].\\\nstr.contains(\"Catch Basin Clogged\", na=False).astype(int)\n\n# Aggregate counts by zip code and date\ndf_grouped = df_2024.groupby([\"Zipcode\", \"Date\"])[[\"SFcount\", \"CBcount\"]].sum().reset_index()\n\n# Generate a full range of dates including 2023-12-31 for lag calculation\nall_dates = pd.date_range(start=\"2023-12-31\", end=\"2024-12-31\")\nall_zipcodes = df_grouped[\"Zipcode\"].unique()\n\n# Create a complete grid of all zip codes and dates\nmulti_index = pd.MultiIndex.from_product([all_zipcodes, all_dates], names=[\"Zipcode\", \"Date\"])\nfull_df = pd.DataFrame(index=multi_index).reset_index()\n\n# Ensure 'Date' is in datetime format\nfull_df[\"Date\"] = pd.to_datetime(full_df[\"Date\"])\ndf_grouped[\"Date\"] = pd.to_datetime(df_grouped[\"Date\"])\n\n# Merge to include all combinations and fill missing values with 0\ndf_final = full_df.merge(df_grouped, on=[\"Zipcode\", \"Date\"], how=\"left\").fillna(0)\n\n# Convert counts to integers\ndf_final[\"SFcount\"] = df_final[\"SFcount\"].astype(int)\ndf_final[\"CBcount\"] = df_final[\"CBcount\"].astype(int)\n\n# Add lag-1 variable for CBcount\ndf_final[\"CBcount_Lag1\"] = df_final.groupby(\"Zipcode\")[\"CBcount\"].shift(1)\n\ndf_final.head()\n\n/var/folders/cq/5ysgnwfn7c3g0h46xyzvpj800000gn/T/ipykernel_595/2628519755.py:3: UserWarning:\n\nCould not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n\n\n\n\n\n\n\n\n\n\nZipcode\nDate\nSFcount\nCBcount\nCBcount_Lag1\n\n\n\n\n0\n10001\n2023-12-31\n0\n0\nNaN\n\n\n1\n10001\n2024-01-01\n0\n0\n0.0\n\n\n2\n10001\n2024-01-02\n0\n0\n0.0\n\n\n3\n10001\n2024-01-03\n0\n0\n0.0\n\n\n4\n10001\n2024-01-04\n0\n0\n0.0\n\n\n\n\n\n\n\nNow let’s fit a Poisson model.\n\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\n# Filter out 2023-12-31 since it has missing values for lagged CBcount\ndf_model = df_final[df_final[\"Date\"] &gt;= \"2024-01-01\"].copy()\n\n# Fit Poisson regression\npoisson_model = smf.glm(\"SFcount ~ CBcount_Lag1\", \n                         data=df_model, \n                         family=sm.families.Poisson()).fit()\n\npoisson_model.summary()\n\n\nGeneralized Linear Model Regression Results\n\n\nDep. Variable:\nSFcount\nNo. Observations:\n67344\n\n\nModel:\nGLM\nDf Residuals:\n67342\n\n\nModel Family:\nPoisson\nDf Model:\n1\n\n\nLink Function:\nLog\nScale:\n1.0000\n\n\nMethod:\nIRLS\nLog-Likelihood:\n-13032.\n\n\nDate:\nWed, 16 Apr 2025\nDeviance:\n21120.\n\n\nTime:\n09:26:53\nPearson chi2:\n1.50e+05\n\n\nNo. Iterations:\n21\nPseudo R-squ. (CS):\n0.003481\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n-3.1437\n0.018\n-170.303\n0.000\n-3.180\n-3.107\n\n\nCBcount_Lag1\n0.2232\n0.009\n26.008\n0.000\n0.206\n0.240\n\n\n\n\n\nThen we fit a NB model.\n\n# Fit Negative Binomial regression\nnb_model = smf.negativebinomial(\"SFcount ~ CBcount_Lag1\", \n                    data=df_model).fit()\n\nnb_model.summary()\n\nOptimization terminated successfully.\n         Current function value: 0.193513\n         Iterations: 1\n         Function evaluations: 7\n         Gradient evaluations: 7\n\n\n/Users/junyan/work/teaching/ids-s25/ids-s25/.ids-s25/lib/python3.13/site-packages/statsmodels/base/model.py:595: HessianInversionWarning:\n\nInverting hessian failed, no bse or cov_params available\n\n\n\n\nNegativeBinomial Regression Results\n\n\nDep. Variable:\nSFcount\nNo. Observations:\n67344\n\n\nModel:\nNegativeBinomial\nDf Residuals:\n67342\n\n\nMethod:\nMLE\nDf Model:\n1\n\n\nDate:\nWed, 16 Apr 2025\nPseudo R-squ.:\n-0.1198\n\n\nTime:\n09:26:54\nLog-Likelihood:\n-13032.\n\n\nconverged:\nTrue\nLL-Null:\n-11638.\n\n\nCovariance Type:\nnonrobust\nLLR p-value:\n1.000\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n-3.1437\nnan\nnan\nnan\nnan\nnan\n\n\nCBcount_Lag1\n0.2232\nnan\nnan\nnan\nnan\nnan\n\n\nalpha\n2.252e-10\nnan\nnan\nnan\nnan\nnan\n\n\n\n\n\nNote that smf.glm does not allow the dispersion parameter to be estimated; instead, it is fixed at 1.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTibshirani, R. (1996). Regression shrinkage and selection via the LASSO. Journal of the Royal Statistical Society: Series B (Methodological), 58(1), 267–288.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Statistical Tests and Models</span>"
    ]
  },
  {
    "objectID": "machinelearning.html",
    "href": "machinelearning.html",
    "title": "8  Machine Learning: Overview",
    "section": "",
    "text": "8.1 Introduction\nMachine Learning (ML) is a branch of artificial intelligence that enables systems to learn from data and improve their performance over time without being explicitly programmed. At its core, machine learning algorithms aim to identify patterns in data and use those patterns to make decisions or predictions.\nMachine learning can be categorized into three main types: supervised learning, unsupervised learning, and reinforcement learning. Each type differs in the data it uses and the learning tasks it performs, addressing addresses different tasks and problems. Supervised learning aims to predict outcomes based on labeled data, unsupervised learning focuses on discovering hidden patterns within the data, and reinforcement learning centers around learning optimal actions through interaction with an environment.\nLet’s define some notations to introduce them:",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Machine Learning: Overview</span>"
    ]
  },
  {
    "objectID": "machinelearning.html#introduction",
    "href": "machinelearning.html#introduction",
    "title": "8  Machine Learning: Overview",
    "section": "",
    "text": "\\(X\\): A set of feature vectors representing the input data. Each element \\(X_i\\) corresponds to a set of features or attributes that describe an instance of data.\n\\(Y\\): A set of labels or rewards associated with outcomes. In supervised learning, \\(Y\\) is used to evaluate the correctness of the model’s predictions. In reinforcement learning, \\(Y\\) represents the rewards that guide the learning process.\n\\(A\\): A set of possible actions in a given context. In reinforcement learning, actions \\(A\\) represent choices that can be made in response to a given situation, with the goal of maximizing a reward.\n\n\n8.1.1 Supervised Learning\nSupervised learning is the most widely used type of machine learning. In supervised learning, we have both feature vectors \\(X\\) and their corresponding labels \\(Y\\). The objective is to train a model that can predict \\(Y\\) based on \\(X\\). This model is trained on labeled examples, where the correct outcome is known, and it adjusts its internal parameters to minimize the error in its predictions, which occurs as part of the cross-validation process.\nKey tasks in supervised learning include:\n\nClassification: Assigning data points to predefined categories or classes.\nRegression: Predicting a continuous value based on input data.\n\nIn supervised learning, the data consists of both feature vectors \\(X\\) and labels \\(Y\\), namely, \\((X, Y)\\).\n\n\n8.1.2 Unsupervised Learning\nUnsupervised learning involves learning patterns from data without any associated labels or outcomes. The objective is to explore and identify hidden structures in the feature vectors \\(X\\). Since there are no ground-truth labels \\(Y\\) to guide the learning process, the algorithm must discover patterns on its own. This is particularly useful when subject matter experts are unsure of common properties within a data set.\nCommon tasks in unsupervised learning include:\n\nClustering: Grouping similar data points together based on certain features.\nDimension Reduction: Simplifying the input data by reducing the number of features while preserving essential patterns.\n\nIn unsupervised learning, the data consists solely of feature vectors \\(X\\).\n\n\n8.1.3 Reinforcement Learning\nReinforcement learning involves learning how to make a sequence of decisions to maximize a cumulative reward. Unlike supervised learning, where the model learns from a static dataset of labeled examples, reinforcement learning involves an agent that interacts with an environment by taking actions \\(A\\), receiving feedback in the form of rewards \\(Y\\), and learning over time which actions lead to the highest cumulative reward.\nThe process in reinforcement learning involves:\n\nStates: The context or environment the agent is in, represented by feature vectors \\(X\\).\nActions: The set of possible choices the agent can make in response to the current state, denoted as \\(A\\).\nRewards: Feedback the agent receives after taking an action, which guides the learning process.\n\nIn reinforcement learning, the data consists of feature vectors \\(X\\), actions \\(A\\), and rewards \\(Y\\), namely, \\((X, A, Y)\\).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Machine Learning: Overview</span>"
    ]
  },
  {
    "objectID": "machinelearning.html#bias-variance-tradeoff",
    "href": "machinelearning.html#bias-variance-tradeoff",
    "title": "8  Machine Learning: Overview",
    "section": "8.2 Bias-Variance Tradeoff",
    "text": "8.2 Bias-Variance Tradeoff\nThe variance-bias trade-off is a core concept in machine learning that explains the relationship between the complexity of a model, its performance on training data, and its ability to generalize to unseen data. It applies to both supervised and unsupervised learning, though it manifests differently in each.\n\n8.2.1 Bias\nBias refers to the error introduced by approximating a complex real-world problem with a simplified model. A model with high bias makes strong assumptions about the data, leading to oversimplified patterns and poor performance on both the training data and new data. High bias results in underfitting, where the model fails to capture important trends in the data.\n\nExample (Supervised): In supervised learning, using a linear regression model to fit data that has a nonlinear relationship results in high bias because the model cannot capture the complexity of the data.\nExample (Unsupervised): In clustering (an unsupervised task), setting the number of clusters too low (e.g., forcing data into two clusters when more exist) leads to high bias, as the model oversimplifies the underlying structure.\n\n\n\n8.2.2 Variance\nVariance refers to the model’s sensitivity to small changes in the training data. A model with high variance will adapt closely to the training data, potentially capturing noise or fluctuations that are not representative of the general data distribution. High variance leads to overfitting, where the model performs well on training data but poorly on new, unseen data.\n\nExample (Supervised): A decision tree with many branches can exhibit high variance. The model perfectly fits the training data but may perform poorly on test data because it overfits to specific idiosyncrasies in the training set.\nExample (Unsupervised): In clustering, setting the number of clusters too high or fitting overly flexible cluster shapes (e.g., in Gaussian Mixture Models) can lead to overfitting, where the model captures noise and splits data unnecessarily.\n\n\n\n8.2.3 The Trade-Off\nThe bias-variance trade-off reflects the tension between bias and variance. As model complexity increases:\n\nBias decreases: The model becomes more flexible and can capture more details of the data.\nVariance increases: The model becomes more sensitive to the particular training data, potentially capturing noise.\n\nConversely, a simpler model will:\n\nHave high bias: It may not capture all relevant patterns in the data.\nHave low variance: It will be less sensitive to fluctuations in the data and is more likely to generalize well to unseen data.\n\n\n\n8.2.4 Bias-Variance in Supervised Learning\nIn supervised learning, the goal is to strike the right balance between bias and variance to minimize prediction error. This balance is critical for developing models that generalize well to new data. The total error of a model can be decomposed into:\n\\[\n\\text{Total Error} = \\text{Bias}^2 + \\text{Variance}\n+\\text{Irreducible Error}.\n\\]\n\nBias: The error from using a model that is too simple.\nVariance: The error from using a model that is too complex and overfits the training data.\nIrreducible Error: This is the noise inherent in the data itself, which cannot be eliminated no matter how well the model is tuned.\n\n\n\n8.2.5 Bias-Variance in Unsupervised Learning\nIn unsupervised learning, the bias-variance trade-off is less formally defined but still relevant. For example:\n\nIn clustering, choosing the wrong number of clusters can lead to either high bias (too few clusters, oversimplifying the data) or high variance (too many clusters, overfitting the data).\nIn dimensionality reduction, keeping too few components in Principal Component Analysis (PCA) increases bias by losing important information, while keeping too many components retains noise, increasing variance.\n\nIn unsupervised learning, balancing bias and variance typically involves tuning hyperparameters (e.g., number of clusters, number of components) to find the right complexity level.\n\n\n8.2.6 Striking the Right Balance\nTo strike the balance between bias and variance in both supervised and unsupervised learning, techniques such as regularization, early stopping, cross-validation, and hyperparameter tuning are essential. These techniques help ensure the model is complex enough to capture patterns in the data but not so complex that it overfits to noise or irrelevant details.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Machine Learning: Overview</span>"
    ]
  },
  {
    "objectID": "machinelearning.html#crossvalidation",
    "href": "machinelearning.html#crossvalidation",
    "title": "8  Machine Learning: Overview",
    "section": "8.3 Crossvalidation",
    "text": "8.3 Crossvalidation\nCross-validation is a technique used to evaluate machine learning models and tune hyperparameters by splitting the dataset into multiple subsets. This approach helps to avoid overfitting and provides a better estimate of the model’s performance on unseen data. Cross-validation is especially useful for managing the bias-variance trade-off by allowing you to test how well the model generalizes without relying on a single train-test split.\n\n8.3.1 K-Fold Cross-Validation\nThe most commonly used method is \\(k\\)-fold cross-validation:\n\nSplit the data: The dataset is divided into \\(k\\) equally-sized folds (subsets).\nTrain on \\(k - 1\\) folds: The model is trained on \\(k - 1\\) folds, leaving one fold as a test set.\nTest on the remaining fold: The model’s performance is evaluated on the fold that was left out.\nRepeat: This process is repeated \\(k\\) times, with each fold used once as the test set.\nAverage performance: The final cross-validation score is the average performance across all \\(k\\) iterations.\n\nBy averaging the results across multiple test sets, cross-validation provides a more robust estimate of model performance and helps avoid overfitting or underfitting to any particular training-test split.\nLeave-One-Out Cross-Validation (LOOCV) takes each observation as one fold. The dataset is split into \\(n\\) subsets (where \\(n\\) is the sample size), with each sample acting as a test set once. While this method provides the most exhaustive evaluation, it can be computationally expensive for large datasets.\n\n\n8.3.2 Benefits of Cross-Validation\n\nPrevents overfitting: By testing the model on multiple subsets of data, cross-validation helps to identify if the model is too complex and overfits to the training data.\nPrevents underfitting: If the model performs poorly across all folds, it may indicate that the model is too simple (high bias).\nBetter estimation: Cross-validation gives a better estimate of how the model will perform on unseen data compared to a single train-test split.\n\n\n\n8.3.3 Cross-Validation in Unsupervised Learning\nWhile cross-validation is most commonly used in supervised learning, it can also be applied to unsupervised learning through:\n\nStability testing: Running unsupervised algorithms (e.g., clustering) on different data splits and measuring the stability of the results (e.g., using the silhouette score).\nInternal validation metrics: In clustering, internal metrics like the silhouette score or Davies-Bouldin index can be used to evaluate the quality of clustering across different data splits.\n\nThe bias-variance trade-off is a universal problem in machine learning, affecting both supervised and unsupervised models. Cross-validation is a powerful tool for controlling this trade-off by providing a reliable estimate of model performance and helping to fine-tune model complexity. By balancing bias and variance through careful model selection, regularization, and cross-validation, you can develop models that generalize well to unseen data without overfitting or underfitting.\n\n\n8.3.4 A Curve-Fitting with Splines: An Example\nOverfitting occurs when a model becomes overly complex and starts to capture not just the underlying patterns in the data but also the noise or random fluctuations. This can lead to poor generalization to new, unseen data. A clear sign of overfitting is when a model performs very well on the training data but performs poorly on test data, as it fails to generalize beyond the data it was trained on.\nIn this example, we illustrate overfitting using cubic spline regression with different numbers of knots. Splines are a flexible tool that allow for piecewise polynomial regression, with knots defining where the pieces of the polynomial meet. The more knots we use, the more flexible the model becomes, which can potentially lead to overfitting.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import SplineTransformer\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import mean_squared_error\n\ndef true_function(X):\n    return np.sin(1.5 * X) + 0.5 * np.cos(0.5 * X) + np.sin(2 * X)\n\n# Generate synthetic data using the more complex true function\nX = np.sort(np.random.rand(200) * 10).reshape(-1, 1)\ny = true_function(X).ravel() + np.random.normal(0, 0.2, X.shape[0])\n\n# Split data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Function to explore overfitting by plotting both errors and fitted curves\ndef overfitting_example_with_fitted_curves(X_train, y_train, X_test, y_test, knots_list):\n    train_errors = []\n    test_errors = []\n    \n    # Generate fine grid for plotting the true curve and fitted models\n    X_line = np.linspace(0, 10, 1000).reshape(-1, 1)\n    y_true = true_function(X_line)\n    \n    # Plot the true function and observed data\n    plt.figure(figsize=(10, 6))\n    plt.scatter(X_train, y_train, color='blue', label='Training data', alpha=0.6)\n    plt.plot(X_line, y_true, label='True function', color='black', linestyle='--')\n    \n    for n_knots in knots_list:\n        # Create a spline model with fixed degree = 3 (cubic) and varying knots\n        spline = SplineTransformer(degree=3, n_knots=n_knots, include_bias=False)\n        model = make_pipeline(spline, LinearRegression())\n        \n        # Fit the model to training data\n        model.fit(X_train, y_train)\n        \n        # Predict on training and test data\n        y_pred_train = model.predict(X_train)\n        y_pred_test = model.predict(X_test)\n        y_pred = model.predict(X_line)\n        \n        # Calculate training and test errors (mean squared error)\n        train_errors.append(mean_squared_error(y_train, y_pred_train))\n        test_errors.append(mean_squared_error(y_test, y_pred_test))\n        \n        # Plot the fitted curve\n        plt.plot(X_line, y_pred, label=f'{n_knots} Knots (Fit)', alpha=0.7)\n\n    print(train_errors, test_errors)\n    \n    plt.title('Overfitting Example: Fitted Curves and Observed Data')\n    plt.xlabel('X')\n    plt.ylabel('y')\n    plt.legend()\n    plt.show()\n    \n    # Plot training and test error separately\n    plt.figure(figsize=(10, 6))\n    plt.plot(knots_list, train_errors, label='Training Error', marker='o', color='blue')\n    plt.plot(knots_list, test_errors, label='Test Error', marker='o', color='red')\n    plt.title('Training vs. Test Error with Varying Knots')\n    plt.xlabel('Number of Knots')\n    plt.ylabel('Mean Squared Error')\n    plt.legend()\n    plt.show()\n\n# Explore overfitting by varying the number of knots and overlaying the fitted curves\nknots_list = [6, 8, 10, 12, 14, 16]\noverfitting_example_with_fitted_curves(X_train, y_train, X_test, y_test, knots_list)\n\n[0.0941038034348758, 0.03647270930197563, 0.02687090498268356, 0.02675769996764363, 0.025503734093287652, 0.025143537696468987] [0.10692810563807131, 0.04663988297046514, 0.04752621151729337, 0.042686730415836274, 0.04142771803397926, 0.043598382263467385]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn the first plot, we fit cubic splines with 2, 4, 6, 8, 10, and 12 knots to the data. As the number of knots increases, the model becomes more flexible and better able to fit the training data. With only 2 knots, the model is quite smooth and underfits the data, capturing only broad trends but missing the detailed structure of the true underlying function. With 4 or 6 knots, the model begins to capture more of the data’s structure, balancing the bias-variance trade-off effectively. However, as we increase the number of knots to 10 and 12, the model becomes too flexible. It starts to fit the noise in the training data, producing a curve that adheres too closely to the data points. This is a classic case of overfitting: the model fits the training data very well, but it no longer generalizes to new data.\nIn the second plot, we observe the training error and test error as the number of knots increases. As expected, the training error consistently decreases as the number of knots increases, since a more complex model can fit the training data better. However, the test error tells a different story. Initially, the test error decreases as the model becomes more flexible, indicating that the model is learning meaningful patterns from the data. But after a certain point, the test error begins to increase, signaling overfitting.\nThis is a key insight into the bias-variance trade-off. While adding more complexity (in this case, more knots) reduces bias and improves fit on the training data, it also increases variance, making the model more sensitive to fluctuations and noise in the data. This results in worse performance on test data, as the model becomes too specialized to the training set.\nThe example clearly demonstrates how overfitting can occur when the model becomes too complex. In practice, it’s important to find a balance between underfitting (high bias) and overfitting (high variance). Techniques such as cross-validation, regularization, or limiting model complexity (e.g., setting a reasonable number of knots in spline regression) can help manage this trade-off and produce models that generalize well to unseen data.\nBy tuning the number of knots or other model parameters, we can achieve a model that strikes the right balance, capturing the true patterns in the data without fitting the noise.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Machine Learning: Overview</span>"
    ]
  },
  {
    "objectID": "supervised.html",
    "href": "supervised.html",
    "title": "9  Supervised Learning",
    "section": "",
    "text": "9.1 Decision Trees: Foundation\nDecision trees are widely used supervised learning models that predict the value of a target variable by iteratively splitting the dataset based on decision rules derived from input features. The model functions as a piecewise constant approximation of the target function, producing clear, interpretable rules that are easily visualized and analyzed (Breiman et al., 1984). Decision trees are fundamental in both classification and regression tasks, serving as the building blocks for more advanced ensemble models such as Random Forests and Gradient Boosting Machines.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Supervised Learning</span>"
    ]
  },
  {
    "objectID": "supervised.html#decision-trees-foundation",
    "href": "supervised.html#decision-trees-foundation",
    "title": "9  Supervised Learning",
    "section": "",
    "text": "9.1.1 Algorithm Formulation\nThe core mechanism of a decision tree algorithm is the identification of optimal splits that partition the data into subsets that are increasingly homogeneous with respect to the target variable. At any node \\(m\\), the data subset is denoted as \\(Q_m\\) with a sample size of \\(n_m\\). The objective is to find a candidate split \\(\\theta\\), defined as a threshold for a given feature, that minimizes an impurity or loss measure \\(H\\).\nWhen a split is made at node \\(m\\), the data is divided into two subsets: \\(Q_{m,l}\\) (left node) with sample size \\(n_{m,l}\\), and \\(Q_{m,r}\\) (right node) with sample size \\(n_{m,r}\\). The split quality, measured by \\(G(Q_m, \\theta)\\), is given by:\n\\[\nG(Q_m, \\theta) = \\frac{n_{m,l}}{n_m} H(Q_{m,l}(\\theta)) +\n\\frac{n_{m,r}}{n_m} H(Q_{m,r}(\\theta)).\n\\]\nThe algorithm aims to identify the split that minimizes the impurity:\n\\[\n\\theta^* = \\arg\\min_{\\theta} G(Q_m, \\theta).\n\\]\nThis process is applied recursively at each child node until a stopping condition is met.\n\nStopping Criteria: The algorithm stops when the maximum tree depth is reached or when the node sample size falls below a preset threshold.\nPruning: Reduce the complexity of the final tree by removing branches that add little predictive value. This reduces overfitting and improves the generalization accuracy of the model.\n\n\n\n9.1.2 Search Space for Possible Splits\nAt each node in the decision tree, the search space for possible splits comprises all features in the dataset and potential thresholds derived from the values of each feature. For a given feature, the algorithm considers each unique value in the current node’s subset as a possible split point. The potential thresholds are typically set as midpoints between consecutive unique values, ensuring the data is partitioned effectively.\nFormally, let the feature set be \\(\\{X_1, X_2, \\ldots, X_p\\}\\), where \\(p\\) is the total number of features, and let the unique values of feature \\(X_j\\) at node \\(m\\) be denoted by \\(\\{v_{j,1}, v_{j,2}, \\ldots, v_{j,k_j}\\}\\). The search space at node \\(m\\) includes:\n\nFeature candidates: \\(\\{X_1, X_2, \\ldots, X_p\\}\\).\nThreshold candidates for \\(X_j\\): \\[\n\\left\\{ \\frac{v_{j,i} + v_{j,i+1}}{2} \\mid 1 \\leq i &lt; k_j \\right\\}.\n\\]\n\nThe search space therefore encompasses all combinations of features and their respective thresholds. While the complexity of this search can be substantial, particularly for high-dimensional data or features with numerous unique values, efficient algorithms use sorting and single-pass scanning techniques to mitigate the computational cost.\n\n\n9.1.3 Metrics\n\n9.1.3.1 Classification\nIn decision tree classification, several criteria can be used to measure the quality of a split at each node. These criteria are based on how “pure” the resulting nodes are after the split. A pure node contains samples that predominantly belong to a single class. The goal is to minimize impurity, leading to nodes that are as homogeneous as possible.\n\nGini Index: The Gini index measures the impurity of a node by calculating the probability of randomly choosing two different classes. A perfect split (all instances belong to one class) has a Gini index of 0. At node \\(m\\), the Gini index is \\[\nH(Q_m) = \\sum_{k=1}^{K} p_{mk} (1 - p_{mk}),\n\\] where \\(p_{mk}\\) is the proportion of samples of class \\(k\\) at node \\(m\\); and\\(K\\) is the total number of classes The Gini index is often preferred for its speed and simplicity, and it’s used by default in many implementations of decision trees, including sklearn.\nEntropy (Information Gain): Entropy is another measure of impurity, derived from information theory. It quantifies the “disorder” of the data at a node. Lower entropy means higher purity. At node \\(m\\), it is defined as \\[\nH(Q_m) = - \\sum_{k=1}^{K} p_{mk} \\log p_{mk}\n\\] Entropy is commonly used in decision tree algorithms like ID3 and C4.5. The choice between Gini and entropy often depends on specific use cases, but both perform similarly in practice.\nMisclassification Error: Misclassification error focuses solely on the most frequent class in the node. It measures the proportion of samples that do not belong to the majority class. Although less sensitive than Gini and entropy, it can be useful for classification when simplicity is preferred. At node \\(m\\), it is defined as \\[\nH(Q_m) = 1 - \\max_k p_{mk},\n\\] where \\(\\max_k p_{mk}\\) is the largest proportion of samples belonging to any class \\(k\\).\n\n\n\n9.1.3.2 Regression Criteria\nIn decision tree regression, different criteria are used to assess the quality of a split. The goal is to minimize the spread or variance of the target variable within each node.\n\nMean Squared Error (MSE): Mean squared error is the most common criterion used in regression trees. It measures the average squared difference between the actual values and the predicted values (mean of the target in the node). The smaller the MSE, the better the fit. At node \\(m\\), it is \\[\nH(Q_m) = \\frac{1}{n_m} \\sum_{i=1}^{n_m} (y_i - \\bar{y}_m)^2,\n\\] where\n\n\\(y_i\\) is the actual value for sample \\(i\\);\n\\(\\bar{y}_m\\) is the mean value of the target at node \\(m\\);\n\\(n_m\\) is the number of samples at node \\(m\\).\n\nMSE works well when the target is continuous and normally distributed.\nHalf Poisson Deviance (for count targets): When dealing with count data, the Poisson deviance is used to model the variance in the number of occurrences of an event. It is well-suited for target variables representing counts (e.g., number of occurrences of an event). At node \\(m\\), it is \\[\nH(Q_m) = \\sum_{i=1}^{n_m} \\left( y_i \\log\\left(\\frac{y_i}{\\hat{y}_i}\\right) - (y_i - \\hat{y}_i) \\right),\n\\] where \\(\\hat{y}_i\\) is the predicted count. This criterion is especially useful when the target variable represents discrete counts, such as predicting the number of occurrences of an event.\nMean Absolute Error (MAE): Mean absolute error is another criterion that minimizes the absolute differences between actual and predicted values. While it is more robust to outliers than MSE, it is slower computationally due to the lack of a closed-form solution for minimization. At node \\(m\\), it is \\[\nH(Q_m) = \\frac{1}{n_m} \\sum_{i=1}^{n_m} |y_i - \\bar{y}_m|.\n\\] MAE is useful when you want to minimize large deviations and can be more robust in cases where outliers are present in the data.\n\n\n\n9.1.3.3 Summary\nIn decision trees, the choice of splitting criterion depends on the type of task (classification or regression) and the nature of the data. For classification tasks, the Gini index and entropy are the most commonly used, with Gini offering simplicity and speed, and entropy providing a more theoretically grounded approach. Misclassification error can be used for simpler cases. For regression tasks, MSE is the most popular choice, but Poisson deviance and MAE are useful for specific use cases such as count data and robust models, respectively.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Supervised Learning</span>"
    ]
  },
  {
    "objectID": "supervised.html#gradient-boosted-models",
    "href": "supervised.html#gradient-boosted-models",
    "title": "9  Supervised Learning",
    "section": "9.2 Gradient-Boosted Models",
    "text": "9.2 Gradient-Boosted Models\nGradient boosting is a powerful ensemble technique in machine learning that combines multiple weak learners into a strong predictive model. Unlike bagging methods, which train models independently, gradient boosting fits models sequentially, with each new model correcting errors made by the previous ensemble (Friedman, 2001). While decision trees are commonly used as weak learners, gradient boosting can be generalized to other base models. This iterative method optimizes a specified loss function by repeatedly adding models designed to reduce residual errors.\n\n9.2.1 Introduction\nGradient boosting builds on the general concept of boosting, aiming to construct a strong predictor from an ensemble of sequentially trained weak learners. The weak learners are often shallow decision trees (stumps), linear models, or generalized additive models (Hastie et al., 2009). Each iteration adds a new learner focusing primarily on the data points poorly predicted by the existing ensemble, thereby progressively enhancing predictive accuracy.\nGradient boosting’s effectiveness stems from:\n\nError Correction: Each iteration specifically targets previous errors, refining predictive accuracy.\nWeighted Learning: Iteratively focuses more heavily on difficult-to-predict data points.\nFlexibility: Capable of handling diverse loss functions and various types of predictive tasks.\n\nThe effectiveness of gradient-boosted models has made them popular across diverse tasks, including classification, regression, and ranking. Gradient boosting forms the foundation for algorithms such as XGBoost (Chen & Guestrin, 2016), LightGBM (Ke et al., 2017), and CatBoost (Prokhorenkova et al., 2018), known for their high performance and scalability.\n\n\n9.2.2 Gradient Boosting Process\nGradient boosting builds an ensemble by iteratively minimizing the residual errors from previous models. This iterative approach optimizes a loss function, \\(L(y, F(x))\\), where \\(y\\) represents the observed target variable and \\(F(x)\\) the model’s prediction for a given feature vector \\(x\\).\nKey concepts:\n\nLoss Function: Guides model optimization, such as squared error for regression or logistic loss for classification.\nLearning Rate: Controls incremental updates, balancing training speed and generalization.\nRegularization: Reduces overfitting through tree depth limitation, subsampling, and L1/L2 penalties.\n\n\n9.2.2.1 Model Iteration\nThe gradient boosting algorithm proceeds as follows:\n\nInitialization: Define a base model \\(F_0(x)\\), typically the mean of the target variable for regression or the log-odds for classification.\nIterative Boosting: At each iteration \\(m\\):\n\nCompute pseudo-residuals representing the negative gradient of the loss function at the current predictions. For each observation \\(i\\): \\[\nr_i^{(m)} = -\\left.\\frac{\\partial L(y_i, F(x_i))}{\\partial F(x_i)}\\right|_{F(x)=F_{m-1}(x)},\n\\] where \\(x_i\\) and \\(y_i\\) denote the feature vector and observed value for the \\(i\\)-th observation, respectively.\nFit a new weak learner \\(h_m(x)\\) to these residuals.\nUpdate the model: \\[\nF_m(x) = F_{m-1}(x) + \\eta \\, h_m(x),\n\\] where \\(\\eta\\) is a small positive learning rate (e.g., 0.01–0.1), controlling incremental improvement and reducing overfitting.\n\nFinal Model: After \\(M\\) iterations, the ensemble model is: \\[\n  F_M(x) = F_0(x) + \\sum_{m=1}^M \\eta \\, h_m(x).\n  \\]\n\nStochastic gradient boosting is a variant that enhances gradient boosting by introducing randomness through subsampling at each iteration, selecting a random fraction of data points (typically 50%–80%) to fit the model (Friedman, 2002). This randomness helps reduce correlation among trees, improve model robustness, and reduce the risk of overfitting.\n\n\n\n9.2.3 Demonstration\nHere’s a practical example using scikit-learn to demonstrate gradient boosting on the California housing dataset. First, import necessary libraries and load the data:\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\n# Load data\nhousing = fetch_california_housing(as_frame=True)\nX, y = housing.data, housing.target\n\nNext, split the dataset into training and testing sets:\n\n# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=20250407\n)\n\nThen, set up and train a stochastic gradient boosting model:\n\n# Gradient Boosting Model with stochastic subsampling\ngbm = GradientBoostingRegressor(\n    n_estimators=200,\n    learning_rate=0.1,\n    max_depth=3,\n    subsample=0.7,  # stochastic gradient boosting\n    random_state=20250408\n)\n\ngbm.fit(X_train, y_train)\n\nGradientBoostingRegressor(n_estimators=200, random_state=20250408,\n                          subsample=0.7)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GradientBoostingRegressor?Documentation for GradientBoostingRegressoriFittedGradientBoostingRegressor(n_estimators=200, random_state=20250408,\n                          subsample=0.7) \n\n\nFinally, make predictions and evaluate the model performance:\n\n# Predictions\ny_pred = gbm.predict(X_test)\n\n# Evaluate\nmse = mean_squared_error(y_test, y_pred)\nprint(f\"Test MSE: {mse:.4f}\")\n\nTest MSE: 0.2697\n\n\n\n\n9.2.4 XGBoost: Extreme Gradient Boosting\nXGBoost is a scalable and efficient implementation of gradient-boosted decision trees (Chen & Guestrin, 2016). It has become one of the most widely used machine learning methods for structured data due to its high predictive performance, regularization capabilities, and speed. XGBoost builds an ensemble of decision trees in a stage-wise fashion, minimizing a regularized objective that balances training loss and model complexity.\nThe core idea of XGBoost is to fit each new tree to the gradient of the loss function with respect to the model’s predictions. Unlike traditional boosting algorithms like AdaBoost, which use only first-order gradients, XGBoost optionally uses second-order derivatives (Hessians), enabling better convergence and stability (Friedman, 2001).\nXGBoost is widely used in data science competitions and real-world applications. It supports regularization (L1 and L2), handles missing values internally, and is designed for distributed computing.\nXGBoost builds upon the same foundational idea as gradient boosted machines—sequentially adding trees to improve the predictive model— but introduces a number of enhancements:\n\n\n\n\n\n\n\n\nAspect\nTraditional GBM\nXGBoost\n\n\n\n\nImplementation\nBasic gradient boosting\nOptimized, regularized boosting\n\n\nRegularization\nShrinkage only\nL1 and L2 regularization\n\n\nLoss Optimization\nFirst-order gradients\nFirst- and second-order\n\n\nMissing Data\nRequires manual imputation\nHandled automatically\n\n\nTree Construction\nDepth-wise\nLevel-wise (faster)\n\n\nParallelization\nLimited\nBuilt-in\n\n\nSparsity Handling\nNo\nYes\n\n\nObjective Functions\nFew options\nCustom supported\n\n\nCross-validation\nExternal via GridSearchCV\nBuilt-in xgb.cv\n\n\n\nXGBoost is therefore more suitable for large-scale problems and provides better generalization performance in many practical tasks.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Supervised Learning</span>"
    ]
  },
  {
    "objectID": "supervised.html#variable-importance-metrics-in-supervised-learning",
    "href": "supervised.html#variable-importance-metrics-in-supervised-learning",
    "title": "9  Supervised Learning",
    "section": "9.3 Variable Importance Metrics in Supervised Learning",
    "text": "9.3 Variable Importance Metrics in Supervised Learning\nThis section was written by Xavier Febles, a junior majoring in Statistical Data Science at the University of Connecticut.\nThis section explores different methods for interpreting machine learning models in Python. We’ll use scikit-learn to train models and calculate feature importances through Gini importance from Random Forests, Permutation importance for evaluating the effect of shuffling features, and Lasso regression to observe how regularization impacts feature coefficients. We’ll also use pandas for data handling and SHAP for visualizing global and local model explanations, helping us better understand feature contributions and interactions in our models.\n\n9.3.1 Introduction\nVariable importance metrics help measure the contribution of each feature in predicting the target variable. These metrics improve model interpretability, assist in feature selection, and enhance performance.\n\n9.3.1.1 Topics Covered:\n\nGini Importance (Tree-based models)\nPermutation Importance (Model-agnostic)\nRegularization (Lasso)\nSHAP (Shapley Additive Explanations)\n\n\n\n9.3.1.2 What are Variable Importance Metrics?\nVariable importance metrics assess how much each feature contributes to the prediction outcome in supervised learning tasks.\n\n\n9.3.1.3 Types of Variable Importance Metrics\n\nTree-based methods: e.g., Gini Importance\nEnsemble models: e.g., Random Forest\nLinear models: e.g., Regularization (Lasso, Ridge)\nModel-agnostic methods: e.g., Permutation Importance, SHAP\n\n\n\n\n9.3.2 Gini Importance\nGini Importance measures the mean decrease in impurity from splits using a feature. Higher values indicate greater influence on predictions. It is calculated based on how much a feature contributes to reducing uncertainty at each split in the decision trees.\n\nfrom sklearn.ensemble import RandomForestClassifier\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import make_classification\n\n# Simulate dataset\nX, y = make_classification(n_samples=1000, n_features=10, n_informative=5, random_state=42)\nX = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(1, 11)])\n\n# Split dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train model\nrf = RandomForestClassifier(random_state=42)\nrf.fit(X_train, y_train)\n\n# Gini importance\ndf_gini = pd.DataFrame({\n    'feature': X_train.columns,\n    'gini_importance': rf.feature_importances_\n}).sort_values(by='gini_importance', ascending=False)\n\nprint(df_gini)\n\n      feature  gini_importance\n4   feature_5         0.206109\n9  feature_10         0.167711\n0   feature_1         0.159516\n5   feature_6         0.143680\n3   feature_4         0.120340\n6   feature_7         0.069936\n1   feature_2         0.065147\n8   feature_9         0.025048\n2   feature_3         0.021608\n7   feature_8         0.020905\n\n\nA classification dataset was simulated with 1,000 samples and 10 features. The data was split into training and testing sets, using an 80/20 split. A Random Forest Classifier was trained on the training data to build the model. Gini importance was then calculated for each feature to understand their contribution to the model’s decisions.\nThe output ranks the features by their importance scores, showing that feature_5, feature_10, and feature_1 are the top three most influential features. Feature_5 has the highest importance, indicating it plays the largest role in predicting the target variable in this simulated dataset.\n\n\n9.3.3 Permutation Importance\nPermutation Importance measures the drop in model performance after shuffling a feature. Positive values suggest that these features contribute positively to the model, while negative values indicate they may harm performance.\nThe standard deviation of Permutation Importance shows the variability in each feature’s importance score.\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import make_classification\nfrom sklearn.inspection import permutation_importance\n\n# Simulate a dataset\nX, y = make_classification(n_samples=1000, n_features=10, n_informative=5, random_state=42)\n\n# Convert to a DataFrame for easier handling\nX = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(1, 11)])\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Fit a RandomForest model\nrf = RandomForestClassifier(random_state=42)\nrf.fit(X_train, y_train)\n\n# Get Permutation Importance\nperm_importance = permutation_importance(rf, X_test, y_test, random_state=42)\n\n# Create a DataFrame to display the results\ndf_perm = pd.DataFrame({\n    'feature': X_test.columns,\n    'permutation_importance_mean': perm_importance.importances_mean,\n    'permutation_importance_std': perm_importance.importances_std\n}).sort_values(by='permutation_importance_mean', ascending=False)\n\n# Display the feature importances\nprint(df_perm)\n\n      feature  permutation_importance_mean  permutation_importance_std\n4   feature_5                        0.185                    0.028810\n5   feature_6                        0.110                    0.019748\n0   feature_1                        0.096                    0.028531\n9  feature_10                        0.035                    0.013038\n3   feature_4                        0.025                    0.011402\n1   feature_2                        0.007                    0.002449\n6   feature_7                       -0.002                    0.006782\n8   feature_9                       -0.003                    0.002449\n7   feature_8                       -0.007                    0.006000\n2   feature_3                       -0.008                    0.004000\n\n\nThis code demonstrates how to compute Permutation Importance using a RandomForest model. It starts by generating a synthetic dataset with 1,000 samples and 10 features, of which 5 are informative. The dataset is split into training and testing sets. A RandomForestClassifier is trained on the training data, and then the permutation importance is calculated on the test set. The permutation importance measures how much the model’s accuracy drops when the values of each feature are randomly shuffled, thus breaking the relationship between the feature and the target. Finally, the results are stored in a DataFrame and sorted by importance.\nThe output is a table listing each feature along with its mean and standard deviation of permutation importance. Higher mean values indicate that shuffling the feature leads to a larger drop in model performance, meaning the feature is important for predictions. For example, feature_5 has the highest importance, while features with negative values like feature_3 and feature_8 may contribute noise or have little predictive power.\n\n\n9.3.4 Regularization (Lasso)\nRegularization techniques shrink the coefficients to reduce overfitting. Lasso, in particular, drives some coefficients to zero, promoting feature selection. By doing so, it simplifies the model and helps improve interpretability, especially when dealing with high-dimensional data.\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import Lasso\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import make_regression\n\n# Simulate a dataset\nX, y = make_regression(n_samples=1000, n_features=10, n_informative=5, noise=0.1, random_state=42)\n\n# Convert to a DataFrame for easier handling\nX = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(1, 11)])\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Fit a Lasso model\nlasso = Lasso(alpha=0.05, random_state=42)\nlasso.fit(X_train, y_train)\n\n# Get Lasso coefficients\ncoefficients = lasso.coef_\n\n# Create a DataFrame to display the results\ndf_lasso = pd.DataFrame({\n    'feature': X_train.columns,\n    'lasso_coefficient': coefficients\n}).sort_values(by='lasso_coefficient', ascending=False)\n\n# Display the coefficients\nprint(df_lasso)\n\n      feature  lasso_coefficient\n0   feature_1          58.243886\n3   feature_4          32.080140\n6   feature_7          10.253066\n9  feature_10           9.377729\n4   feature_5           7.121199\n1   feature_2          -0.000000\n5   feature_6           0.000000\n2   feature_3          -0.000000\n7   feature_8          -0.000000\n8   feature_9          -0.000000\n\n\nThis code uses Lasso regression for feature selection. It starts by creating a simulated dataset with 1,000 samples and 10 features, where 5 features are informative. The features are stored in a pandas DataFrame, and the data is split into training and testing sets. A Lasso model is set up with alpha 0.05, which controls how strongly the model penalizes large coefficients. The model is trained on the training data, and the resulting coefficients for each feature are extracted. These coefficients are then stored in a DataFrame and sorted to show which features have the most impact on the predictions.\nThe output lists each feature and its Lasso coefficient. Features with non-zero coefficients, like feature_1 and feature_4, are identified as important for the model. Features with coefficients of zero, such as feature_3 and feature_6, are excluded from the model. This shows how Lasso helps simplify the model by shrinking unimportant feature coefficients to zero, making it easier to interpret and reducing overfitting.\n\n\n9.3.5 SHAP: Introduction\n\n9.3.5.1 What is SHAP (SHAPley Additive Explanations)?\nSHAP is based on Shapley values from game theory. The goal is to fairly distribute the contribution of each feature to a prediction. SHAP is model agnostic, so it works with Linear models, Tree-based models, Deep learning models\n\n\n9.3.5.2 Why SHAP?\n\nConsistency – If a feature increases model performance, SHAP assigns it a higher value.\nLocal accuracy – SHAP values sum to the exact model prediction.\nHandles feature interactions better than other methods.\n\n\n\n\n9.3.6 SHAP: Mathematics Behind It\nThe Shapley value equation is:\nφᵢ = the sum over all subsets S of N excluding i of (|S|! × (|N| − |S| − 1)! ÷ |N|!) times [f(S ∪ {i}) − f(S)].\nWhere:\nS = subset of features N = total number of features f(S) = model output for subset ϕᵢ = contribution of feature i\nEssentially SHAP values compute the marginal contribution of a feature averaged over all possible feature combinations. The sum of all SHAP values equals the model’s total prediction.\n\n\n9.3.7 Installing SHAP\nTo install SHAP, run:\npip install shap\nMake sure your NumPy version is 2.1 or lower for compatibility.\nYou also need to have Microsoft C++ Build Tools installed.\n\n\n9.3.8 SHAP Visualizations\nThe shap package allows for for 3 main types of plots.\nshap.summary_plot(shap_values, X_test) This creates a global summary plot of feature importance. shap_values contains the SHAP values for all predictions, showing how features impact the model output. X_test provides the feature data. Each dot in the plot represents a feature’s impact for one instance, with color showing feature value. This helps identify the most influential features overall.\nshap.dependence_plot('variable_name', shap_values, X_test, interaction_index) This shows the relationship between a single feature and its SHAP value. The string 'variable_name' selects the feature to plot. shap_values provides the SHAP values, while X_test supplies feature data. The interaction_index controls which feature’s value is used for coloring, highlighting potential feature interactions.\nshap.plots.force(shap_values[index]) This creates a force plot to explain an individual prediction. shap_values[index] selects the SHAP values for one specific instance. The plot visualizes how each feature contributes to pushing the prediction higher or lower, providing an intuitive breakdown of that decision.\nImportant note: This will not be included in the code as it requires being saved separately in html or Jupyter Notebook format.\n\n9.3.8.1 Example of SHAP\nFor this example we will be using the built in sklearn diabetes dataset.\nA quick summary of each varaible:\n\nage — Age of the patient\n\nsex — Sex of the patient\n\nbmi — Body Mass Index\n\nbp — Blood pressure\n\ns1 — T-cell count\n\ns2 — LDL cholesterol\n\ns3 — HDL cholesterol\n\ns4 — Total cholesterol\n\ns5 — Serum triglycerides (blood sugar proxy)\n\ns6 — Blood sugar level\n\n\n\n9.3.8.2 SHAP Code using Linear Model\n\nimport shap\nimport sklearn\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\n\n# Load dataset\nX, y = load_diabetes(return_X_y=True, as_frame=True)\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Fit linear regression model\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\n# Create the SHAP explainer with the masker\nexplainer = shap.Explainer(model, X_train)\nshap_values = explainer(X_test)\n\n# SHAP Summary Plot\nshap.summary_plot(shap_values.values, X_test)\n\n# SHAP Dependence Plot (example with 'bmi')\nshap.dependence_plot('bmi', shap_values.values, X_test, interaction_index='bp')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n9.3.8.3 Explaining the code and output\n\nexplainer = shap.Explainer(model, X_train):\nThis initializes a SHAP explainer using the trained model and the training data (X_train). The explainer learns how the model makes predictions by seeing how it behaves on this data. X_train acts as the masker, which helps SHAP understand the feature distribution when simulating the absence of a feature.\nshap_values = explainer(X_test):\nThis computes the SHAP values for the test data (X_test). The SHAP values show the contribution of each feature to every individual prediction in the test set.\n\nThe first plot is a SHAP summary plot that ranks the features by their overall importance to the model’s predictions. Features like s1, s5, and bmi show the greatest impact, meaning changes in these values most strongly influence the output. Each dot represents an individual data point, with color indicating the value of the feature (high in pink, low in blue). This helps show not only which features matter most but also how their value ranges affect the predictions.\nThe second plot is a SHAP dependence plot focusing on bmi. It shows a clear positive relationship: as bmi increases, its contribution to the model prediction rises sharply. The color gradient represents blood pressure (bp), helping to illustrate how bmi interacts with bp in shaping the outcome. Higher bp values (pink) tend to cluster with higher bmi and higher SHAP values, suggesting a compounding effect.\n\n\n9.3.8.4 Other SHAP functions\n\nshap.TreeExplainer(model): Explains tree based models\nshap.DeepExplainer(model): Explains deep learning based models (works well with packages like tensorflow)\n\n\n\n9.3.8.5 SHAP: Advantages and Challenges\nAdvantages: - Handles feature interactions. - Provides consistent and reliable explanations. - Works across different model types.\nChallenges: - Computationally expensive for large datasets.This could mean that it will require approximation for complex models.\n\n\n9.3.8.6 Pros and Cons of Each Method:\n\n\n\n\n\n\n\n\nMethod\nPros\nCons\n\n\n\n\nGini Importance\nFast, easy to compute\nBiased toward high-\n\n\n\n\ncardinality features\n\n\nLasso\nPerforms feature selection\nSensitive to correlated\n\n\n\nfast\nfeatures, assumes\n\n\n\n\nlinearity\n\n\nPermutation\nSimple to compute\nAffected by correlated\n\n\n\n\nfeatures\n\n\nSHAP\nHandles interactions,\nComputationally expensive\n\n\n\nconsistent\n\n\n\n\n\n\n9.3.8.7 How Each Method Handles Feature Interactions:\n\n\n\n\n\n\n\n\nMethod\nHandles Interactions\nNotes\n\n\n\n\nGini Importance\nIndirectly\nCaptures splits based on\n\n\n\n\nfeature combinations\n\n\nPermutation\nYes\nMeasures impact after\n\n\nImportance\n\nshuffling all features\n\n\nLasso\nNo\nTreats features\n\n\n\n\nindependently, assumes\n\n\n\n\nlinearity\n\n\nSHAP\nYes\nCaptures interaction\n\n\n\n\neffects explicitly\n\n\n\n\n\n9.3.8.8 When to Use Which Method\n\nTree-based models: Use SHAP or permutation importance for better accuracy.\nLinear models: Coefficients and regularization for interpretability.\nComplex models: SHAP handles feature interactions better.\n\n\n\n9.3.8.9 Recommended Strategy\n\nStart with permutation importance or Gini importance for quick insights.\nUse SHAP for deeper understanding and interaction effects.\nFor linear models, regularization helps with feature selection.\n\n\n\n\n9.3.9 Conclusion:\n\nVariable importance helps in understanding and improving models.\nSHAP provides the most consistent and interpretable results.\nDifferent methods work better depending on model type and complexity.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Supervised Learning</span>"
    ]
  },
  {
    "objectID": "supervised.html#explaining-xgboost-predictions-with-shap",
    "href": "supervised.html#explaining-xgboost-predictions-with-shap",
    "title": "9  Supervised Learning",
    "section": "9.4 Explaining XGBoost Predictions with SHAP",
    "text": "9.4 Explaining XGBoost Predictions with SHAP\nThis section is prepared by Yifei Chen. I am a junior student double majoring in Statistical Data Science and Economics.\n\n9.4.1 Introduction\nIn this section, we explore how to interpret predictions made by XGBoost models using SHAP (SHapley Additive exPlanations). XGBoost is a powerful gradient boosting framework widely used for structured data tasks, but its predictions are often difficult to explain directly. SHAP offers a principled, game-theoretic approach to decompose each prediction into contributions from individual features.\n\n\n9.4.2 What is SHAP?\nSHAP is a model-agnostic method for interpreting machine learning models by quantifying the contribution of each feature to individual predictions.\n\nBased on game theory: Shapley values from cooperative games.\nAssigns each feature a “contribution” to the prediction.\nWorks well with tree-based models (like XGBoost) using Tree SHAP algorithm.\nLocal interpretability: Explaining a single prediction. (Why did the model predict this value for this instance?)\nGlobal interpretability: Understanding the model’s behavior across all predictions. (Which features are most important across the whole dataset?)\n\n\n\n9.4.3 Simulated Airbnb Data\nWe start by simulating Airbnb listing data with four predictors: room type, borough, number of reviews, and availability. The target variable is price, which is a function of these inputs with added noise. This setup helps us evaluate SHAP against known relationships.\n\n9.4.3.1 Importing Libraries\nFirst, we import the necessary Python libraries: NumPy and Pandas.\n\nimport numpy as np\nimport pandas as pd\nnp.random.seed(1)\n\n\n\n9.4.3.2 Creating the Base Dataset\nWe generate a dataset with 200 observations, randomly assigning values to room type, borough, number of reviews, and availability.\n\nn = 200\ndf = pd.DataFrame({\n    \"room_type\": np.random.choice([\"Entire home\", \"Private room\"], n),\n    \"borough\": np.random.choice([\"Manhattan\", \"Brooklyn\", \"Queens\"], n),\n    \"number_of_reviews\": np.random.poisson(20, n),\n    \"availability\": np.random.randint(10, 365, n),\n})\n\n\n\n9.4.3.3 Defining Base Price Logic\nWe define the price based on room type, borough, and the number of reviews using a basic linear relationship.\n\ndf[\"price\"] = (\n    80\n    + (df[\"room_type\"] == \"Entire home\") * 60\n    + (df[\"borough\"] == \"Manhattan\") * 50\n    + np.log1p(df[\"number_of_reviews\"]) * 3\n)\n\nThis formula increases the base price if the listing is an entire home or located in Manhattan, and slightly adjusts it based on the number of reviews.\n\n\n9.4.3.4 Adding Nonlinear Effect of Availability\nWe introduce a nonlinear relationship between availability and price to reflect that listings available more days tend to be priced higher.\n\ndf[\"price\"] += 0.02 * df[\"availability\"] ** 1.5\n\n\n\n9.4.3.5 Adding Interaction Effects\nWe add an interaction term that further boosts the price if a listing is both an entire home and located in Manhattan.\n\ndf[\"price\"] += (\n    ((df[\"room_type\"] == \"Entire home\") &\n     (df[\"borough\"] == \"Manhattan\"))\n    * 25\n)\n\n\n\n9.4.3.6 Adding Random Noise\nFinally, we add normally distributed noise to simulate real-world variation in prices.\n\ndf[\"price\"] += np.random.normal(0, 10, n)\n\n\n\n9.4.3.7 Viewing the First Few Rows\nWe display the first few rows of the simulated dataset\n\ndf.head()\n\n\n\n\n\n\n\n\nroom_type\nborough\nnumber_of_reviews\navailability\nprice\n\n\n\n\n0\nPrivate room\nManhattan\n20\n95\n150.343046\n\n\n1\nPrivate room\nQueens\n19\n44\n94.628590\n\n\n2\nEntire home\nManhattan\n21\n244\n296.406151\n\n\n3\nEntire home\nQueens\n24\n230\n222.006869\n\n\n4\nPrivate room\nBrooklyn\n10\n220\n160.837083\n\n\n\n\n\n\n\n\n\n\n9.4.4 Encode and Train XGBoost Model\nWe one-hot encode categorical variables and split the data into training and test sets. Then, we train an XGBoost regressor model.\n\n9.4.4.1 Importing Libraries\nFirst, we import the required libraries for modeling and evaluation.\n\nfrom xgboost import XGBRegressor\nimport xgboost as xgb\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n\n\n9.4.4.2 Preparing the Feature Matrix and Target Variable\nWe separate the predictors (X) and the outcome variable (y).\nCategorical variables are one-hot encoded using pd.get_dummies, which transforms them into binary indicators.\n\nX = pd.get_dummies(df.drop(\"price\", axis=1), drop_first=True)\ny = df[\"price\"]\n\n\n\n9.4.4.3 Splitting the Data\nWe split the dataset into a training set (80%) and a test set (20%) to evaluate model performance.\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n\n\n9.4.4.4 Training the XGBoost Regressor\nWe fit an XGBoost regressor with 100 trees (n_estimators=100), a learning rate of 0.1, and a maximum tree depth of 3 (max_depth=3).\nThese hyperparameters help balance model complexity and generalization.\n\nmodel = XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)\nmodel.fit(X_train, y_train)\n\nXGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, device=None, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric=None, feature_types=None,\n             feature_weights=None, gamma=None, grow_policy=None,\n             importance_type=None, interaction_constraints=None,\n             learning_rate=0.1, max_bin=None, max_cat_threshold=None,\n             max_cat_to_onehot=None, max_delta_step=None, max_depth=3,\n             max_leaves=None, min_child_weight=None, missing=nan,\n             monotone_constraints=None, multi_strategy=None, n_estimators=100,\n             n_jobs=None, num_parallel_tree=None, ...)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.XGBRegressor?Documentation for XGBRegressoriFittedXGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, device=None, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric=None, feature_types=None,\n             feature_weights=None, gamma=None, grow_policy=None,\n             importance_type=None, interaction_constraints=None,\n             learning_rate=0.1, max_bin=None, max_cat_threshold=None,\n             max_cat_to_onehot=None, max_delta_step=None, max_depth=3,\n             max_leaves=None, min_child_weight=None, missing=nan,\n             monotone_constraints=None, multi_strategy=None, n_estimators=100,\n             n_jobs=None, num_parallel_tree=None, ...) \n\n\n\n\n9.4.4.5 Evaluating the Model\nWe predict on the test set and calculate evaluation metrics:\n\nRoot Mean Squared Error (RMSE) measures the typical size of prediction errors.\nR-squared (R²) indicates the proportion of variance in the target variable that the model can explain.\n\n\npreds = model.predict(X_test)\nprint(\"RMSE:\", np.sqrt(mean_squared_error(y_test, preds)))\nprint(\"R^2:\", r2_score(y_test, preds))\n\nRMSE: 12.634626845730063\nR^2: 0.9593441283227434\n\n\nAfter training XGBoost on our enhanced dataset with nonlinear and interaction terms, we achieved a Root Mean Squared Error (RMSE) of about $12.63, meaning on average, our model’s predictions are off by around $12.63 from the actual listing price.\nAnd the model explains over 95.9% of the variation in prices (R² = 0.959), which indicates a very strong fit. This shows that XGBoost effectively captures the complex relationships we embedded in our simulated data — including the nonlinear availability effect and the interaction between room type and borough.\n\n\n\n9.4.5 Compare XGBoost Feature Importance vs. SHAP\nXGBoost has a built-in feature importance plot. It shows how often each feature is used for splitting, but doesn’t reflect how much a feature contributes to prediction outcomes. It’s useful, but not always reliable—so we turn to SHAP for deeper insight.\n\nimport matplotlib.pyplot as plt\nxgb.plot_importance(model)\nplt.title(\"XGBoost Feature Importance\")\nplt.show()\n\n\n\n\n\n\n\n\nInterpretation:\nThis bar chart shows the built-in feature importance from XGBoost. It measures how frequently each feature is used to split nodes across all trees in the ensemble:\n\nAvailability is the most frequently used feature, with over 300 splits, suggesting it plays a dominant role in decision making.\nNumber of reviews and room_type_Private room are also influential.\nBorough variables (especially borough_Manhattan and borough_Queens) are used less frequently.\n\nBut frequency ≠ influence\nThis method doesn’t tell us how much a feature contributes to raising or lowering predictions — just how often it’s used in splits.\n\n\n9.4.6 SHAP: Global Feature Importance\nSHAP’s summary plot shows global feature importance. Each point represents a feature’s effect on a single prediction. Red indicates high feature values; blue is low. Here, ‘room_type_Entire home’ and ‘borough_Manhattan’ consistently drive price increases.\n\nimport shap\nshap.initjs()\nexplainer = shap.Explainer(model)\nshap_values = explainer(X_test)\nshap.summary_plot(shap_values, X_test)\n\n\n\n\n/var/folders/cq/5ysgnwfn7c3g0h46xyzvpj800000gn/T/ipykernel_10118/3575566828.py:5: FutureWarning:\n\nThe NumPy global RNG was seeded by calling `np.random.seed`. In a future version this function will no longer use the global RNG. Pass `rng` explicitly to opt-in to the new behaviour and silence this warning.\n\n\n\n\n\n\n\n\n\n\nInterpretation:\nThis SHAP summary plot shows the impact of each feature on the model’s predictions, based on SHAP values:\n\nEach point represents a SHAP value for one observation.\nThe position along the x-axis shows whether that feature pushed the prediction higher (right) or lower (left).\nThe color represents the actual feature value:\n\nRed = high values\nBlue = low values\nPurple medium feature values\n\n\nListings associated with high values of room_type_Private room consistently lead to lower predicted prices. Higher availability, as indicated by red points, tends to raise the predicted price, whereas lower availability, shown in blue, typically decreases it. Being located in Manhattan, observed through the blue points on borough_Manhattan, significantly increases the predicted price. Similarly, a greater number of reviews generally results in slightly higher predicted prices, as reflected by the right-shifted red dots on number_of_reviews. Unlike XGBoost’s traditional frequency-based feature importance, SHAP provides a direct quantification of each feature’s contribution to individual predictions, offering a more nuanced and interpretable understanding of the model’s behavior.\n\n\n9.4.7 SHAP Dependence Plot\nThis plot isolates the relationship between number of reviews and its contribution to predicted price. We observe diminishing returns—more reviews increase price up to a point, after which the effect stabilizes.\n\nshap.dependence_plot(\"number_of_reviews\", shap_values.values, X_test)\n\n\n\n\n\n\n\n\nThis SHAP dependence plot shows how the feature number_of_reviews affects the model’s predictions:\n\nThe x-axis shows the actual number of reviews.\nThe y-axis shows the SHAP value — the effect of that feature on the predicted price.\nEach point is a listing, and the color represents the value of another feature: room_type_Private room (0 = blue, 1 = red).\n\nAs the number of reviews increases, the SHAP values generally rise, indicating that a greater number of reviews tends to lead to higher predicted prices. Listings with fewer reviews, concentrated on the left side of the plot, exhibit negative SHAP values, thereby lowering the price predictions. The color pattern suggests an interaction effect: while both room types can exhibit positive or negative influences, the distribution of SHAP values shifts slightly depending on the room type. Purple points represent medium feature values, illustrating how mid-range values influence predictions relative to more extreme values. Since room_type_Private room is a binary feature (0 or 1), most points are either red or blue. Overall, this dependence plot reveals an interaction between number_of_reviews and room_type_Private room, providing insight into how these two features jointly affect the model’s predictions.\n\n\n9.4.8 SHAP Waterfall Plot\nThe waterfall plot explains one individual prediction by breaking it into base value + feature contributions. You can see which features pushed the price up or down—this is the kind of transparency stakeholders appreciate.\n\nshap.plots.waterfall(shap_values[0])\n\n\n\n\n\n\n\n\nThis SHAP waterfall plot explains a single prediction by showing how each feature contributed to moving the prediction away from the average.\n\nThe model’s baseline prediction (expected value E[f(X)]) is about 202.48, which is the average prediction across all data.\nThe final prediction for this listing is ~299.22.\nEach bar shows how a feature pushes the prediction up or down:\n\nPositive contributions (in red) increase the predicted price.\nNegative contributions (in blue) decrease it.\n\n\nFeature impacts for this listing:\n\nroom_type_Private room = False (i.e., Entire home) adds +37.46\nborough_Manhattan = True adds +35.35\navailability = 253 adds +22.83\nborough_Queens = False adds +1.16\nnumber_of_reviews = 19 has a negligible negative effect (−0.08)\n\nTogether, these factors raise the predicted price from the baseline (~202) to about 299. This plot makes the model’s reasoning fully transparent for this specific example.\n\n\n9.4.9 SHAP Waterfall Plots for Multiple Examples\nWe look at a few different listings to see how explanations change. For instance, this one might have a lower price due to being in Queens, while another might be higher due to being an entire home in Manhattan. SHAP helps compare these directly.\n\nshap.plots.waterfall(shap_values[1])\nshap.plots.waterfall(shap_values[2])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n9.4.9.1 Example 1 – Final Prediction: 237.37\n\nThe listing is not a private room (entire home), which increases the prediction by +33.68.\nHigh availability (242) contributes +24.72.\nIt’s not in Manhattan, which reduces the price by −21.98.\nFewer reviews (15) slightly reduce the prediction (−2.69).\nNot in Queens adds a small positive effect (+1.15).\n\nOverall, the price is pulled up due to availability and room type, even though being outside Manhattan and having few reviews lowers it.\n\n\n9.4.9.2 Example 2 – Final Prediction: 159.37\n\nThis listing has low availability (48), which strongly reduces the price by −48.09.\nIt’s not a private room (entire home), which adds +34.22.\nIt’s not in Manhattan (−24.5) but is in Queens, which also slightly decreases the price (−3.11).\nThe number of reviews is again low (15), slightly pulling the price down (−1.64).\n\nDespite being an entire home, the extremely low availability and location drag the prediction far below average.\nThese examples highlight how SHAP provides transparent, instance-level reasoning for model predictions, allowing us to understand and trust the model’s decisions.\n\n\n\n9.4.10 Real-World Use: Exporting SHAP Values\nHere, we export SHAP values for each listing alongside the model’s predictions and true prices. These can feed into dashboards or reports—turning raw model output into business insights or decisions.\n\nshap_df = pd.DataFrame(shap_values.values, columns=X_test.columns)\nshap_df[\"prediction\"] = model.predict(X_test)\nshap_df[\"true_price\"] = y_test.reset_index(drop=True)\nshap_df.head()\n\n\n\n\n\n\n\n\nnumber_of_reviews\navailability\nroom_type_Private room\nborough_Manhattan\nborough_Queens\nprediction\ntrue_price\n\n\n\n\n0\n-0.075032\n22.834732\n37.460049\n35.352047\n1.164263\n299.218506\n303.381871\n\n\n1\n-2.692971\n24.723129\n33.684650\n-21.976707\n1.153586\n237.374176\n217.884045\n\n\n2\n-1.639472\n-48.089657\n34.221230\n-24.496450\n-3.109193\n159.368958\n139.684180\n\n\n3\n1.993280\n-46.662403\n-31.141033\n-17.910049\n-6.507442\n102.254799\n103.821194\n\n\n4\n0.748187\n63.279129\n33.034374\n-18.868820\n-6.664352\n274.011047\n279.036104\n\n\n\n\n\n\n\nThis table shows exported SHAP values for each Airbnb listing, along with the model’s predicted price and the actual price. Each feature’s SHAP value represents how much it contributed to shifting the prediction up or down from the baseline. For example, in row 0, features like high availability, Manhattan location, and the fact that it’s not a private room all contributed to increasing the predicted price to $299.22, closely matching the true price of $303.38. This kind of output makes it easy to understand and explain individual predictions, which is especially useful for reports, dashboards, or stakeholder communication.\n\n\n9.4.11 Limitations\n\nSHAP can be slow with very large datasets. SHAP calculations, especially for kernel-based models, require estimating the contribution of each feature across many possible coalitions of other features (like in cooperative game theory). This is computationally expensive for tree-based models, Tree SHAP is faster, but still non-trivial on large data.\nSHAP explanations may still require domain expertise. Even though SHAP provides clear numeric contributions, interpreting their meaning often depends on context. For example, a +20 SHAP value for availability means it increased the price prediction — but whether that’s reasonable or expected depends on domain knowledge (e.g., tourism demand, market saturation, etc.).\nSHAP explanations depend on model quality. SHAP values faithfully reflect the model’s internal logic — not the truth. If the model is biased, underfit, or overfit, the SHAP values will simply explain a flawed decision. Statistically, SHAP does not correct for omitted variable bias, endogeneity, or poor model specification — it merely reveals the mechanics of the trained model.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Supervised Learning</span>"
    ]
  },
  {
    "objectID": "supervised.html#support-vector-machinesvm",
    "href": "supervised.html#support-vector-machinesvm",
    "title": "9  Supervised Learning",
    "section": "9.5 Support Vector Machine(SVM)",
    "text": "9.5 Support Vector Machine(SVM)\nThis section has been prepared by Shubhan Tamhane, a sophmore majoring in Statistical Data Science and minoring in Financial Analysis at the University of Connecticut. This section will primarily focus on the ins and outs of the popular supervised machine learning algorithim, Support Vector Machine. There will also be a brief demonstration using the MNIST dataset for image classification.\n\n9.5.1 Introduction\nWhat is SVM?\n\nSVM is a supervised machine learning algorhithm that is used mainly for classification tasks\nIt works well in high dimensional spaces and makes a clear decision boundry between groups\n\nCore Idea\n\nSVM works by finding the best boundry (Hyperplane) that seperates the data into classes\nIt chooses a hyperplane with the maximum margin, the widest gap between the classes\nThe closest points to the boundry are called support vectors\n\n\n\n9.5.2 Example 1\nCode to set up original\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nX_class0 = np.array([-4, -3, -2])\nX_class1 = np.array([2, 3, 4])\nsupport_vectors = np.array([-1.5, 2])\nX_all = np.concatenate([X_class0, X_class1, support_vectors])\n\ny_all = np.array([0]*3 + [1]*3 + [0, 1])\n\nplt.figure(figsize=(10, 2))\nplt.scatter(X_class0, np.zeros_like(X_class0), color='blue', s=100, \nlabel='Class 0')\nplt.scatter(X_class1, np.zeros_like(X_class1), color='red', s=100, \nlabel='Class 1')\nplt.scatter(support_vectors, np.zeros_like(support_vectors), \n            facecolors='none', edgecolors='green', s=200, \n            linewidths=2, label='Support Vectors')\n\nplt.axvline(0, color='black', linestyle='--', linewidth=2, label='Hyperplane')\n\nplt.axvspan(-1.5, 2, color='yellow', alpha=0.3, label='Maximum Margin')\n\nplt.title(\"1D SVM: Hyperplane, Support Vectors, and Correct Maximum Margin\")\nplt.xlabel(\"Feature Value (1D)\")\nplt.yticks([])\nplt.legend(loc='upper left')\nplt.xlim(-4.5, 4.5)\nplt.grid(axis='x', linestyle=':', alpha=0.5)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nSuppose we have a dataset of Class 0 and Class 1\nData is easily seperable\nWe can see the hyperplane, maximum margin, and the support vectors\n\n\n\n9.5.3 Key Concepts\nMargin\n\nDistance between decision boundry and nearest data points\nSVM tries to maximize this margin\n\nSupport Vectors\n\nCritical data points that determine the position of the decision boundry\nOnly the support vectors are used to make the decision boundry, unlike most ML algorithms\n\nLinear vs Non-Linear\n\nLinear: A straight line can seperate the classes (Example 1)\nNon-linear data: Use kernels to transform the space (Next Example)\n\n\n\n9.5.4 More Realistic Example\n\nX = np.array([-3, -2, -1, 0, 1, 2, 3])\ny = np.array([1, 1, 0, 0, 0, 1, 1])\n\nplt.figure(figsize=(10, 2))\nfor xi, yi in zip(X, y):\n    plt.scatter(xi, 0, color='blue' if yi == 0 else 'red', s=100)\n\n# Making legend\nplt.scatter([], [], color='blue', label='Class 0')\nplt.scatter([], [], color='red', label='Class 1')\nplt.legend(loc='upper left')\n\nplt.title(\"1D Data: Not Linearly Separable\")\nplt.xlabel(\"Feature Value (1D)\")\nplt.yticks([])\nplt.grid(axis='x', linestyle=':', alpha=0.5)\nplt.xlim(-4, 4)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nIn this example, we can see that the data is impossible to set a hyperplane without having many misclassifications due to high overlap\nTo fix this problem, we can plot this data in a 2-dimensional space\n\nKeep the X-axis as the original data points\nSet the Y-axis as the square of the original data points\n\n\n\nX = np.array([-3, -2, -1, 0, 1, 2, 3])\ny = np.array([1, 1, 0, 0, 0, 1, 1])\n\nX_squared = X**2\n\nsupport_idx = [0, 2, 4, 6]\n\nplt.figure(figsize=(6, 6))\nfor xi, x2i, yi in zip(X, X_squared, y):\n    plt.scatter(xi, x2i, color='blue' if yi == 0 else 'red', s=100)\n\nplt.scatter(X[support_idx], X_squared[support_idx], \n            facecolors='none', edgecolors='black', s=200, \n            linewidths=2, label='Support Vectors')\n\nplt.axhline(2.5, color='black', \nlinestyle='--', \nlinewidth=2, label='Hyperplane')\n\nplt.axhspan(1, 4, color='yellow', alpha=0.3, label='Maximum Margin')\nplt.axhline(1, color='black', linestyle=':', linewidth=1)\nplt.axhline(4, color='black', linestyle=':', linewidth=1)\n\nplt.scatter([], [], color='blue', label='Class 0')\nplt.scatter([], [], color='red', label='Class 1')\nplt.legend(loc='upper left')\n\nplt.title(\"SVM in 2D: Hyperplane, Margin, and Support Vectors\")\nplt.xlabel(\"Original Feature (x)\")\nplt.ylabel(\"Transformed Feature (x²)\")\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nIn the 2d transformation, we can clearly see a clear maximum margin without any misclassifications\nThe transformed data is now linearly seperable\n\n\n\n9.5.5 The Kernel Trick\n\nWhen data is not linearily seperable we can use kernels to implicitly transform them to a higher dimension where it can be more seperable\nThis happens without computing the transformation directly, saving computational space\nUsing the Kernel Trick allows SVM to draw a straight boundry in the transformed space\n\n\n\n\n\n\n\n\nKernel\nUse Case\n\n\n\n\nLinear\nData that’s already separable with a straight line (Example 1)\n\n\nPolynomial\nWhen the decision boundary is curved (Example 2)\n\n\nRBF (Radial Basis Function)\nFor complex, non-linear boundaries\n\n\n\nRadial Basis Function Kernel\n\nRBF kernel flexibly bends the boundary to fit the shape\nMeasures similarity between two points based on distance\nMaps data points in an infinite-dimensional feature space\nFlexible and powerful\n\n\n\n9.5.6 Parameters\n\n\n\n\n\n\n\n\nParameter\nDescription\nExample\n\n\n\n\nkernel\nDefines the shape of the decision boundary\n'linear', 'rbf', 'poly'\n\n\nC\nControls trade-off between margin size and misclassification\nC = 0.1 (wide margin), C = 100 (strict)\n\n\ngamma\nDefines how far the influence of a point reaches\n'scale', 0.1, 1\n\n\n\n\n\n9.5.7 Demonstration with MNIST data\nBelow is Python code showcasing a demonstration of Support Vector Machine on the MNIST dataset, classifying digits as either a 4 or a 9.\nLoading in neccesary libraries and data\n\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import classification_report, accuracy_score \nfrom sklearn.metrics import precision_score, confusion_matrix, f1_score\nimport matplotlib.pyplot as plt \nfrom sklearn import datasets \nimport ssl\nimport certifi\n\nssl._create_default_https_context = lambda: ssl.create_default_context(\n    cafile=certifi.where())\n\nfrom sklearn.datasets import fetch_openml\nmnist = fetch_openml('mnist_784', version=1, as_frame=False)\n\nSeperate data into X and Y and into training and testing data\n\nX, y = mnist['data'], mnist['target'].astype(int)\n\nmask = np.isin(y, [4,9])\nX, y = X[mask], y[mask]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, \nrandom_state=3255, test_size=0.2)\n\nFit a Support Vector Classifier model on data, using a Radius Basis Function Kernel, gamma set to scale, and C set to 1.\n\nm1 = SVC(kernel = 'rbf', gamma='scale', C =1)\nm1.fit(X_test, y_test)\ny_pred = m1.predict(X_test)\n\nModel evaluation.\n\nprint(\"Accuracy Score:\", accuracy_score(y_test, y_pred))\nprint(\"Precision Score:\", precision_score(y_test, y_pred, pos_label=9))\nprint(\"F1 Score:\", f1_score(y_test, y_pred, pos_label=9))\nprint(\"Confusion Matrix:\", confusion_matrix(y_test, y_pred))\n\nAccuracy Score: 0.9916575988393181\nPrecision Score: 0.9921033740129217\nF1 Score: 0.9917473986365267\nConfusion Matrix: [[1352   11]\n [  12 1382]]\n\n\nFrom the above output we can see that the SVM model has a high accuracy, precision, and F1 score. Furthermore, we can see from the confusion matrix that there are few misclassifications.\n\n\n9.5.8 Further Readings\n\nSupport Vector Machines: Theory and Applications (Springer)\n\nSupport Vector Machines for Machine Learning – Machine Learning Mastery\n\nScikit-learn: Support Vector Machines Documentation\n\n\n\n\n\n\n\nBreiman, L., Friedman, J. H., Olshen, R., & Stone, C. J. (1984). Classification and regression trees. Wadsworth.\n\n\nChen, T., & Guestrin, C. (2016). XGBoost: A scalable tree boosting system. Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 785–794. https://doi.org/10.1145/2939672.2939785\n\n\nFriedman, J. H. (2001). Greedy function approximation: A gradient boosting machine. The Annals of Statistics, 29(5), 1189–1232.\n\n\nFriedman, J. H. (2002). Stochastic gradient boosting. Computational Statistics & Data Analysis, 38(4), 367–378.\n\n\nHastie, T., Tibshirani, R., & Friedman, J. (2009). The elements of statistical learning: Data mining, inference, and prediction. Springer.\n\n\nKe, G., Meng, Q., Finley, T., Wang, T., Chen, W., Ma, W., Ye, Q., & Liu, T.-Y. (2017). LightGBM: A highly efficient gradient boosting decision tree. Advances in Neural Information Processing Systems, 3146–3154.\n\n\nProkhorenkova, L., Gusev, G., Vorobev, A., Dorogush, A. V., & Gulin, A. (2018). CatBoost: Unbiased boosting with categorical features. Advances in Neural Information Processing Systems, 6638–6648.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Supervised Learning</span>"
    ]
  },
  {
    "objectID": "unsupervised.html",
    "href": "unsupervised.html",
    "title": "10  Unsupervised Learning",
    "section": "",
    "text": "10.1 K-Means Clustering\nSo far, we have explored various supervised learning algorithms such as Decision Trees and Random Forests, which rely on labeled data with known outcomes. In contrast, unsupervised learning techniques analyze unlabeled data to identify patterns, making them particularly useful for clustering and association problems. Among these, K-means clustering stands out as one of the simplest and most widely used algorithms.\nK-means clustering aims to divide a dataset into non-overlapping groups based on similarity. Given a set of data points, each represented as a vector in a multi-dimensional space, the algorithm assigns each point to one of \\(k\\) clusters in a way that minimizes the variation within each cluster. This is done by reducing the sum of squared distances between each point and its assigned cluster center. Mathematically, we seek to minimize:\n\\[\\begin{equation*}\n\\sum_{i=1}^{k}\\sum_{\\boldsymbol{x}\\in S_i}\n\\left\\|\\boldsymbol{x}-\\boldsymbol{\\mu}_i\\right\\|^2\n\\end{equation*}\\]\nwhere \\(S_i\\) represents each cluster and \\(\\boldsymbol{\\mu}_i\\) is the mean of the points within that cluster.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Unsupervised Learning</span>"
    ]
  },
  {
    "objectID": "unsupervised.html#k-means-clustering",
    "href": "unsupervised.html#k-means-clustering",
    "title": "10  Unsupervised Learning",
    "section": "",
    "text": "10.1.1 Lloyd’s Algorithm\nK-means clustering is typically solved using Lloyd’s algorithm, which operates iteratively as follows:\n\nInitialization: Select \\(k\\) initial cluster centroids \\(\\boldsymbol{\\mu}_i\\) randomly.\nIteration:\n\nAssignment step: Assign each point \\(\\boldsymbol{x}\\) to the cluster whose centroid is closest based on the squared Euclidean distance.\nUpdate step: Recompute the centroids as the mean of all points assigned to each cluster:\n\\[\\begin{equation*}\n\\boldsymbol{\\mu}_i \\leftarrow \\frac{1}{|S_i|}\n\\sum_{\\boldsymbol{x}_j \\in S_i} \\boldsymbol{x}_j\n\\end{equation*}\\]\n\nTermination: The process stops when either the assignments no longer change or a predefined number of iterations is reached.\n\n\n\n10.1.2 Example: Iris Data\nK-means clustering can be implemented using the scikit-learn library. Below, we apply it to the Iris dataset.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\nfrom sklearn.cluster import KMeans\n\n\n# Load the Iris dataset\niris = datasets.load_iris()\nX = iris.data[:, :2]  # Using only two features\ny = iris.target\n\nWe visualize the observations based on their true species labels.\n\n# Scatter plot of true species labels\nfig, ax = plt.subplots()\nscatter = ax.scatter(X[:, 0], X[:, 1], c=y,\n                      cmap='viridis', edgecolors='k')\nax.legend(*scatter.legend_elements(), loc=\"upper left\",\n          title=\"Species\")\nplt.xlabel(\"Feature 1\")\nplt.ylabel(\"Feature 2\")\nplt.title(\"True Species Distribution\")\nplt.show()\n\n\n\n\n\n\n\n\nNow, we apply K-means clustering to the data.\n\n# Train K-means model\nKmean = KMeans(n_clusters=3, init='k-means++',\n               n_init=10, random_state=42)\nKmean.fit(X)\n\nKMeans(n_clusters=3, n_init=10, random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KMeans?Documentation for KMeansiFittedKMeans(n_clusters=3, n_init=10, random_state=42) \n\n\nSeveral parameters can be adjusted for better performance. See: &lt;https://scikit-learn.org/stable/modules/generated/ sklearn.cluster.KMeans.html&gt;\nK-means provides cluster centroids, representing the center of each cluster.\n\n# Print predicted cluster centers\nprint(\"Cluster Centers:\")\nprint(Kmean.cluster_centers_)\n\nCluster Centers:\n[[6.81276596 3.07446809]\n [5.77358491 2.69245283]\n [5.006      3.428     ]]\n\n\nWe plot the centroids along with clustered points.\n\n# Plot centroids on the scatter plot\nfig, ax = plt.subplots()\nax.scatter(X[:, 0], X[:, 1], c=Kmean.labels_,\n           cmap='viridis', edgecolors='k', alpha=0.5)\nax.scatter(Kmean.cluster_centers_[:, 0],\n           Kmean.cluster_centers_[:, 1],\n           c=\"black\", s=200, marker='s',\n           label=\"Centroids\")\nax.legend()\nplt.xlabel(\"Feature 1\")\nplt.ylabel(\"Feature 2\")\nplt.title(\"K-means Clustering Results\")\nplt.show()\n\n\n\n\n\n\n\n\n\n10.1.2.1 Comparing True and Predicted Labels\nBy plotting the results side by side, we can see how well K-means clustering approximates the true labels.\n\n# Compare true vs. predicted labels\nfig, axs = plt.subplots(ncols=2, figsize=(12, 5),\n                        constrained_layout=True)\n\n# True labels plot\naxs[0].scatter(X[:, 0], X[:, 1], c=y,\n               cmap='viridis', alpha=0.5,\n               edgecolors='k')\naxs[0].set_title(\"True Labels\")\naxs[0].set_xlabel(\"Feature 1\")\naxs[0].set_ylabel(\"Feature 2\")\n\n# Predicted clusters plot\naxs[1].scatter(X[:, 0], X[:, 1], c=Kmean.labels_,\n               cmap='viridis', alpha=0.5,\n               edgecolors='k')\naxs[1].scatter(Kmean.cluster_centers_[:, 0],\n               Kmean.cluster_centers_[:, 1],\n               marker=\"s\", c=\"black\", s=200,\n               alpha=1, label=\"Centroids\")\naxs[1].set_title(\"Predicted Clusters\")\naxs[1].set_xlabel(\"Feature 1\")\naxs[1].set_ylabel(\"Feature 2\")\naxs[1].legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n10.1.3 Making Predictions on New Data\nOnce trained, the model can classify new data points.\n\n# Sample test data points\nsample_test = np.array([[3, 4], [7, 4]])\n\n# Predict cluster assignment\nprint(\"Predicted Clusters:\", Kmean.predict(sample_test))\n\nPredicted Clusters: [2 0]\n\n\n\n\n10.1.4 Discussion\nK-means is intuitive but has limitations:\n\nSensitivity to initialization: Poor initialization can yield suboptimal results. k-means++ mitigates this issue.\nChoosing the number of clusters: The choice of \\(k\\) is critical. The elbow method helps determine an optimal value.\nAssumption of spherical clusters: K-means struggles when clusters have irregular shapes. Alternative methods such as kernel-based clustering may be more effective.\n\nDespite its limitations, K-means is a fundamental tool in exploratory data analysis and practical applications.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Unsupervised Learning</span>"
    ]
  },
  {
    "objectID": "unsupervised.html#stochastic-neighbor-embedding",
    "href": "unsupervised.html#stochastic-neighbor-embedding",
    "title": "10  Unsupervised Learning",
    "section": "10.2 Stochastic Neighbor Embedding",
    "text": "10.2 Stochastic Neighbor Embedding\nStochastic Neighbor Embedding (SNE) is a dimensionality reduction technique used to project high-dimensional data into a lower-dimensional space (often 2D or 3D) while preserving local neighborhoods of points. It is particularly popular for visualization tasks, helping to reveal clusters or groupings among similar points. Key characteristics include:\n\nUnsupervised: It does not require labels, relying on similarity or distance metrics among data points.\nProbabilistic framework: Pairwise distances in the original space are interpreted as conditional probabilities, which SNE attempts to replicate in the lower-dimensional space.\nCommon for exploratory data analysis: Especially useful for high-dimensional datasets such as images, text embeddings, or genetic data.\n\n\n10.2.1 Statistical Rationale\nThe core idea behind SNE is to preserve local neighborhoods of each point in the data:\n\nFor each point \\(x_i\\) in the high-dimensional space, SNE defines a conditional probability \\(p_{j|i}\\) that represents how likely \\(x_j\\) is a neighbor of \\(x_i\\).\nThe probability \\(p_{j|i}\\) is modeled using a Gaussian distribution centered on \\(x_i\\):\n\\[\np_{j|i} = \\frac{\\exp\\left(- \\| x_i - x_j \\|^2 / 2 \\sigma_i^2\\right)}{\\sum_{k \\neq i} \\exp\\left(- \\| x_i - x_k \\|^2 / 2 \\sigma_i^2\\right)},\n\\] where \\(\\sigma_i\\) is a variance parameter controlling the neighborhood size.\nEach point \\(x_i\\) is mapped to a lower-dimensional counterpart \\(y_i\\), and a corresponding probability \\(q_{j|i}\\) is defined similarly in that space.\nThe objective function minimizes the Kullback–Leibler (KL) divergence between the high-dimensional and low-dimensional conditional probabilities, encouraging a faithful representation of local neighborhoods.\n\n\n\n10.2.2 t-SNE Variation\nThe t-SNE (t-distributed Stochastic Neighbor Embedding) addresses two main issues in the original formulation of SNE:\n\nThe crowding problem: In high dimensions, pairwise distances tend to spread out; in 2D or 3D, they can crowd together. t-SNE uses a Student t-distribution (with one degree of freedom) in the low-dimensional space, which has heavier tails than a Gaussian.\nSymmetric probabilities: t-SNE symmetrizes probabilities \\(p_{ij} = (p_{j|i} + p_{i|j}) / (2N)\\), simplifying computation.\n\nThe Student t-distribution for low-dimensional similarity is given by: \\[\nq_{ij} = \\frac{\\bigl(1 + \\| y_i - y_j \\|^2 \\bigr)^{-1}}{\\sum_{k \\neq l} \\bigl(1 + \\| y_k - y_l \\|^2 \\bigr)^{-1}}.\n\\] This heavier tail ensures that distant points are not forced too close, thus reducing the crowding effect.\n\n\n10.2.3 Supervised Variation\nAlthough SNE and t-SNE are fundamentally unsupervised, it is possible to integrate label information. In a supervised variant, distances between similarly labeled points may be reduced (or differently weighted), and additional constraints can be imposed to promote class separation in the lower-dimensional embedding. These approaches can help when partial label information is available and you want to blend supervised and unsupervised insights.\n\n\n10.2.4 Demonstration with a Subset of the NIST Digits Data\nBelow is a brief example in Python using t-SNE on a small subset of the MNIST digits (which is itself a curated subset of the original NIST data).\n\nimport numpy as np\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\n\nmnist = fetch_openml('mnist_784', version=1)\nX = mnist.data[:2000]\ny = mnist.target[:2000]\n\ntsne = TSNE(n_components=2, perplexity=30, learning_rate='auto', \n            init='random', random_state=42)\nX_embedded = tsne.fit_transform(X)\n\n# Create a separate scatter plot for each digit to show a legend\nplt.figure()\ndigits = np.unique(y)\nfor digit in digits:\n    idx = (y == digit)\n    plt.scatter(\n        X_embedded[idx, 0],\n        X_embedded[idx, 1],\n        label=f\"Digit {digit}\",\n        alpha=0.5\n    )\nplt.title(\"t-SNE on a Subset of MNIST Digits (by class)\")\nplt.xlabel(\"Dimension 1\")\nplt.ylabel(\"Dimension 2\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nIn the visualization:\n\nPoints belonging to the same digit typically cluster together.\nAmbiguous or poorly written digits often end up bridging two clusters.\nSome digits, such as 3 and 5, may be visually similar and can appear partially overlapping in the 2D space.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Unsupervised Learning</span>"
    ]
  },
  {
    "objectID": "unsupervised.html#principal-component-analysis-pca",
    "href": "unsupervised.html#principal-component-analysis-pca",
    "title": "10  Unsupervised Learning",
    "section": "10.3 Principal Component Analysis (PCA)",
    "text": "10.3 Principal Component Analysis (PCA)\nThe following section is written by Mezmur Edo, a PhD student in the physics department. This section will focus on the motivation, intuition and theory behind PCA. It will also demonstrate the importance of scaling for proper implementation of PCA.\n\n10.3.1 Motivation\nSome of the motivations behind PCA are:\n\nComputation Efficiency\nFeature Extraction\nVisualization\nCurse of dimensionality\n\n\n10.3.1.1 Curse of Dimensionality\nThe Euclidean distance between data points, which we represent as vectors, shrinks with the number of dimensions. To demonstrate this, let’s generate 10,000 vectors of n dimensions each, where n ranges from 2 to 50, with integer entries ranging from -100 to 100. By selecting a random vector, Q, of the same dimension, we can calculate the Euclidean distance of Q to each of these 10,000 vectors. The plot below shows the logarithm, to the base 10, of difference between the maximum and minimum distances divided by the minimum distance as a function of the number of dimensions.\n\n#import libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler, scale, normalize\nimport os\nimport math\nfrom matplotlib.ticker import AutoMinorLocator\n\n#define a list to store delta values\n#delta is the logarithm, to the base 10, of difference between\n#the maximum and minimum Euclidean distances divided \n#by the minimum distance\ndeltas = []\n\n#loop through dimensions from 2 to 49\nfor N in range(2, 50):\n  #generate 10,000 random N-dimensional vectors, P, and\n  #a single random N-dimensional vector, Q\n  P = [np.random.randint(-100, 100, N) for _ in range(10000)]\n  Q = np.random.randint(-100, 100, N)\n  \n  #calculate the Euclidean distances between each point in P and Q\n  diffs = [np.linalg.norm(p - Q) for p in P]\n  \n  #find the maximum and minimum Euclidean distances\n  mxd = max(diffs)\n  mnd = min(diffs)\n  \n  #calculate delta\n  delta = math.log10(mxd - mnd) / mnd\n  deltas.append(delta)\n\n#plot delta versus N, the number of dimensions\nplt.plot(range(2, 50), deltas)\nplt.xlabel('Number of dimension', loc='right', fontsize=10)\nplt.ylabel('Euclidean Distance', loc='top', fontsize=10)\nax = plt.gca()\n\n#add minor locators to the axes\nax.xaxis.set_minor_locator(AutoMinorLocator())\nax.yaxis.set_minor_locator(AutoMinorLocator())\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n10.3.2 Intuition\nWe aim to find orthogonal directions of maximum variance in data. Directions with sufficiently low variance in the data can be removed.\n\nrng = np.random.RandomState(0)\nn_samples = 200\n\n#generate a 2D dataset with 200 entries from \n#a multivariate normal distribution\n#with covariances [[3, 3], [3, 4]]\n#and mean [0, 0]\nX = rng.multivariate_normal(mean=[0,0], \\\ncov=[[3, 3], [3, 4]], size=n_samples)\n\n#perform PCA on the generated data to find \n#the two principal components\npca = PCA(n_components=2).fit(X)\n\n#plot the generated data wih label 'Data'\nplt.scatter(X[:,0], X[:,1], label = 'Data')\n\n#plot the first principal component scaled by \n#its explained variance\n#set color, linewidth and label\nfirst_principal_cpt_explained_var = pca.explained_variance_[0]\nfirst_principal_cpt = [[0, pca.components_[0][0]*first_principal_cpt_explained_var] \\\n, [0, pca.components_[0][1]*first_principal_cpt_explained_var]]\n\nplt.plot(first_principal_cpt[0], first_principal_cpt[1] \\\n, color='green', linewidth=5 \\\n, label = r'First Principal Component ($p_1$)')\n\n#plot the second principal component scaled by \n#its explained variance\n#set color, linewidth and label\nsecond_principal_cpt_explained_var = pca.explained_variance_[1]\nsecond_principal_cpt = [[0, pca.components_[1][0]*second_principal_cpt_explained_var] \\\n, [0, pca.components_[1][1]*second_principal_cpt_explained_var]]\n\nplt.plot(second_principal_cpt[0],  second_principal_cpt[1] \\\n, color='red', linewidth=5 \\\n, label = r'Second Principal Component ($p_2$)')\n\nplt.title(\"\")\nplt.xlabel(\"First Feature\", loc = 'right', fontsize = 10)\nplt.ylabel(\"Second Feature\", loc = 'top', fontsize = 10)\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nWe can then project the data onto the first principal component direction, \\(p_1\\).\n\n\n10.3.3 Theory\nLet \\(x\\) be a data point with features \\(f_1\\), \\(f_2\\), \\(f_3\\), …, \\(f_n\\),\n\\[x = \\begin{pmatrix}\nf_1\\\\\nf_2\\\\\nf_3\\\\\n.\\\\\n.\\\\\n.\\\\\nf_n\n\\end{pmatrix}.\n\\]\nThe projection of x onto p is then,\n\\[x^{T} \\frac{p}{||p||}.\\]\nHence, the projection of all data points onto the principal component direction, p, can be written as,\n\\[\\begin{pmatrix}\nx_1^{T} \\frac{p}{||p||}\\\\\nx_2^{T} \\frac{p}{||p||}\\\\\nx_3^{T} \\frac{p}{||p||}\\\\\n.\\\\\n.\\\\\n.\\\\\nx_m^{T} \\frac{p}{||p||}\n\\end{pmatrix}\n= X\\frac{p}{||p||},\\]\nwhere:\n\nX is the design matrix consisting m datapoints.\n\n\n10.3.3.1 The Optimization Problem\nLet \\(\\bar{x}\\) be the sample mean vector such that,\n\\[\\bar{x} = \\frac{1}{m}\\sum_{i=1}^{m}x^{(i)}.\\]\nThe sample covariance matrix is then given by,\n\\[S = \\frac{1}{m} X^TX - \\bar{x}\\bar{x}^T,\\]\nwhere:\n\n\\(S_{ij}\\) is the covarance of feature i and feature j.\n\nFor a sample mean of the projected data, \\(\\bar{a}\\),\n\\[\\bar{a} = \\frac{1}{m}\\sum_{i=1}^{m}x^{(i)T}p = \\bar{x}^Tp,\\]\nthe sample variance of the projected data can be written as,\n\\[\\sigma^{2}= \\frac{1}{m}\\sum_{i=1}^{m}(x^{(i)T}p)^2 - \\bar{a}^{2} = p^{T}Sp.\\]\nThen, our optimization problem simplifies to maximizing the sample variance,\n\\[\\max_p \\space p^{T}Sp \\space s.t. ||p||=1,\\]\nwhich has the following solution,\n\\[Sp = \\lambda p.\\]\n\n\n10.3.3.2 Scikit-learn Implementation\nComputation can be done using the single value decomposition of X,\n\\[X = U \\Sigma V^T.\\]\nIf the data is mean-centered (the default option in scikit-learn), the sample covariance matrix is given by,\n\\[S = \\frac{1}{m} X^TX = \\frac{1}{m} V\\Sigma U^T U \\Sigma V^T = V\\frac{1}{m}\\Sigma^2V^T,\\]\nwhich is the eigenvalue decomposition of S, with its eigenvectors as the columns of \\(V\\) and the corresponding eigenvalues as diagonal entries of \\(\\frac{1}{m}\\Sigma^2\\).\nThe variance explained by the j-th principal component, \\(p_j\\), is \\(\\lambda_{j}\\) and the total variance explained is the sum of all the eigenvalues, which is also equal to the trace of S. The total variance explained by the first k principal componentsis then given by,\n\\[\\frac{\\sum_{j=1}^{k} \\lambda_j}{trace(s)}.\\]\n\n\n\n10.3.4 PCA With and Without Scaling\nFor proper implementation of PCA, data must be scaled. To demonstrate this, we generate a dataset with the first 4 features selected from a normal distribution with mean 0 and standard deviation 1. We then append a fifth feature drawn from a uniform distribution with integer entries ranging from 1 to 10. The plot of the projection of the data onto first principal component versus the projection onto the second principal component does not show the expected noise structure unless the data is scaled.\n\nnp.random.seed(42)\n\n#generate a feature of size 10,000 with integer entries \n#ranging from 1 to 10\nfeature = np.random.randint(1, 10, 10000)\nN = 10000\nP = 4\n\n#generate a 4D dataset drawn from a normal distribution of 10,000 entries\n#then append the feature to X, making it a 5D dataset\nX = np.random.normal(size=[N,P])\nX = np.append(X, feature.reshape(10000,1), axis = 1)\n\n#perform PCA with 2 components on the dataset without scaling\npca = PCA(2)\npca_no_scale = pca.fit_transform(X)\n\n#plot the projection of the data onto the first principal\n#component versus the projection onto \n#the second principal component\nplt.scatter(pca_no_scale[:,0], pca_no_scale[:,1])\nplt.title(\"PCA without Scaling\")\nplt.xlabel(\"Principal Component 1\", loc = 'right', fontsize = 10)\nplt.ylabel(\"Principal Component 2\", loc = 'top', fontsize = 10)\nplt.show()\n\n#scale data, mean-center and divide by the standard deviation\nXn = scale(X)\n\n#perform PCA with 2 components on the scaled data\npca = PCA(2)\npca_scale = pca.fit_transform(Xn)\n\n#plot the projection of the data onto the first principal \n#component versus the projection onto\n#the second principal component\nplt.scatter(pca_scale[:,0], pca_scale[:,1])\nplt.title(\"PCA with Scaling\")\nplt.xlabel(\"Principal Component 1\", loc = 'right', fontsize = 10)\nplt.ylabel(\"Principal Component 2\", loc = 'top', fontsize = 10)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10.3.5 Summary\n\nPCA is a dimensionality reduction technique that projects data onto directions which explain the most variance in the data.\nThe principal component directions are the eigenvectors of the sample covariance matrix and the corresponding eigenvalues represent the variances explained.\nFor proper implementation of PCA, data must be mean-centered, scikit-learn default, and scaled.\n\n\n\n10.3.6 Further Readings\n\nPrincipal component analysis: a review and recent developments\nPrincipal Components Analysis",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Unsupervised Learning</span>"
    ]
  },
  {
    "objectID": "unsupervised.html#choosing-the-optimal-number-of-clusters",
    "href": "unsupervised.html#choosing-the-optimal-number-of-clusters",
    "title": "10  Unsupervised Learning",
    "section": "10.4 Choosing the Optimal Number of Clusters",
    "text": "10.4 Choosing the Optimal Number of Clusters\nThis section was contributed by Nicholas Pfeifer, a junior majoring in Statistics and minoring in Real Estate and Computer Science.\nThis section will cover the following:\n\nWhy use clustering? What are its applications?\nK-means Clustering and Hierarchical Clustering algorithms\nHow to determine the optimal number of clusters\n\n\n10.4.1 Why Clustering? What is it?\nClustering is an exploratory approach looking to identify natural categories in the data. The overall goal is to Place observations into groups (“clusters”) based on similarities or patterns. It can be viewed as an Unsupervised Learning technique since the algorithm does not use a target variable to discover patterns and make groups. This is in contrast to regression, for instance, where the target variable is used in the process of generating a model. Clustering can be effective at identifying trends, patterns, or outliers in a dataset.\n\nClustering is useful when…\n\nthe true number of clusters is not known in advance\nworking with large unlabeled data\nlooking to detect anomolies/outliers\n\n\n\n10.4.1.1 Applications\nClustering has a plethora of applications. Some of the most popular ones are outlined below.\n\nMarket Reasearch\n\nCustomer Segmentation - grouping customers by demographics or behaviors\nSales Analysis - based on the clusters, which groups purchase the product/service and which groups do not\n\nAnomaly Detection\n\nBanks - combat fraud by distinguishing characteristics that stand out\n\nImage Segmentation\n\nIdentifying sections, objects, or regions of interest\nClassify land using satellite imagery - vegetation, industrial use, etc.\n\n\n\n\n\n10.4.2 How to measure the quality of clustering outcome\nWhen assigning data points to clusters, there are two aspects to consider when judging the quality of the resulting clusters:\n\nIntra-cluster Distance: The distance between data points within a cluster (can also be referred to as within-cluster distance)\n\nThe smaller the distance/variation within clusters, the better the clustering result\nIdeally similar data points are clustered together\n\nInter-cluster Distance: The distance between data points in separate clusters (can also be referred to as between-cluster distance)\n\nThe larger the distance/variation between clusters, the better the clustering result\nIdeally dissimilar data points are in different clusters\n\n\nIn essence, the objective is for points within a cluster to be as similar to each other as possible, and for points belonging to different clusters to be as distinct as possible.\nThe following code outputs two possible ways to cluster 10 observations from the MNIST handwritten digits dataset introduced in the Unsupervised Learning chapter of these class notes. The dimensionally of the observations has been reduced to 2 dimensions using t-SNE in order to make visualization easier.\n\nfrom sklearn.datasets import fetch_openml\nimport numpy as np\nimport pandas as pd\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\n\nmnist = fetch_openml('mnist_784', version=1)\nmnist_example_df = pd.DataFrame(mnist.data)\nmnist_example_df = mnist_example_df[:10]\n\ntsne = TSNE(n_components=2, perplexity=5,\n            learning_rate='auto',\n            init='random', random_state=416)\n\nmnist_example_df = tsne.fit_transform(mnist_example_df)\n\nmnist_example_df = pd.DataFrame(mnist_example_df)\nmnist_example_df.columns = ['dimension_1', 'dimension_2']\n\nmnist_example_df['clustering_1'] = [1, 1, 3, 2, 3, 3, 2, 1, 2, 3]\nmnist_example_df['clustering_2'] = [1, 1, 2, 2, 3, 3, 1, 3, 2, 2]\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n\nax1.scatter(mnist_example_df['dimension_1'],\n            mnist_example_df['dimension_2'],\n            c = mnist_example_df['clustering_1'],\n            cmap = 'rainbow')\nax1.set_xlabel('Dimension 1')\nax1.set_ylabel('Dimension 2')\nax1.set_title('Clustering 1')\n\nax2.scatter(mnist_example_df['dimension_1'],\n            mnist_example_df['dimension_2'],\n            c = mnist_example_df['clustering_2'],\n            cmap = 'rainbow')\nax2.set_xlabel('Dimension 1')\nax2.set_ylabel('Dimension 2')\nax2.set_title('Clustering 2')\n\nplt.tight_layout();\n\n\n\n\n\n\n\n\nHere are two different clusterings. Hopefully it is apparent which clustering is preferred. Clustering 1 is better than clustering 2 since points in the same cluster are closer to each other, and the clusters themselves are further apart. Some points in clustering 2 are more similar to points of other clusters than points within their own cluster. Ideally a clustering more closely resembles the result seen in clustering 1.\n\n\n10.4.3 Clustering Algorithms\nThey are many different clustering algorithms out there, but for simplicity this section will focus on the K-means and Hierarchical clustering algorithms.\n\nK-means\n\nTop-down approach\nCentroid based\n\nHierarchical (Agglomerative)\n\nBottom-up approach\nTree-like structure\n\nOthers include:\n\nK-mediods, DBSCAN, Gaussian Mixture Model, etc.\n\n\n\n10.4.3.1 K-means Algorithm\nThe K-means algorithm has already been introduced in the unsupervised learning chapter, so this will serve as a brief refresher. The steps of the algorithm are as follows:\n\nMust specify a number of clusters k\nData points are randomly assigned to k intial clusters\nThe centroid of each cluster is calculated\nData points are reassigned to the cluster with the closest centroid according to euclidean distance\nIterate the previous 2 steps until cluster assignments no longer change or a set number of iterations have been completed\n\n\nfrom sklearn.cluster import KMeans\n\nmnist_example_df = mnist_example_df.drop(['clustering_1', 'clustering_2'],\naxis = 1)\n\nkmeans = KMeans(n_clusters = 3, random_state = 416, \nn_init = 16).fit(mnist_example_df)\n\nmnist_example_df['labels'] = kmeans.labels_\n\nplt.figure(figsize=(10, 7))\nplt.scatter(mnist_example_df['dimension_1'],\n        mnist_example_df['dimension_2'],\n        c = mnist_example_df['labels'],\n        cmap = 'rainbow')\nplt.scatter(kmeans.cluster_centers_[:, 0],\n        kmeans.cluster_centers_[:, 1],\n        marker = '*', c = 'y', label = 'Centroids',\n        s = 100)\n\nplt.xlabel('Dimension 1')\nplt.ylabel('Dimension 2')\nplt.title('K-means with k = 3')\nplt.legend();\n\n\n\n\n\n\n\n\nHere is an example clustering result of K-means clustering with k = 3 (clusters) on the same 10 MNIST observations. The final centroids are included in the plot.\n\n\n10.4.3.2 Hierarchical Clustering Algorithm\nHierarchical clustering is another algorithm which differs substantially from K-means. Something particularly of note is that the hierarchical approach does not require the number of clusters to be specified in advance. This can be seen as a drawback of K-means. The decision on the number of clusters can be made by based on the resulting tree-like structure called a Dendrogram. The steps of this algorithm are shown below:\n\nEach data point is initailly assigned to its own cluster\nCheck the distance between every possible pair of clusters\nMerge the closest pair of clusters into one cluster\nIterate the previous 2 steps until all of the data points are in one cluster\nCut the resulting Dendrogram\n\n\nfrom sklearn.cluster import AgglomerativeClustering\nfrom scipy.cluster.hierarchy import dendrogram, linkage\n\nmnist_example_df = mnist_example_df.drop(['labels'], axis = 1)\n\nfig, axs = plt.subplots(2, 2, figsize=(10, 8))\n\nH_Clust = AgglomerativeClustering(n_clusters = None, distance_threshold = 0,\nlinkage = 'ward')\nclusters = H_Clust.fit_predict(mnist_example_df)\n\nclust_linkage = linkage(mnist_example_df, method = 'ward',\nmetric = 'euclidean')\n\n#plt.figure(figsize=(10, 7))\ndendrogram(clust_linkage, ax = axs[0, 1])\naxs[0, 1].set_title('Dendrogram')\naxs[0, 1].set_xlabel('Sample Index')\naxs[0, 1].set_ylabel('Distance')\n\naxs[0, 0].scatter(mnist_example_df['dimension_1'],\n            mnist_example_df['dimension_2'], c=clusters, cmap='rainbow')\nfor i, label in enumerate(range(0, 10)):\n    axs[0, 0].text(mnist_example_df['dimension_1'][i] - 3,\n    mnist_example_df['dimension_2'][i], str(label),\n    fontsize = 16, ha = 'right')\naxs[0, 0].set_title('Hierarchical Clustering')\naxs[0, 0].set_xlabel('Dimension 1')\naxs[0, 0].set_ylabel('Dimension 2')\n\n\nH_Clust = AgglomerativeClustering(n_clusters = 4, distance_threshold = None,\nlinkage = 'ward')\nclusters = H_Clust.fit_predict(mnist_example_df)\n\nclust_linkage = linkage(mnist_example_df, method = 'ward',\nmetric = 'euclidean')\n\n#plt.figure(figsize=(10, 7))\ndendrogram(clust_linkage, color_threshold = 70, ax = axs[1, 1])\naxs[1, 1].set_title('Dendrogram')\naxs[1, 1].set_xlabel('Sample Index')\naxs[1, 1].set_ylabel('Distance')\n\n\naxs[1, 0].scatter(mnist_example_df['dimension_1'],\n            mnist_example_df['dimension_2'], c=clusters, cmap='rainbow')\nfor i, label in enumerate(range(0, 10)):\n    axs[1, 0].text(mnist_example_df['dimension_1'][i] - 3,\n    mnist_example_df['dimension_2'][i], str(label),\n    fontsize = 16, ha = 'right')\naxs[1, 0].set_title('Hierarchical Clustering')\naxs[1, 0].set_xlabel('Dimension 1')\naxs[1, 0].set_ylabel('Dimension 2')\n\nplt.tight_layout()\n\nplt.show()\n\n\n\n\n\n\n\n\nHere is an example of Hierarchical clustering on the same 10 MNIST observations. The top row is the result when the number of clusters has not been specified. In the top left plot each data point is its own cluster. The indices of the data points can be seen in the dendrogram to the right. For this plot (top right) the colors are irrelevant. Potential clusterings of data points can be seen as the clusters are merged from bottom to top. The smaller the vertical distance the closer those clusters are to each other (and vice-versa). In the bottom row, the algorithm has been instructed to generate 4 clusters. The colors in the dendrogram do not align with those shown in the plot, so it is better to refer to the indices. Here, the dendrogram has been cut such that the closest clusters are merged together until there are 4 clusters. How to choose the height to cut the dendrogram will be discussed later on in the section.\n\n\n\n10.4.4 Methods for selecting the optimal number of clusters\nSelecting the optimal number of clusters is important since the results can be misleading if our clustering differs greatly from the true number of clusters. There are many different methods for selecting the optimal number of clusters, but for now we will delve into 4 of the most popular methods. It is important to note that no method works well in every scenario and that different methods can give differing results.\nHere are the methods covered in this section:\n\nInspect a Dendrogram\nElbow Method\nSilhouette Method\nGap Statistic\n\n\n10.4.4.1 Hierarchical Clustering Example\nIn this example we will continue to use the MNIST dataset, however this time 2000 observations will be selected at random to be clustered.\n\nfrom sklearn.datasets import fetch_openml\nimport numpy as np\nimport pandas as pd\nfrom sklearn.utils import resample\n\n# Fetching NIST dataset\nmnist = fetch_openml('mnist_784', version=1)\n\nmnist_df = pd.DataFrame(mnist.data)\n\n# Taking a random sample of 2000 images\nmnist_rand = resample(mnist_df, n_samples = 2000, random_state = 416)\n\nmnist_rand = mnist_rand.reset_index().drop('index', axis = 1)\n\n# Keeping track of the target values\nmnist_target_df = pd.DataFrame(mnist.target)\nmnist_target_rand = resample(mnist_target_df,\n                             n_samples = 2000,\n                             random_state = 416)\nmnist_target_rand = mnist_target_rand.reset_index().drop('index', axis = 1)\n\n# Distribution is fairly even\nmnist_target_rand['class'].value_counts()\n\nclass\n1    211\n3    209\n2    208\n5    202\n9    202\n8    201\n6    198\n7    197\n0    189\n4    183\nName: count, dtype: int64\n\n\nThe distribution of the 2000 randomly sampled handwritten digits is shown above. The distribution of the digitsappears to be fairly evenly distributed.\nOnce again, the dimensionality of these images is reduced to 2 dimensions using t-SNE.\n\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\n\n# t-SNE dimensionality reduction\ntsne = TSNE(n_components=2, perplexity=30,\n            learning_rate='auto',\n            init='random', random_state=416)\n\nmnist_embedded = tsne.fit_transform(mnist_rand)\n\nmnist_embedded_df = pd.DataFrame(mnist_embedded)\nmnist_embedded_df.columns = ['dimension_1', 'dimension_2']\n\nplt.figure(figsize=(10, 7))\nplt.scatter(mnist_embedded_df['dimension_1'],\n            mnist_embedded_df['dimension_2'])\nplt.title('Random Sample of 2000 MNIST Digits')\nplt.xlabel('Dimension 1')\nplt.ylabel('Dimension 2');\n\n\n\n\n\n\n\n\nHere is a scatterplot of the 2000 randomly sampled images above without looking at their actual labels.\n\nfrom sklearn.cluster import AgglomerativeClustering\nfrom scipy.cluster.hierarchy import dendrogram, linkage\n\nH_Clust =  AgglomerativeClustering(n_clusters = None, distance_threshold = 0,\nlinkage = 'ward')\nclusters = H_Clust.fit_predict(mnist_embedded_df)\n\nclust_linkage = linkage(mnist_embedded_df, method = 'ward')\n\nplt.figure(figsize=(10, 7))\ndendrogram(clust_linkage)\nplt.title('Dendrogram')\nplt.xlabel('Sample Index')\nplt.ylabel('Distance')\nplt.show()\n\n\n\n\n\n\n\n\nAfter conducting Hierarchical clustering without specifying the number of clusters, we have a dendrogram. Now comes the decision of where to make a horizontal cut. There is a paper about “dynamic cuts” that are flexible and do not cut at a constant height, but that is outside of the current scope (Langfelder et al. (2008)). When looking at the dendrogram above, suppose we do not know the true number of clusters. Generally, when cutting the tree, we want the resulting clusters to be around the same height. Vertical distance represents dissimilarity, so we do not want clusters of high disimilarity to be merged together. Remember that good clustering involves small distances within clusters and large distances between clusters. This is a subjective approach and sometimes it may be difficult to find the best height to cut the dendrogram. Perhaps with domain knowledge a predefined threshold could be a good height at which to cut. For this example I chose to cut the tree at a height of 200. That resulted in 11 clusters which will be analyzed below.\n\nfrom scipy.cluster.hierarchy import cut_tree\n\n# cut the tree\nnew_clusters = cut_tree(clust_linkage, height = 200)\n\nmnist_embedded_df['cluster'] = new_clusters\n\n# Plot the new clusters\nplt.figure(figsize=(10, 7))\nplt.scatter(mnist_embedded_df['dimension_1'],\n            mnist_embedded_df['dimension_2'], c=mnist_embedded_df['cluster'], \n            cmap='rainbow')\nplt.title('Hierarchical Clustering (11 clusters)')\nplt.xlabel('Dimension 1')\nplt.ylabel('Dimension 2')\nplt.show()\n\n\n\n\n\n\n\n\nHere are the 11 clusters obtained after cutting the tree. What do these clusters signify? Maybe by adding some labels to the clusters, that will become more clear.\n\n# Plot clusters with labels (cluster labels not actual!)\n\nplt.figure(figsize=(10, 7))\nscatter = plt.scatter(mnist_embedded_df['dimension_1'],\n             mnist_embedded_df['dimension_2'], c=mnist_embedded_df['cluster'], \n             cmap='rainbow')\nplt.title('Hierarchical Clustering (11 clusters)')\nplt.xlabel('Dimension 1')\nplt.ylabel('Dimension 2')\nlegend1 = plt.legend(*scatter.legend_elements(), title=\"Cluster\")\nplt.gca().add_artist(legend1)\nplt.show()\n\n\n\n\n\n\n\n\nNow the clusters have been associated with their cluster label, but this does not represent the actual handwritten digits.\nIn this case it is hard to determine what the clusters signify if the target values are unknown. However we do know the target value (actual handwritten digit) for each image. This information can help to label the clusters and make them more interpretable.\n\nmnist_embedded_df['actual'] = mnist_target_rand['class']\n\n# calculating mode and proportion of observations in the cluster that are the \n# mode in each cluster\nmodes = mnist_embedded_df.groupby('cluster').agg(\n            {'actual': [lambda x: x.mode().iloc[0],\n             lambda y: (y == y.mode().iloc[0]).sum()/len(y)]})\n\nmodes.columns = ['mode', 'proportion']\nmodes\n\n\n\n\n\n\n\n\nmode\nproportion\n\n\ncluster\n\n\n\n\n\n\n0\n9\n0.478764\n\n\n1\n8\n0.950920\n\n\n2\n3\n0.812500\n\n\n3\n0\n0.937824\n\n\n4\n7\n0.401042\n\n\n5\n5\n0.704000\n\n\n6\n6\n0.941463\n\n\n7\n7\n0.697987\n\n\n8\n5\n0.881579\n\n\n9\n2\n0.972973\n\n\n10\n1\n0.661442\n\n\n\n\n\n\n\nThis code above calculates the mode digit of each cluster along with the proportion of observations in the cluster that are the mode. Now let’s label the clusters by their mode.\n\n# Plot clusters with (actual) labels (modes)\n\nnew_labels = modes['mode']\n\nplt.figure(figsize=(10, 7))\nscatter = plt.scatter(mnist_embedded_df['dimension_1'],\n             mnist_embedded_df['dimension_2'], c=mnist_embedded_df['cluster'], \n             cmap='rainbow')\nplt.title('Hierarchical Clustering (11 clusters)')\nplt.xlabel('Dimension 1')\nplt.ylabel('Dimension 2')\n\nhandles, _ = scatter.legend_elements()\nplt.legend(handles, new_labels, title=\"Mode\")\n\nplt.show()\n\n\n\n\n\n\n\n\nNow we can get a better understanding of the clustering. Although there are 11 clusters in total, you will notice that every digit does not appear as the mode of a cluster. 9 is not the mode of any cluster while 4 and 7 are the modes of multiple clusters. At the very least the clusters with 4 and 7 as the mode are very close to each other. Also intuitively the digits 0, 6, and 8 are written similarly, so it makes sense to see those clusters in the same general area.\nJust out of curiosity, let’s look at the actual distribution of the digits.\n\n# Showing the actual distribution of classes\n\nmnist_embedded_df['actual'] = mnist_embedded_df['actual'].astype('int64')\n\nplt.figure(figsize=(10, 7))\nscatter = plt.scatter(mnist_embedded_df['dimension_1'],\n            mnist_embedded_df['dimension_2'],\n            c = mnist_embedded_df['actual'], cmap='rainbow')\nplt.xlabel('Dimension 1')\nplt.ylabel('Dimension 2')\nplt.title('True Distribution of Target')\nlegend1 = plt.legend(*scatter.legend_elements(), title=\"Value\")\nplt.gca().add_artist(legend1);\n\n\n\n\n\n\n\n\nAnalyzing the cluster performance by viewing the actual distribution of the target is becoming de facto supervized learning, but not really since the clustering algorithm does not know or use the information of the target. For the purposes of this section it is just to see how well the clustering found the true clusters. For the most part it looks like the clustering did a moderately good job at identifying the true clusters of the digits in 2 dimensions. The digits 4, 7, and 9 seem to be very similar in 2D and is understandably more difficult for the algorithm to distinguish.\nSince the true number of clusters is known, let’s see what it looks like with 10 clusters just out of curiosity again.\n\n# Try cutting with 10 clusters instead\nnew_clusters = cut_tree(clust_linkage, n_clusters=10)\nmnist_embedded_df['cluster'] = new_clusters\n\nmodes = mnist_embedded_df.groupby('cluster').agg(\n            {'actual': [lambda x: x.mode().iloc[0],\n             lambda y: (y == y.mode().iloc[0]).sum()/len(y),\n             lambda x: x.value_counts().index[1],\n             lambda y: (y == y.value_counts().index[1]).sum()/len(y)]})\n\nmodes.columns = ['mode', 'proportion', 'mode_2', 'proportion_2']\n\n# Plot clusters with mode labels\n\nnew_labels = modes['mode']\n\nplt.figure(figsize=(10, 7))\nscatter = plt.scatter(mnist_embedded_df['dimension_1'],\n             mnist_embedded_df['dimension_2'], c=mnist_embedded_df['cluster'],\n             cmap='rainbow')\nplt.title('Hierarchical Clustering (10 clusters)')\nplt.xlabel('Dimension 1')\nplt.ylabel('Dimension 2')\n\nhandles, _ = scatter.legend_elements()\nplt.legend(handles, new_labels, title=\"Mode\")\n\nplt.show()\n\n\n\n\n\n\n\n\nThe difference appears to be that the two clusters with 4 as mode merged into one cluster.\n\nmodes\n\n\n\n\n\n\n\n\nmode\nproportion\nmode_2\nproportion_2\n\n\ncluster\n\n\n\n\n\n\n\n\n0\n9\n0.478764\n4\n0.424710\n\n\n1\n8\n0.576389\n5\n0.305556\n\n\n2\n3\n0.812500\n5\n0.125000\n\n\n3\n0\n0.937824\n2\n0.031088\n\n\n4\n7\n0.401042\n9\n0.307292\n\n\n5\n6\n0.941463\n0\n0.029268\n\n\n6\n7\n0.697987\n4\n0.228188\n\n\n7\n5\n0.881579\n3\n0.065789\n\n\n8\n2\n0.972973\n9\n0.009009\n\n\n9\n1\n0.661442\n2\n0.250784\n\n\n\n\n\n\n\nThis table contains the mode of each cluster as well as the second most common value in each cluster denoted at mode_2. Interestingly, 9 appears as the second most common value in 3 different clusters.\n\n\n10.4.4.2 K-means Clustering Example: Elbow Method\nFor the next 3 methods the K-means algorithm will be used on the same random 2000 MNIST images in 2 dimensions.\nThe goal of the Elbow method is to minimize the within cluster sum of squares (WSS), which is also refered to as inertia. The optimal number of clusters is K such that adding another cluster does not (significantly) improve WSS. Whenever the number of clusters increases, inertia will decrease since there are fewer points in each cluster that become closer to their cluster’s center. The idea of the Elbow method is that the rate of decrease in WSS changes based on the optimal number of clusters, K. When k &lt; K, (approaching optimal number) inertia decreases rapidly. When k &gt; K, (going past optimal number) inertia decreases slowly. K is found by plotting inertia over a range of k and looking for a bend or “elbow”, hence the name.\n\n# K-means\nfrom sklearn.cluster import KMeans\n\n# removing non-nist columns\nmnist_embedded_df = mnist_embedded_df.drop(['cluster', 'actual'], axis = 1)\n\n# elbow method for k between 1 and 20 on same MNIST data\n\nwcss = []\n\nfor k in range(1, 21):\n     model = KMeans(n_clusters = k, random_state = 416).fit(mnist_embedded_df)\n     wcss.append(model.inertia_)\n\nplt.figure(figsize=(10, 7))\nplt.plot(range(1, 21), wcss, 'bx-')\nplt.xlabel('Number of Clusters (k)')\nplt.ylabel('Within-Cluster Sum of Squares')\nplt.title('Elbow Method')\nplt.show()\n# Seems inconclusive, maybe 7?\n\n\n\n\n\n\n\n\nThe code above stores the inertia for values of k between 1 and 20, and creates the plot. In this case it is somewhat inconclusive. It looks like the decrease in inertia starts to slow down at k = 7. Like with the dendrogram, this method is also subjective.\n\n\n10.4.4.3 K-means Clustering Example: Silhouette Method\nNext is the Silhouette method, which is the most objective method of the 4 covered in this section\nSilhouette Score\nBefore delving into the Silhouette method, it is good to get an understanding of Silhouette Score. The silhouette s of a data point is, \\[s = (b-a)/\\max(a, b).\\]\n\nAt each data point, the distance to its cluster’s center = a\nAnd the distance to the second best cluster center = b\n\nSecond best suggests closest cluster that is not the current cluster\n\ns can take any value between -1 and 1\n\nInterpreting Silhouette Score\nThere are 3 main categories that a data point can fall into:\n\nIf a data point is very close to its own cluster and very far from the second best cluster (a is small, and b is big), then s is close to 1 (close to \\(b/b\\))\nIf a data point is roughly the same distance to its own cluster as the second best cluster (\\(a \\approx b\\)), then s \\(\\approx\\) 0\nIf a data point is very far from its own cluster and very close to the second best cluster (a is big, and b is small), then s is close to -1 (close to -a/a)\n\nFor optimal clustering, we want most data points to fall into the first category. In other words we want silhouette scores to be as close to 1 as possible.\nSilhouette Coefficient\nThe Silhouette Coefficient is represented by the average silhouette score of the data points. This metric does a good job of summarizing both within-cluster and between-cluster variation. The closer the Silhouette Coefficient is to 1, the better the clustering. Similar to the Elbow method, the optimal K is selected by calculating the Silhouette Coefficient over a range of k’s, and choosing K with the maximum Silhouette Coefficient.\n\n# Silhoutte method\nfrom sklearn.metrics import silhouette_score\n\nsilhouette_average_scores = []\n\nfor k in range (2, 21):\n    kmeans = KMeans(n_clusters = k, random_state = 416)\n    cluster_labels = kmeans.fit_predict(mnist_embedded_df)\n\n    silhouette_avg = silhouette_score(mnist_embedded_df, cluster_labels)\n    silhouette_average_scores.append(silhouette_avg)\n\n# Plot silhouette scores\nplt.figure(figsize=(10, 7))\nplt.plot(list(range(2,21)), silhouette_average_scores, marker='o')\nplt.title(\"Silhouette Coefficients\")\nplt.xlabel(\"Number of Clusters (k)\")\nplt.ylabel(\"Average Silhouette Score\")\nplt.show()\n# k = 7 has the highest average silhouette score\n\n\n\n\n\n\n\n\nHere the Silhouette Coefficient is calculated for k between 2 and 20. The maximum occurs at k = 7, which is coincidentally the same result as the Elbow method. Let’s visualize how these 7 clusters look on our 2000 MNIST digits.\n\nkmeans = KMeans(n_clusters = 7, random_state = 416)\ncluster_labels = kmeans.fit_predict(mnist_embedded_df)\nmnist_embedded_df['cluster'] = cluster_labels\n\n# K-means with k= 7 (cluster labels, not actual!)\nplt.figure(figsize=(10, 7))\nscatter = plt.scatter(mnist_embedded_df['dimension_1'],\n             mnist_embedded_df['dimension_2'], c=mnist_embedded_df['cluster'],\n             cmap='rainbow')\nplt.title('K-means with k = 7 clusters')\nplt.xlabel('Dimension 1')\nplt.ylabel('Dimension 2')\nlegend1 = plt.legend(*scatter.legend_elements(), title=\"Cluster\")\nplt.gca().add_artist(legend1)\nplt.show()\n\n\n\n\n\n\n\n\nThese are just the cluster labels, not the actual digits.\n\nmnist_embedded_df['actual'] = mnist_target_rand['class']\n\nmodes = mnist_embedded_df.groupby('cluster').agg(\n            {'actual': [lambda x: x.mode().iloc[0],\n             lambda y: (y == y.mode().iloc[0]).sum()/len(y),\n             lambda x: x.value_counts().index[1],\n             lambda y: (y == y.value_counts().index[1]).sum()/len(y)]})\n\nmodes.columns = ['mode', 'proportion', 'mode_2', 'proportion_2']\nmodes\n\n\n\n\n\n\n\n\nmode\nproportion\nmode_2\nproportion_2\n\n\ncluster\n\n\n\n\n\n\n\n\n0\n1\n0.642633\n2\n0.250784\n\n\n1\n0\n0.858491\n5\n0.070755\n\n\n2\n9\n0.467626\n4\n0.410072\n\n\n3\n8\n0.433566\n2\n0.384615\n\n\n4\n3\n0.610932\n5\n0.225080\n\n\n5\n7\n0.565079\n4\n0.200000\n\n\n6\n6\n0.695341\n5\n0.243728\n\n\n\n\n\n\n\nHere are the modes which can be used to label the 7 clusters.\n\n# Plot clusters with (actual) labels (modes)\nnew_labels = modes['mode']\n\nplt.figure(figsize=(10, 7))\nscatter = plt.scatter(mnist_embedded_df['dimension_1'],\n             mnist_embedded_df['dimension_2'], c=mnist_embedded_df['cluster'],\n             cmap='rainbow')\nplt.title('K-means with k = 7 clusters')\nplt.xlabel('Dimension 1')\nplt.ylabel('Dimension 2')\n\nhandles, _ = scatter.legend_elements()\nplt.legend(handles, new_labels, title=\"Mode\")\n\nplt.show()\n\n\n\n\n\n\n\n\nNow we have the actual mode labels of the 7 clusters obtained from K-means. Interestingly, the area that used to have 4 as the label now has 9. Now this digits that do not appear as the mode in any cluster are 4, 5, and 8. Looking back at the modes table we see that these digits frequently appear as the second most common value in a cluster at a high rate. Obviously 7 is not the true number of clusters, but perhaps the 2D representation is obscuring the ability to find disimilarities between some of the digits.\n\n# Showing the actual distribution of classes\n\nmnist_embedded_df['actual'] = mnist_embedded_df['actual'].astype('int64')\n\nplt.figure(figsize=(10, 7))\nscatter = plt.scatter(mnist_embedded_df['dimension_1'],\n            mnist_embedded_df['dimension_2'],\n            c = mnist_embedded_df['actual'], cmap='rainbow')\nplt.xlabel('Dimension 1')\nplt.ylabel('Dimension 2')\nplt.title('True Distribution of Target')\nlegend1 = plt.legend(*scatter.legend_elements(), title=\"Value\")\nplt.gca().add_artist(legend1);\n\n\n\n\n\n\n\n\nFor reference, here is the true distribution of the handwritten digits again.\n\n\n10.4.4.4 K-means Clustering Example: Gap Statistic\nThe last method to be covered is the Gap Statistic. The Gap Statistic for a number of clusters k can be written as\n\\[Gap(k) = \\frac{1}{B}\\sum_{b=1}^{B} \\log(W_{kb}) - \\log(W_k).\\]\n\nCompares the total (within) intra-cluster variation for a range of k’s with their expected values\nCalculated by comparing the inertia of a clustered dataset with the inertia of a uniformly distributed random data set (covering the same ranges in the data space)\nA number of random samples (B) are generated that are then clustered over a range of k’s while keeping track of the inertia\n\\(W_{kb}\\) is the inertia of the b-th random sample with k clusters and \\(W_k\\) is the inertia of the original data with k clusters\n\nWe also need the standard deviation,\n\\[s_k = \\sqrt{1 + \\frac{1}{B}}\\sqrt{\\frac{1}{B}\\sum_{b=1}^{B} (\\log(W_{kb}) - \\overline{W})^2}.\\]\nWhere\n\\[\\overline{W} = \\frac{1}{B}\\sum_{b=1}^{B} \\log(W_{kb}).\\]\nChoose the smallest k such that the gap statistic is within one standard deviation of the gap at k + 1.\nThis can be represented by the expression,\n\\[Gap(k) \\geq Gap(k+1) - s_{k+1}.\\]\nThe optimal k may vary over multiple gap statistic simulations since there is randomness involved.\n\n# gap statistic\n\n# removing non-nist columns\nmnist_embedded_df = mnist_embedded_df.drop(['cluster', 'actual'], axis = 1)\n\ndef calc_gap_statistic(data, max_k, n = 10):\n    # Generate reference data from a uniform distribution\n    def generate_reference_data(X):\n        return np.random.uniform(low = data.min(axis=0),\n        high = data.max(axis=0),\n        size=X.shape)\n\n    gap_values = []\n\n    # Loop over a range of k values\n    for k in range(1, max_k + 1):\n        # Fit K-means to the original data\n        kmeans = KMeans(n_clusters = k, random_state = 416)\n        kmeans.fit(data)\n        original_inertia = kmeans.inertia_\n    \n        # Compute the average inertia for the reference datasets\n        reference_inertia = []\n        for _ in range(n):\n            random_data = generate_reference_data(data)\n            kmeans.fit(random_data)\n            reference_inertia.append(kmeans.inertia_)\n        \n        # Calculate the Gap statistic\n        gap = np.log(np.mean(reference_inertia)) - np.log(original_inertia)\n        gap_values.append(gap)\n\n    return gap_values\n\ngap_values = calc_gap_statistic(mnist_embedded_df, 20, n = 100)\n\nplt.figure(figsize=(10, 7))\nplt.plot(range(1, 21), gap_values, marker='o')\nplt.title('Gap Statistic vs Number of Clusters')\nplt.xlabel('Number of Clusters (k)')\nplt.ylabel('Gap Statistic')\nplt.grid()\nplt.show()\n# 2 is the best?\n\n\n\n\n\n\n\n\nHere a function is defined to calculate the gap statistic. It is calculated for k between 1 and 20 with B = 100 random datasets (the more datasets that are used, the more computationally expensive). In the plot we are looking for the k where the gap statistic is greater than at k + 1 minus standard deviation. In this case we do not even need standard deviation since we observe that Gap(2) is greater than Gap(3). This means that the optimal K is 2 based on this method.\nThis process can also be conducted using the gapstatistics package.\npip install gapstatistics\n\nfrom gapstatistics import gapstatistics\n\ngs = gapstatistics.GapStatistics(distance_metric='euclidean')\n\noptimal = gs.fit_predict(K = 20, X = np.array(mnist_embedded_df))\n\nprint(f'Optimal: {optimal}')\n\nOptimal: 2\n\n\nThe result is also an optimal K of 2. It appears that this method is not very good for this dataset.\n\nkmeans = KMeans(n_clusters = 2, random_state = 416)\ncluster_labels = kmeans.fit_predict(mnist_embedded_df)\nmnist_embedded_df['cluster'] = cluster_labels\n\n# Cluster labels!\nplt.figure(figsize=(10, 7))\nscatter = plt.scatter(mnist_embedded_df['dimension_1'],\n             mnist_embedded_df['dimension_2'], c=mnist_embedded_df['cluster'],\n             cmap='rainbow')\nplt.title('K-means with k = 2 clusters')\nplt.xlabel('Dimension 1')\nplt.ylabel('Dimension 2')\nlegend1 = plt.legend(*scatter.legend_elements(), title=\"Cluster\")\nplt.gca().add_artist(legend1)\nplt.show()\n\n\n\n\n\n\n\n\nHere we have K-means with k = 2 with default cluster labels.\n\nmnist_embedded_df['actual'] = mnist_target_rand['class']\n\nmodes = mnist_embedded_df.groupby('cluster').agg(\n            {'actual': [lambda x: x.mode().iloc[0],\n             lambda y: (y == y.mode().iloc[0]).sum()/len(y),\n             lambda x: x.value_counts().index[1],\n             lambda y: (y == y.value_counts().index[1]).sum()/len(y)]})\n\nmodes.columns = ['mode', 'proportion', 'mode_2', 'proportion_2']\n\n# Plot clusters with labels (actual)\nnew_labels = modes['mode']\n\nplt.figure(figsize=(10, 7))\nscatter = plt.scatter(mnist_embedded_df['dimension_1'],\n             mnist_embedded_df['dimension_2'], c=mnist_embedded_df['cluster'],\n             cmap='rainbow')\nplt.title('K-means with k = 2 clusters')\nplt.xlabel('Dimension 1')\nplt.ylabel('Dimension 2')\n\nhandles, _ = scatter.legend_elements()\nplt.legend(handles, new_labels, title=\"Mode\")\n\nplt.show()\n\n\n\n\n\n\n\n\nHere are the mode labels but that does not tell us very much.\n\nmnist_embedded_df[mnist_embedded_df['cluster'] == 0]['actual'].value_counts()\n# 1, 9, 7, 4, and 2 are similar\n\nactual\n1    211\n9    197\n7    196\n4    181\n2    117\n5     70\n8     22\n3     10\n0      1\n6      1\nName: count, dtype: int64\n\n\n\nmnist_embedded_df[mnist_embedded_df['cluster'] == 1]['actual'].value_counts()\n# 3, 6, 0, 8, and 5 are similar\n\nactual\n3    199\n6    197\n0    188\n8    179\n5    132\n2     91\n9      5\n4      2\n7      1\n1      0\nName: count, dtype: int64\n\n\nAt the very least we can see which images of handwritten digits look similar in 2 dimensions.\n\n\n\n10.4.5 Conclusions\n\nUsing clustering we can figure out which digits look similar to each other when writing by hand\nThe true number of clusters in 2D may be different than in the original dimensions. Maybe the algorithms would be better at identifying the different clusters of the MNIST data in 3D\nChoosing the right number of clusters can be challenging but is very important\nThere are many methods for selecting the optimal number of clusters and they can yield different results\n\n\n\n10.4.6 Further Readings\nDefining clusters from a hierarchical cluster tree\nsklearn AgglomerativeClustering Documentation\ngapstatistics PyPI Documentation\nHow many Clusters? - Towards Data Science\n\n\n\n\nLangfelder, P., Zhang, B., & Horvath, S. (2008). Defining clusters from a hierarchical cluster tree: The dynamic tree cut package for r. Bioinformatics, 24(5), 719–720.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Unsupervised Learning</span>"
    ]
  },
  {
    "objectID": "exercises.html",
    "href": "exercises.html",
    "title": "12  Exercises",
    "section": "",
    "text": "Quarto and Git setup Quarto and Git are two important tools for data science. Get familiar with them through the following tasks. Please use the templates/hw.qmd template to document, for each step, what you did, the obstacles you encountered, and how you overcame them. Think of this as a user manual for students who are new to this. Use the command line interface.\n\nSet up SSH authentication between your computer and your GitHub account.\nInstall Quarto onto your computer following the instructions of Get Started.\nPick a tool of your choice (e.g., VS Code, Jupyter Notebook, Emacs, etc.), follow the instructions to reproduce the example of line plot on polar axis.\nRender the homework into a pdf file and put the file into a release in your GitHub repo.\n\nWorking on Homework Problems All the requirements on homework styles have reasons. Reviewing these questions help you to understand them.\n\nWhat are the differences between binary and source files?\nWhy do we not want to track binary files in a repo?\nWhy do I require pdf output via release?\nWhy do I not want your files added via ‘upload’?\nWhy do I require line width under 80?\nWhy is it not a good idea to have spaces in file/folder names?\n\nContributing to the Class Notes To contribute to the classnotes, you need to have a working copy of the sources on your computer. Document the following steps in a qmd file in the form of a step-by-step manual, as if you are explaining them to someone who wants to contribute too. Make at least 10 commits for this task, each with an informative message.\n\nCreate a fork of the notes repo into your own GitHub account.\nClone it to an appropriate folder on your computer.\nRender the classnotes on your computer; document the obstacles and solutions.\nMake a new branch (and name it appropriately) to experiment with your changes.\nCheckout your branch and add your wishes to the wish list; commit with an informative message; and push the changes to your GitHub account.\nMake a pull request to class notes repo from your fork at GitHub. Make sure you have clear messages to document the changes.\n\nMonty Hall Consider a generalized Monty Hall experiment. Suppose that the game start with \\(n\\) doors; after you pick one, the host opens \\(m \\le n - 2\\) doors, that show no award. Include sufficient text around the code chunks to explain them.\n\nWrite a function to simulate the experiment once. The function takes two arguments ndoors and nempty, which represent the number of doors and the number of empty doors showed by the host, respectively, It returns the result of two strategies, switch and no-switch, from playing this game.\nPlay this game with 3 doors and 1 empty a few times.\nPlay this game with 10 doors and 8 empty a few times.\nWrite a function to demonstrate the Monty Hall problem through simulation. The function takes three arguments ndoors, nempty, and ntrials, where ntrial is the number of trials in a simulation. The function should return the proportion of wins for both the switch and no-switch strategy.\nApply your function with 3 doors (1 empty) and 10 doors (8 empty), both with 1000 trials. Summarize your results.\n\nApproximating \\(\\pi\\) Write a function to do a Monte Carlo approximation of \\(\\pi\\). The function takes a Monte Carlo sample size n as input, and returns a point estimate of \\(\\pi\\) and a 95% confidence interval. Apply your function with sample size 1000, 2000, 4000, and 8000. Repeat the experiment 1000 times for each sample size and check the empirical probability that the confidence intervals cover the true value of \\(\\pi\\). Comment on the results.\nGoogle Billboard Ad Find the first 10-digit prime number occurring in consecutive digits of \\(e\\). This was a Google recruiting ad.\nGame 24 The math game 24 is one of the addictive games among number lovers. With four randomly selected cards from a deck of poker cards, use all four values and elementary arithmetic operations (\\(+-\\times /\\)) to come up with 24. Let \\(\\square\\) be one of the four numbers. Let \\(\\bigcirc\\) represent one of the four operators. For example, \\[\\begin{equation*}\n(\\square \\bigcirc \\square) \\bigcirc (\\square \\bigcirc \\square)\n\\end{equation*}\\] is one way to group the the operations.\n\nList all the possible ways to group the four numbers.\nHow many possible ways are there to check for a solution?\nWrite a function to solve the problem in a brutal force way. The inputs of the function are four numbers. The function returns a list of solutions. Some of the solutions will be equivalent, but let us not worry about that for now.\n\nNYC Crash Data Cleaning The NYC motor vehicle collisions data with documentation is available from NYC Open Data. The raw data needs some cleaning.\n\nUse the filter from the website to download the crash data of the week of June 30, 2024 in CSV format; save it under a directory data with an informative name (e.g., nyccrashes_2024w0630_by20240916.csv); read the data into a Panda data frame with careful handling of the date time variables.\nClean up the variable names. Use lower cases and replace spaces with underscores.\nCheck the crash date and time to see if they really match the filter we intented. Remove the extra rows if needed.\nGet the basic summaries of each variables: missing percentage; descriptive statistics for continuous variables; frequency tables for discrete variables.\nAre their invalid longitude and latitude in the data? If so, replace them with NA.\nAre there zip_code values that are not legit NYC zip codes? If so, replace them with NA.\nAre there missing in zip_code and borough? Do they always co-occur?\nAre there cases where zip_code and borough are missing but the geo codes are not missing? If so, fill in zip_code and borough using the geo codes.\nIs it redundant to keep both location and the longitude/latitude at the NYC Open Data server?\nCheck the frequency of crash_time by hour. Is there a matter of bad luck at exactly midnight? How would you interpret this?\nAre the number of persons killed/injured the summation of the numbers of pedestrians, cyclist, and motorists killed/injured? If so, is it redundant to keep these two columns at the NYC Open Data server?\nPrint the whole frequency table of contributing_factor_vehicle_1. Convert lower cases to uppercases and check the frequencies again.\nProvided an opportunity to meet the data provider, what suggestions would you make based on your data exploration experience?\n\nNYC Crash Data Exploration Except for the first question, use the cleaned crash data in feather format.\n\nConstruct a contigency table for missing in geocode (latitude and longitude) by borough. Is the missing pattern the same across boroughs? Formulate a hypothesis and test it.\nConstruct a hour variable with integer values from 0 to 23. Plot the histogram of the number of crashes by hour. Plot it by borough.\nOverlay the locations of the crashes on a map of NYC. The map could be a static map or a Google map.\nCreate a new variable severe which is one if the number of persons injured or deaths is 1 or more; and zero otherwise. Construct a cross table for severe versus borough. Is the severity of the crashes the same across boroughs? Test the null hypothesis that the two variables are not associated with an appropriate test.\nMerge the crash data with the Census zip code database which contains zip-code level demographic or socioeconomic variables.\nFit a logistic model with severe as the outcome variable and covariates that are available in the data or can be engineered from the data. For example, zip code level covariates obtained from merging with the zip code database; crash hour; number of vehicles involved.\n\nNYC Crash severity modeling Using the cleaned NYC crash data, merged with zipcode level information, predict severe of a crash.\n\nSet random seed to 1234. Randomly select 20% of the crashes as testing data and leave the rest 80% as training data.\nFit a logistic model on the training data and validate the performance on the testing data. Explain the confusion matrix result from the testing data. Compute the F1 score.\nFit a logistic model on the training data with \\(L_1\\) regularization. Select the tuning parameter with 5-fold cross-validation in F1 score\nApply the regularized logistic regression to predict the severity of the crashes in the testing data. Compare the performance of the two logistic models in terms of accuracy, precision, recall, F1-score, and AUC.\n\nMidterm project: Street flood in NYC The class presentation at the 2025 NYC Open Data Weeik is scheduled for Monday, March 24, 2:00-3:00 pm.\nThe NYC Open Data of 311 Service Requests contains all service requests from 2010 to the present. This analysis focuses on two sewer-related complaints in 2024: Street Flooding (SF) and Catch Basin (CB). SF complaints serve as a practical indicator of street flooding, while CB complaints provide insights into a key infrastructural factor—when catch basins fail to drain rainwater properly due to blockages or structural issues, water accumulates on the streets. SF complaints are typically filed when residents observe standing water or flooding, whereas CB complaints report clogged basins, defective grates, or other drainage problems. The dataset is available in CSV format as data/nycflood2024.csv. Refer to the online data dictionary for a detailed explanation of variable meanings. Try to tell a story in your report while going through the questions.\n\nData cleaning.\n\nImport the data, rename the columns with our preferred styles.\nSummarize the missing information. Are there variables that are close to completely missing?\nAre there redundant information in the data? Try storing the data using the Arrow format and comment on the efficiency gain.\nAre there invalid NYC zipcode or borough? Can some of the missing values be filled? Fill them if yes.\nAre there date errors? Examples are earlier closed_date than created_date; closed_date and created_date matching to the second; dates exactly at midnight or noon to the second.\nSummarize your suggestions to the data curator in several bullet points.\n\nExploratory analysis.\n\nVisualize the locations of complaints on a NYC map, with different symbols for different descriptors.\nCreate a variable response_time, which is the duration from created_date to closed_date.\nVisualize the comparison of response time by complaint descriptor and borough. The original may not be the best given the long tail or outlers.\nIs there significant difference in response time between SF and CB complaints? Across different boroughs? Does the difference between SF and CB depend on borough? State your hypothesis, justify your test, and summarize your results in plain English.\nCreate a binary variable over3d to indicate that a service request took three days or longer to close.\nDoes over3d depend on the complaint descriptor, borough, or weekday (vs weekend/holiday)? State your hypotheses, justify your test, and summarize your results.\n\nModeling the occurrence of overly long response time.\n\nCreate a data set which contains the outcome variable over3d and variables that might be useful in predicting it. Consider including time-of-day effects (e.g., rush hour vs. late-night), seasonal trends, and neighborhood-level demographics. Zip code level information could be useful too, such as the zip code area and the ACS 2023 variables (data/nyc_zip_areas.feather and data/acs2023.feather).\nRandomly select 20% of the complaints as testing data with seeds 1234. Build a logistic model to predict over3d for the complaints with the training data. If you have tuning parameters, justify how they were selected.\nConstruct the confusion matrix from your prediction with a threshold of 1/2 on both training and testing data. Explain your accuracy, recall, precision, and F1 score to a New Yorker.\nConstruct the ROC curve of your fitted logistic model and obtain the AUROC for both training and testing data. Explain your results to a New Yorker.\nIdentify the most important predictors of over3d. Use model coefficients or feature importance (e.g., odds ratios, standardized coefficients, or SHAP values).\nSummarize your results to a New Yorker who is not data science savvy in several bullet points.\n\nModeling the count of SF complains by zip code.\n\nCreate a data set by aggregate the count of SF and SB complains by day for each zipcode.\nMerge the NYC precipitation (data/rainfall_CP.csv), by day to this data set.\nMerge the NYC zip code level landscape variables (data/nyc_zip_lands.csv) and ACS 2023 variables into the data set.\nFor each day, create two variables representing 1-day lag of the precipitation and the number of CB complaints.\nFilter data from March 1 to November 30, excluding winter months when flooding is less frequent. November 30.\nCompare a Poisson regression with a Negative Binomial regression to account for overdispersion. Which model fits better? Explain the results to a New Yorker.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Exercises</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Agonafir, C., Lakhankar, T., Khanbilvardi, R., Krakauer, N., Radell, D.,\n& Devineni, N. (2022). A machine learning approach to evaluate the\nspatial variability of New York\nCity’s 311 street flooding complaints. Computers,\nEnvironment and Urban Systems, 97, 101854.\n\n\nAgonafir, C., Pabon, A. R., Lakhankar, T., Khanbilvardi, R., &\nDevineni, N. (2022). Understanding New York\nCity street flooding through 311 complaints. Journal of\nHydrology, 605, 127300.\n\n\n(ASA), A. S. A. (2018). Ethical guidelines for statistical\npractice.\n\n\nBansal, R. (2024). SQL using python.\n\n\nBreiman, L., Friedman, J. H., Olshen, R., & Stone, C. J. (1984).\nClassification and regression trees. Wadsworth.\n\n\nCafferky, B. (2019). Master using SQL with python: Lesson 1 - using\nSQL with pandas.\n\n\nChen, T., & Guestrin, C. (2016). XGBoost: A scalable tree boosting\nsystem. Proceedings of the 22nd ACM SIGKDD International Conference\non Knowledge Discovery and Data Mining, 785–794. https://doi.org/10.1145/2939672.2939785\n\n\nComputing Machinery (ACM), A. for. (2018). Code of ethics and\nprofessional conduct.\n\n\nCone, M. (2025). Markdown cheat sheet | markdown guide. https://www.markdownguide.org/cheat-sheet/\n\n\nCongress, U. S. (1990). Americans with disabilities act of 1990\n(ADA).\n\n\nDervieux, C. (2025). Markdown-basics. https://quarto.org/docs/authoring/markdown-basics.html\n\n\nFriedman, J. H. (2001). Greedy function approximation: A gradient\nboosting machine. The Annals of Statistics, 29(5),\n1189–1232.\n\n\nFriedman, J. H. (2002). Stochastic gradient boosting. Computational\nStatistics & Data Analysis, 38(4), 367–378.\n\n\nHastie, T., Tibshirani, R., & Friedman, J. (2009). The elements\nof statistical learning: Data mining, inference, and prediction.\nSpringer.\n\n\nHealth, U. S. D. of, & Services, H. (1996). Health insurance\nportability and accountability act of 1996 (HIPAA).\n\n\nKe, G., Meng, Q., Finley, T., Wang, T., Chen, W., Ma, W., Ye, Q., &\nLiu, T.-Y. (2017). LightGBM: A highly efficient gradient\nboosting decision tree. Advances in Neural Information Processing\nSystems, 3146–3154.\n\n\nLangfelder, P., Zhang, B., & Horvath, S. (2008). Defining clusters\nfrom a hierarchical cluster tree: The dynamic tree cut package for r.\nBioinformatics, 24(5), 719–720.\n\n\nMacFarlane, J. (2006). Pandoc user’s guide. https://pandoc.org/MANUAL.html#pandocs-markdown\n\n\nMacFarlane, J. (2019). GitHub flavored markdown spec. https://github.github.com/gfm/\n\n\nProkhorenkova, L., Gusev, G., Vorobev, A., Dorogush, A. V., & Gulin,\nA. (2018). CatBoost: Unbiased boosting with categorical features.\nAdvances in Neural Information Processing Systems, 6638–6648.\n\n\nProtection of Human Subjects of Biomedical, N. C. for the, &\nResearch, B. (1979). The belmont report: Ethical principles and\nguidelines for the protection of human subjects of research.\n\n\nPrzybyla, M. (2024). How to use SQL in python.\n\n\nTeam, F. D. S. D. (2019). Federal data strategy 2020 action\nplan.\n\n\nTibshirani, R. (1996). Regression shrinkage and selection via the\nLASSO. Journal of the Royal Statistical Society: Series\nB (Methodological), 58(1), 267–288.\n\n\nVanderPlas, J. (2016). Python data science handbook:\nEssential tools for working with data. O’Reilly Media,\nInc.\n\n\nW3Schools. (2025). Python MySQL.",
    "crumbs": [
      "References"
    ]
  }
]