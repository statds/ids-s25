[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Data Science",
    "section": "",
    "text": "Preliminaries\nThe notes were developed with Quarto; for details about Quarto, visit https://quarto.org/docs/books.\nThis book free and is licensed under a Creative Commons Attribution-NonCommercial-NoDerivs 3.0 United States License.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#sources-at-github",
    "href": "index.html#sources-at-github",
    "title": "Introduction to Data Science",
    "section": "Sources at GitHub",
    "text": "Sources at GitHub\nThese lecture notes for STAT 3255/5255 in Spring 2025 represent a collaborative effort between Professor Jun Yan and the students enrolled in the course. This cooperative approach to education was facilitated through the use of GitHub, a platform that encourages collaborative coding and content development. To view these contributions and the lecture notes in their entirety, please visit our GitHub repository at https://github.com/statds/ids-s25.\nStudents contributed to the lecture notes by submitting pull requests to our GitHub repository. This method not only enriched the course material but also provided students with practical experience in collaborative software development and version control.\nFor those interested, class notes from Fall 2024, Spring 2024, Spring 2023, and Spring 2022 are also publicly accessible. These archives offer insights into the evolution of the course content and the different perspectives brought by successive student cohorts.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#compiling-the-classnotes",
    "href": "index.html#compiling-the-classnotes",
    "title": "Introduction to Data Science",
    "section": "Compiling the Classnotes",
    "text": "Compiling the Classnotes\nTo reproduce the classnotes output on your own computer, here are the necessary steps. See Section 3.2 Compiling the Classnotes for details.\n\nClone the classnotes repository to an appropriate location on your computer; see Chapter 2  Project Management for using Git.\nSet up a Python virtual environment in the root folder of the source; see Section 4.8 Virtual Environment.\nActivate your virtual environment.\nInstall all the packages specified in requirements.txt in your virtual environment:\n\npip install -r requirements.txt\n\nFor some chapters that need to interact with certain sites that require account information. For example, for Google map services, you need to save your API key in a file named api_key.txt in the root folder of the source.\nRender the book with quarto render from the root folder on a terminal; the rendered book will be stored under _book.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#midterm-project",
    "href": "index.html#midterm-project",
    "title": "Introduction to Data Science",
    "section": "Midterm Project",
    "text": "Midterm Project\nReproduce NYC street flood research (Agonafir, Lakhankar, et al., 2022; Agonafir, Pabon, et al., 2022).\nFour students will be selected to present their work in a workshop at the 2025 NYC Open Data Week. You are welcome to invite your family and friends to join the the workshop.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#final-project",
    "href": "index.html#final-project",
    "title": "Introduction to Data Science",
    "section": "Final Project",
    "text": "Final Project\nStudents are encouraged to start designing their final projects from the beginning of the semester. There are many open data that can be used. Here is a list of data challenges that you may find useful:\n\nASA Data Challenge Expo: big data in 2025\nKaggle\nDrivenData\n15 Data Science Hackathons to Test Your Skills in 2025\nIf you work on sports analytics, you are welcome to submit a poster to Connecticut Sports Analytics Symposium (CSAS) 2025.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#adapting-to-rapid-skill-acquisition",
    "href": "index.html#adapting-to-rapid-skill-acquisition",
    "title": "Introduction to Data Science",
    "section": "Adapting to Rapid Skill Acquisition",
    "text": "Adapting to Rapid Skill Acquisition\nIn this course, students are expected to rapidly acquire new skills, a critical aspect of data science. To emphasize this, consider this insightful quote from VanderPlas (2016):\n\nWhen a technologically-minded person is asked to help a friend, family member, or colleague with a computer problem, most of the time it’s less a matter of knowing the answer as much as knowing how to quickly find an unknown answer. In data science it’s the same: searchable web resources such as online documentation, mailing-list threads, and StackOverflow answers contain a wealth of information, even (especially?) if it is a topic you’ve found yourself searching before. Being an effective practitioner of data science is less about memorizing the tool or command you should use for every possible situation, and more about learning to effectively find the information you don’t know, whether through a web search engine or another means.\n\nThis quote captures the essence of what we aim to develop in our students: the ability to swiftly navigate and utilize the vast resources available to solve complex problems in data science. Examples tasks are: install needed software (or even hardware); search and find solutions to encountered problems.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#wishlist",
    "href": "index.html#wishlist",
    "title": "Introduction to Data Science",
    "section": "Wishlist",
    "text": "Wishlist\nThis is a wish list from all members of the class (alphabetical order, last name first, comma, then first name). Here is an example.\n\nYan, Jun\n\nMake practical data science tools accessible to undergraduates.\nPass real-world data science project experience to students.\nCo-develop a Quarto book in collaboration with the students.\nTrain students to participate in real data science competitions.\n\n\nAdd yours through a pull request; note the syntax of nested list in Markdown.\n\nStudents in 3255\n\nAckerman, John\n\nGet comfortable with command line interface\nHands-on experience with AI\nLearn practical tools & tricks for professional data scientist\n\nAlsadadi, Ammar Shaker\n\nLearn about the applications of Data Science in Finance\nLearn more about time series and random walk\n\nChen, Yifei\n\nLearn more advanced python programming skills.\nLearn to use github for future projects\nGet a good grade in this class.\n\nEl Zein, Amer Hani\nFebles, Xavier Milan\n\nFurther develop skills with git\nLearn more about specific tools used for data science\nBecome more comfortable with sql\n\nHorn, Alyssa Noelle -Be confident in using Git and Github -Learn how to collaborate with others on projects through Github\nHutchins, Isabella Grace\nJun, Joann\n\nBecome proficient in using GitHub\nLearn more about the applications of data science\n\nKline, Daniel Esteban\nLagutin, Vladislav\n\nLearn how to do data science projects in python and interact with them using git\nLearn how to do good visualizations of the data; explore appropriate libraries\n\nLang, Lang\nLi, Shiyi\nLin, Selena\n\nGet a good grade in this class.\nLearn and get familier with using GitHub.\nHands on experience with the data science skills learned in this class.\n\nLong, Ethan Kenneth\n\nBecome more comfortable using Git commands and CLI\nLearn more about the data science field\nUnderstand proper coding grammar\nDevelop good learning habits\n\nNasejje, Ruth Nicole\n\nDevelop an organized coding style in python, quarto, & git\nLearn various packages in python related to data science\nDeepen knowledge in statistical modeling and data analysis\n\nPfeifer, Nicholas Theodore\n\nLearn about data science techniques in python\nLearn and thoroughly practice using git and github\nGet more comfortable with decision trees and random forests\n\nReed, Kyle Daniel\n\nGain full confidence using Git/GitHub and corresponding applications.\nUnderstand the workflow in professional data science projects.\nBuild on existing python skills.\n\nRoy, Luke William\n\nHave fun\nDevelop skills in financial data analysis using python and relevant libraries like pandas and numpy.\nLearn advanced data visualization techniques with a focus on the grammar of graphics.\nGet an introduction to machine learning via scikit-learn, and explore applications in financial analysis and forensic accoutning.\n\nSchittina, Thomas\n\nBecome more comfortable using git and GitHub\nBecome more familiar with popular data science packages in Python\n\nSchlessel, Jacob E\nSymula, Sebastian\n\nLearn SQL\nBecome better at working through each step in the data science pipeline to make better, cleaner looking projects\n\nTamhane, Shubhan\nTomaino, Mario Anthony\nXu, Peiwen\n\nLearn some data analysis techniques\nLearn how to use git and other essential tools for data science\n\n\n\n\nStudents in 5255\n\nEdo, Mezmur Wossenu\n\nI hope to become adept working with github.\nI hope to work on real-World data science projects.\nI hope to learn about the different machine learning techniques.\n\nMundiwala, Mohammad Moiz\n\nBecome more familiar with collaboration process of programming so that I can be more orderly while working with others.\nI hope to become more efficient processing data that is messy, unstructured, or unlabeled.\n\nVellore, Ajeeth Krishna\n\nUnderstand the utility provided by GitHub and practice using its tools\nLearn how to participate in a large-scale development project like how they are done in industry\nLearn how to code properly and professionally instead of using “backyard” computer science techniques and formatting\nUnderstand principles of coding documentation and readability while practicing their application\n\nZhang, Gaofei\nKravette, Noah\n\nBecome better at program collaboration.\nBecome adept with git and github.\nBe able to quickly and efficently process and analyze any data.\nGain better skills at data prep, organization, and visulization.\nLearn new helpful statistical tools for data.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#course-logistics",
    "href": "index.html#course-logistics",
    "title": "Introduction to Data Science",
    "section": "Course Logistics",
    "text": "Course Logistics\n\nPresentation Orders\nThe topic presentation order is set up in class.\n\nwith open('rosters/3255.txt', 'r') as file:\n    ug = [line.strip() for line in file]\nwith open('rosters/5255.txt', 'r') as file:\n    gr = [line.strip() for line in file]\npresenters = ug + gr\n\nimport random\n## seed jointly set by the class\nrandom.seed(6895 + 283 + 3184 + 3078 + 5901 + 36)\nrandom.sample(presenters, len(presenters))\n## random.shuffle(presenters) # This would shuffle the list in place\n\n['Li,Shiyi',\n 'Jun,Joann',\n 'Alsadadi,Ammar Shaker',\n 'Lang,Lang',\n 'Ackerman,John',\n 'Horn,Alyssa Noelle',\n 'Xu,Peiwen',\n 'Schittina,Thomas',\n 'Kline,Daniel Esteban',\n 'Edo,Mezmur Wossenu',\n 'Roy,Luke William',\n 'Febles,Xavier Milan',\n 'Tamhane,Shubhan',\n 'Nasejje,Ruth Nicole',\n 'Lagutin,Vladislav',\n 'Zhang,Gaofei',\n 'Long,Ethan Kenneth',\n 'El Zein,Amer Hani',\n 'Kravette,Noah',\n 'Symula,Sebastian',\n 'Tomaino,Mario Anthony',\n 'Reed,Kyle Daniel',\n 'Chen,Yifei',\n 'Mundiwala,Mohammad Moiz',\n 'Lin,Selena',\n 'Pfeifer,Nicholas Theodore',\n 'Vellore,Ajeeth Krishna']\n\n\nSwitching slots is allowed as long as you find someone who is willing to switch with you. In this case, make a pull request to switch the order and let me know.\nYou are welcome to choose a topic that you are interested the most, subject to some order restrictions. For example, decision tree should be presented before random forest or extreme gradient boosting. This justifies certain requests for switching slots.\n\n\nPresentation Task Board\nHere are some example tasks:\n\nMaking presentations with Quarto\nMarkdown jumpstart\nData science communication skills\nImport/Export data\nArrow as a cross-platform data format\nDatabase operation with Structured query language (SQL)\nGrammer of graphics\nHandling spatial data\nVisualize spatial data in a Google map\nAnimation\nClassification and regression trees\nSupport vector machine\nRandom forest\nNaive Bayes\nBagging vs boosting\nNeural networks\nDeep learning\nTensorFlow\nAutoencoders\nReinforcement learning\nDeveloping a Python package\nWeb scraping\n\nPlease use the following table to sign up.\n\n\n\n\n\n\n\n\nDate\nPresenter\nTopic\n\n\n\n\n09/11\nZachary Blanchard\nPresentation with Quarto\n\n\n09/16\nDeyu Xu\nImport/Export data\n\n\n09/18\nSara Clokey\nCommunications in Data Science\n\n\n09/23\nDorathea Johnson\nDatabase with SQL\n\n\n09/25\nXavier Febles\nStatistical tests\n\n\n09/30\nJack Bienvenue\nVisualizing Spatial Data in a Google Map\n\n\n10/02\nJulia Mazzola\nData VisualiZation with Plotnine\n\n\n10/07\nSuha Akach\nNaive Bayes classifier\n\n\n10/09\nRahul Manna\nAnimation\n\n\n10/23\nJaden Astle\nClassification and Regression Trees\n\n\n10/23\nOlivia Kashalapov\nSynthetic Minority Oversampling Technique (SMOTE)\n\n\n10/28\nData science alumni panel\n\n\n\n10/30\nEmily Borowski\nRandom Forest\n\n\n10/30\nAditya Paricharak\nNeural Networks\n\n\n11/04\nMelanie Desroches\nWeb Scraping\n\n\n11/06\nQianruo Tan\nReinforcement Learning\n\n\n11/11\nAansh Jha\nK-means Clustering\n\n\n11/11\nOwen Babiec\nCalling R from Python and Vice Versa\n\n\n11/13\nStef Baptista\n\n\n\n11/13\nMohammad Parvez\nExtracting and Analyzing Census Data\n\n\n\n\n\nFinal Project Presentation Schedule\nWe use the same order as the topic presentation for undergraduate final presentation. An introduction on how to use Quarto to prepare presentation slides is availabe under the templates directory in the classnotes source tree, thank to Zachary Blanchard, which can be used as a template to start with.\n\n\n\n\n\n\n\nDate\nPresenter\n\n\n\n\n11/18\nSara Clokey; Dorothea Johnson; Xavier Febles; Jack Bienvenue\n\n\n11/20\nJulia Mazzola; Suha Akach; Rahul Manna; Jaden Astle\n\n\n12/02\nOlivia Kashalapov; Emily Borowski；Qianruo Tan; Melanie Desroches\n\n\n12/04\nAditya Paricharak; Aansh Jha; Owen Babiec; Stef Baptista\n\n\n\n\n\nContributing to the Class Notes\nContribution to the class notes is through a `pull request’.\n\nStart a new branch and switch to the new branch.\nOn the new branch, add a qmd file for your presentation\nIf using Python, create and activate a virtual environment with requirements.txt\nEdit _quarto.yml add a line for your qmd file to include it in the notes.\nWork on your qmd file, test with quarto render.\nWhen satisfied, commit and make a pull request with your quarto files and an updated requirements.txt.\n\nI have added a template file mysection.qmd and a new line to _quarto.yml as an example.\nFor more detailed style guidance, please see my notes on statistical writing.\nPlagiarism is to be prevented. Remember that these class notes are publicly available online with your names attached. Here are some resources on how to avoid plagiarism. In particular, in our course, one convenient way to avoid plagiarism is to use our own data (e.g., NYC Open Data). Combined with your own explanation of the code chunks, it would be hard to plagiarize.\n\n\nHomework Logistics\n\nWorkflow of Submitting Homework Assisngment\n\nClick the GitHub classroom assignment link in HuskCT announcement.\nAccept the assignment and follow the instructions to an empty repository.\nMake a clone of the repo at an appropriate folder on your own computer with git clone.\nGo to this folder, add your qmd source, work on it, and group your changes to different commits.\nPush your work to your GitHub repo with git push.\nCreate a new release and put the generated pdf file in it for ease of grading.\n\n\n\nRequirements\n\nUse the repo from Git Classroom to submit your work. See Chapter 2  Project Management.\n\nKeep the repo clean (no tracking generated files).\n\nNever “Upload” your files; use the git command lines.\nMake commit message informative (think about the readers).\n\nMake at least 10 commits and form a style of frequent small commits.\n\nTrack quarto sources only in your repo. See Chapter 3  Reproducible Data Science.\nFor the convenience of grading, add your standalone html or pdf output to a release in your repo.\nFor standalone pdf output, you will need to have LaTeX installed.\n\n\n\n\nQuizzes about Syllabus\n\nDo I accept late homework?\nCould you list a few examples of email etiquette?\nHow would you lose style points?\nWould you use CLI and GUI?\nHow many students will present at 2025 NYC ODW and when will the presentations be?\nWhat’s the first date on which you have to complete something about your final project?\nCan you use AI for any task in this course?\nAnybody needs a reference letter? How could you help me to help you?",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#practical-tips",
    "href": "index.html#practical-tips",
    "title": "Introduction to Data Science",
    "section": "Practical Tips",
    "text": "Practical Tips\n\nData analysis\n\nUse an IDE so you can play with the data interactively\nCollect codes that have tested out into a script for batch processing\nDuring data cleaning, keep in mind how each variable will be used later\nNo keeping large data files in a repo; assume a reasonable location with your collaborators\n\n\n\nPresentation\n\nDon’t forget to introduce yourself if there is no moderator.\nHighlight your research questions and results, not code.\nGive an outline, carry it out, and summarize.\nUse your own examples to reduce the risk of plagiarism.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#my-presentation-topic-template",
    "href": "index.html#my-presentation-topic-template",
    "title": "Introduction to Data Science",
    "section": "My Presentation Topic (Template)",
    "text": "My Presentation Topic (Template)\nThis section was prepared by John Smith.\nUse Markdown syntax. If not clear on what to do, learn from the class notes sources.\n\nPay attention to the sectioning levels.\nCite references with their bib key.\nIn examples, maximize usage of data set that the class is familiar with.\nCould use datasets in Python packages or downloadable on the fly.\nTest your section by quarto render &lt;filename.qmd&gt;.\n\n\nIntroduction\nHere is an overview.\n\n\nSub Topic 1\nPut materials on topic 1 here\nPython examples can be put into python code chunks:\n\n# import pandas as pd\n\n# do something\n\n\n\nSub Topic 2\nPut materials on topic 2 here.\n\n\nSub Topic 3\nPut matreials on topic 3 here.\n\n\nConclusion\nPut sumaries here.\n\n\nFurther Readings\nPut links to further materials.\n\n\n\n\nAgonafir, C., Lakhankar, T., Khanbilvardi, R., Krakauer, N., Radell, D., & Devineni, N. (2022). A machine learning approach to evaluate the spatial variability of New York City’s 311 street flooding complaints. Computers, Environment and Urban Systems, 97, 101854.\n\n\nAgonafir, C., Pabon, A. R., Lakhankar, T., Khanbilvardi, R., & Devineni, N. (2022). Understanding New York City street flooding through 311 complaints. Journal of Hydrology, 605, 127300.\n\n\nVanderPlas, J. (2016). Python data science handbook: Essential tools for working with data. O’Reilly Media, Inc.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 What Is Data Science?\nData science is a multifaceted field, often conceptualized as resting on three fundamental pillars: mathematics/statistics, computer science, and domain-specific knowledge. This framework helps to underscore the interdisciplinary nature of data science, where expertise in one area is often complemented by foundational knowledge in the others.\nA compelling definition was offered by Prof. Bin Yu in her 2014 Presidential Address to the Institute of Mathematical Statistics. She defines \\[\\begin{equation*}\n\\mbox{Data Science} =\n\\mbox{S}\\mbox{D}\\mbox{C}^3,\n\\end{equation*}\\] where\nComputing underscores the need for proficiency in programming and algorithmic thinking, collaboration/teamwork reflects the inherently collaborative nature of data science projects, often requiring teams with diverse skill sets, and communication to outsiders emphasizes the importance of translating complex data insights into understandable and actionable information for non-experts.\nThis definition neatly captures the essence of data science, emphasizing a balance between technical skills, teamwork, and the ability to communicate effectively.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#what-is-data-science",
    "href": "intro.html#what-is-data-science",
    "title": "1  Introduction",
    "section": "",
    "text": "‘S’ represents Statistics, signifying the crucial role of statistical methods in understanding and interpreting data;\n‘D’ stands for domain or science knowledge, indicating the importance of specialized expertise in a particular field of study;\nthe three ’C’s denotes computing, collaboration/teamwork, and communication to outsiders.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#expectations-from-this-course",
    "href": "intro.html#expectations-from-this-course",
    "title": "1  Introduction",
    "section": "1.2 Expectations from This Course",
    "text": "1.2 Expectations from This Course\nIn this course, students will be expected to achieve the following outcomes:\n\nProficiency in Project Management with Git: Develop a solid understanding of Git for efficient and effective project management. This involves mastering version control, branching, and collaboration through this powerful tool.\nProficiency in Project Reporting with Quarto: Gain expertise in using Quarto for professional-grade project reporting. This encompasses creating comprehensive and visually appealing reports that effectively communicate your findings.\nHands-On Experience with Real-World Data Science Projects: Engage in practical data science projects that reflect real-world scenarios. This hands-on approach is designed to provide you with direct experience in tackling actual data science challenges.\nCompetency in Using Python and Its Extensions for Data Science: Build strong skills in Python, focusing on its extensions relevant to data science. This includes libraries like Pandas, NumPy, and Matplotlib, among others, which are critical for data analysis and visualization.\nFull Grasp of the Meaning of Results from Data Science Algorithms: Learn to not only apply data science algorithms but also to deeply understand the implications and meanings of their results. This is crucial for making informed decisions based on these outcomes.\nBasic Understanding of the Principles of Data Science Methods: Acquire a foundational knowledge of the underlying principles of various data science methods. This understanding is key to effectively applying these methods in practice.\nCommitment to the Ethics of Data Science: Emphasize the importance of ethical considerations in data science. This includes understanding data privacy, bias in data and algorithms, and the broader social implications of data science work.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#computing-environment",
    "href": "intro.html#computing-environment",
    "title": "1  Introduction",
    "section": "1.3 Computing Environment",
    "text": "1.3 Computing Environment\nAll setups are operating system dependent. As soon as possible, stay away from Windows. Otherwise, good luck (you will need it).\n\n1.3.1 Command Line Interface\nOn Linux or MacOS, simply open a terminal.\nOn Windows, several options can be considered.\n\nWindows Subsystem Linux (WSL): https://learn.microsoft.com/en-us/windows/wsl/\nCygwin (with X): https://x.cygwin.com\nGit Bash: https://www.gitkraken.com/blog/what-is-git-bash\n\nTo jump start, here is a tutorial: Ubunto Linux for beginners.\nAt least, you need to know how to handle files and traverse across directories. The tab completion and introspection supports are very useful.\nHere are several commonly used shell commands:\n\ncd: change directory; .. means parent directory.\npwd: present working directory.\nls: list the content of a folder; -l long version; -a show hidden files; -t ordered by modification time.\nmkdir: create a new directory.\ncp: copy file/folder from a source to a target.\nmv: move file/folder from a source to a target.\nrm: remove a file a folder.\n\n\n\n1.3.2 Python\nSet up Python on your computer:\n\nPython 3.\nPython package manager miniconda or pip.\nIntegrated Development Environment (IDE) (Jupyter Notebook; RStudio; VS Code; Emacs; etc.)\n\nI will be using VS Code in class.\nReadability is important! Check your Python coding styles against the recommended styles: https://peps.python.org/pep-0008/. A good place to start is the Section on “Code Lay-out”.\nOnline books on Python for data science:\n\n“Python Data Science Handbook: Essential Tools for Working with Data,” First Edition, by Jake VanderPlas, O’Reilly Media, 2016.\n\n\n“Python for Data Analysis: Data Wrangling with Pandas, NumPy, and IPython.” Third Edition, by Wes McK- inney, O’Reilly Media, 2022.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#data-science-ethics",
    "href": "intro.html#data-science-ethics",
    "title": "1  Introduction",
    "section": "1.4 Data Science Ethics",
    "text": "1.4 Data Science Ethics\n\n1.4.1 Introduction\nEthics in data science is a fundamental consideration throughout the lifecycle of any project. Data science ethics refers to the principles and practices that guide responsible and fair use of data to ensure that individual rights are respected, societal welfare is prioritized, and harmful outcomes are avoided. Ethical frameworks like the Belmont Report (Protection of Human Subjects of Biomedical & Research, 1979)} and regulations such as the Health Insurance Portability and Accountability Act (HIPAA) (Health & Services, 1996) have established foundational principles that inspire ethical considerations in research and data use. This section explores key principles of ethical data science and provides guidance on implementing these principles in practice.\n\n\n1.4.2 Principles of Ethical Data Science\n\n1.4.2.1 Respect for Privacy\nSafeguarding privacy is critical in data science. Projects should comply with data protection regulations, such as the General Data Protection Regulation (GDPR) or the California Consumer Privacy Act (CCPA). Techniques like anonymization and pseudonymization must be applied to protect sensitive information. Beyond legal compliance, data scientists should consider the ethical implications of using personal data.\nThe principles established by the Belmont Report emphasize respect for persons, which aligns with safeguarding individual privacy. Protecting privacy also involves limiting data collection to what is strictly necessary. Minimizing the use of identifiable information and implementing secure data storage practices are essential steps. Transparency about how data is used further builds trust with stakeholders.\n\n\n1.4.2.2 Commitment to Fairness\nBias can arise at any stage of the data science pipeline, from data collection to algorithm development. Ethical practice requires actively identifying and addressing biases to prevent harm to underrepresented groups. Fairness should guide the design and deployment of models, ensuring equitable treatment across diverse populations.\nTo achieve fairness, data scientists must assess datasets for representativeness and use tools to detect potential biases. Regular evaluation of model outcomes against fairness metrics helps ensure that systems remain non-discriminatory. The Americans with Disabilities Act (ADA) (Congress, 1990) provides a legal framework emphasizing equitable access, which can inspire fairness in algorithmic design. Collaborating with domain experts and stakeholders can provide additional insights into fairness issues.\n\n\n1.4.2.3 Emphasis on Transparency\nTransparency builds trust and accountability in data science. Models should be interpretable, with clear documentation explaining their design, assumptions, and decision-making processes. Data scientists must communicate results in a way that stakeholders can understand, avoiding unnecessary complexity or obfuscation.\nTransparent practices include providing stakeholders access to relevant information about model performance and limitations. The Federal Data Strategy (Team, 2019) calls for transparency in public sector data use, offering inspiration for practices in broader contexts. Visualizing decision pathways and using tools like LIME or SHAP can enhance interpretability. Establishing clear communication protocols ensures that non-technical audiences can engage with the findings effectively.\n\n\n1.4.2.4 Focus on Social Responsibility\nData science projects must align with ethical goals and anticipate their broader societal and environmental impacts. This includes considering how outputs may be used or misused and avoiding harm to vulnerable populations. Data scientists should aim to use their expertise to promote public welfare, addressing critical societal challenges such as health disparities, climate change, and education access.\nEngaging with diverse perspectives helps align projects with societal values. Ethical codes, such as those from the Association for Computing Machinery (ACM) (Computing Machinery (ACM), 2018), offer guidance on using technology for social good. Collaborating with policymakers and community representatives ensures that data-driven initiatives address real needs and avoid unintended consequences. Regular impact assessments help measure whether projects meet their ethical objectives.\n\n\n1.4.2.5 Adherence to Professional Integrity\nProfessional integrity underpins all ethical practices in data science. Adhering to established ethical guidelines, such as those from the American Statistical Association (ASA) ((ASA), 2018), ensures accountability. Practices like maintaining informed consent, avoiding data manipulation, and upholding rigor in analyses are essential for maintaining public trust in the field.\nEthical integrity also involves fostering a culture of honesty and openness within data science teams. Peer review and independent validation of findings can help identify potential errors or biases. Documenting methodologies and maintaining transparency in reporting further strengthen trust.\n\n\n\n1.4.3 Ensuring Ethics in Practice\n\n1.4.3.1 Building Ethical Awareness\nPromoting ethical awareness begins with education and training. Institutions should integrate ethics into data science curricula, emphasizing real-world scenarios and decision-making. Organizations should conduct regular training to ensure their teams remain informed about emerging ethical challenges.\nWorkshops and case studies can help data scientists understand the complexities of ethical decision-making. Providing access to resources, such as ethical guidelines and tools, supports continuous learning. Leadership support is critical for embedding ethics into organizational culture.\n\n\n1.4.3.2 Embedding Ethics in Workflows\nEthics must be embedded into every stage of the data science pipeline. Establishing frameworks for ethical review, such as ethics boards or peer-review processes, helps identify potential issues early. Tools for bias detection, explainability, and privacy protection should be standard components of workflows.\nStandard operating procedures for ethical reviews can formalize the consideration of ethics in project planning. Developing templates for documenting ethical decisions ensures consistency and accountability. Collaboration across teams enhances the ability to address ethical challenges comprehensively.\n\n\n1.4.3.3 Establishing Accountability Mechanisms\nClear accountability mechanisms are essential for ethical governance. This includes maintaining documentation for all decisions, establishing audit trails, and assigning responsibility for the outputs of data-driven systems. Organizations should encourage open dialogue about ethical concerns and support whistleblowers who raise issues.\nPeriodic audits of data science projects help ensure compliance with ethical standards. Organizations can benefit from external reviews to identify blind spots and improve their practices. Accountability fosters trust and aligns teams with ethical objectives.\n\n\n1.4.3.4 Engaging Stakeholders\nEthical data science requires collaboration with diverse stakeholders. Including perspectives from affected communities, policymakers, and interdisciplinary experts ensures that projects address real needs and avoid unintended consequences. Stakeholder engagement fosters trust and aligns projects with societal values.\nPublic consultations and focus groups can provide valuable feedback on the potential impacts of data science projects. Engaging with regulators and advocacy groups helps align projects with legal and ethical expectations. Transparent communication with stakeholders builds long-term relationships.\n\n\n1.4.3.5 Continuous Improvement\nEthics in data science is not static; it evolves with technology and societal expectations. Continuous improvement requires regular review of ethical practices, learning from past projects, and adapting to new challenges. Organizations should foster a culture of reflection and growth to remain aligned with ethical best practices.\nEstablishing mechanisms for feedback on ethical practices can identify areas for development. Sharing lessons learned through conferences and publications helps the broader community advance its understanding of ethics in data science.\n\n\n\n1.4.4 Conclusion\nData science ethics is a dynamic and integral aspect of the discipline. By adhering to principles of privacy, fairness, transparency, social responsibility, and integrity, data scientists can ensure their work contributes positively to society. Implementing these principles through structured workflows, stakeholder engagement, and continuous improvement establishes a foundation for trustworthy and impactful data science.\n\n\n\n\n(ASA), A. S. A. (2018). Ethical guidelines for statistical practice.\n\n\nComputing Machinery (ACM), A. for. (2018). Code of ethics and professional conduct.\n\n\nCongress, U. S. (1990). Americans with disabilities act of 1990 (ADA).\n\n\nHealth, U. S. D. of, & Services, H. (1996). Health insurance portability and accountability act of 1996 (HIPAA).\n\n\nProtection of Human Subjects of Biomedical, N. C. for the, & Research, B. (1979). The belmont report: Ethical principles and guidelines for the protection of human subjects of research.\n\n\nTeam, F. D. S. D. (2019). Federal data strategy 2020 action plan.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "git.html",
    "href": "git.html",
    "title": "2  Project Management",
    "section": "",
    "text": "2.1 Set Up Git/GitHub\nMany tutorials are available in different formats. Here is a YouTube video ``Git and GitHub for Beginners — Crash Course’’. The video also covers GitHub, a cloud service for Git which provides a cloud back up of your work and makes collaboration with co-workers easy. Similar services are, for example, bitbucket and GitLab.\nThere are tools that make learning Git easy.\nDownload Git if you don’t have it already.\nTo set up GitHub (other services like Bitbucket or GitLab are similar), you need to\nSee how to get started with GitHub account.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Management</span>"
    ]
  },
  {
    "objectID": "git.html#set-up-gitgithub",
    "href": "git.html#set-up-gitgithub",
    "title": "2  Project Management",
    "section": "",
    "text": "Generate an SSH key if you don’t have one already.\nSign up an GitHub account.\nAdd the SSH key to your GitHub account",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Management</span>"
    ]
  },
  {
    "objectID": "git.html#most-frequently-used-git-commands",
    "href": "git.html#most-frequently-used-git-commands",
    "title": "2  Project Management",
    "section": "2.2 Most Frequently Used Git Commands",
    "text": "2.2 Most Frequently Used Git Commands\nThe following seven commands will get you started and they may be all that you need most of the time.\n\ngit clone:\n\nUsed to clone a repository to a local folder.\nRequires either HTTPS link or SSH key to authenticate.\n\ngit pull:\n\nDownloads any updates made to the remote repository and automatically updates the local repository.\n\ngit status:\n\nReturns the state of the working directory.\nLists the files that have been modified, and are yet to be or have been staged and/or committed.\nShows if the local repository is begind or ahead a remote branch.\n\ngit add:\n\nAdds new or modified files to the Git staging area.\nGives the option to select which files are to be sent to the remote repository\n\ngit rm:\n\nUsed to remove files from the staging index or the local repository.\n\ngit commit:\n\nCommits changes made to the local repository and saves it like a snapshot.\nA message is recommended with every commit to keep track of changes made.\n\ngit push:\n\nUsed to send commits made on local repository to the remote repository.\n\n\nFor more advanced usages:\n\ngit diff\ngit branch\ngit reset",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Management</span>"
    ]
  },
  {
    "objectID": "git.html#tips-on-using-git",
    "href": "git.html#tips-on-using-git",
    "title": "2  Project Management",
    "section": "2.3 Tips on using Git:",
    "text": "2.3 Tips on using Git:\n\nUse the command line interface instead of the web interface (e.g., upload on GitHub)\nMake frequent small commits instead of rare large commits.\nMake commit messages informative and meaningful.\nName your files/folders by some reasonable convention.\n\nLower cases are better than upper cases.\nNo blanks in file/folder names.\n\nKeep the repo clean by not tracking generated files.\nCreat a .gitignore file for better output from git status.\nKeep the linewidth of sources to under 80 for better git diff view.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Management</span>"
    ]
  },
  {
    "objectID": "git.html#pull-request",
    "href": "git.html#pull-request",
    "title": "2  Project Management",
    "section": "2.4 Pull Request",
    "text": "2.4 Pull Request\nTo contribute to an open source project (e.g., our classnotes), use pull requests. Pull requests “let you tell others about changes you’ve pushed to a branch in a repository on GitHub. Once a pull request is opened, you can discuss and review the potential changes with collaborators and add follow-up commits before your changes are merged into the base branch.”\nWatch this YouTube video: GitHub pull requests in 100 seconds.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Management</span>"
    ]
  },
  {
    "objectID": "quarto.html",
    "href": "quarto.html",
    "title": "3  Reproducible Data Science",
    "section": "",
    "text": "3.1 Introduction to Quarto\nData science projects should be reproducible to be trustworthy. Dynamic documents facilitate reproducibility. Quarto is an open-source dynamic document preparation system, ideal for scientific and technical publishing. From the official websites, Quarto can be used to:\nTo get started with Quarto, see documentation at Quarto.\nFor a clean style, I suggest that you use VS Code as your IDE. The ipynb files have extra formats in plain texts, which are not as clean as qmd files. There are, of course, tools to convert between the two representations of a notebook. For example:\nWe will use Quarto for homework assignments, classnotes, and presentations. You will see them in action through in-class demonstrations. The following sections in the Quarto Guide are immediately useful.\nA template for homework is in this repo (hwtemp.qmd) to get you started with homework assignments.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reproducible Data Science</span>"
    ]
  },
  {
    "objectID": "quarto.html#introduction-to-quarto",
    "href": "quarto.html#introduction-to-quarto",
    "title": "3  Reproducible Data Science",
    "section": "",
    "text": "quarto convert hello.ipynb # converts to qmd\nquarto convert hello.qmd   # converts to ipynb\n\n\nMarkdown basics\nUsing Python\nPresentations",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reproducible Data Science</span>"
    ]
  },
  {
    "objectID": "quarto.html#sec-buildnotes",
    "href": "quarto.html#sec-buildnotes",
    "title": "3  Reproducible Data Science",
    "section": "3.2 Compiling the Classnotes",
    "text": "3.2 Compiling the Classnotes\nThe sources of the classnotes are at https://github.com/statds/ids-s25. This is also the source tree that you will contributed to this semester. I expect that you clone the repository to your own computer, update it frequently, and compile the latest version on your computer (reproducibility).\nTo compile the classnotes, you need the following tools: Git, Quarto, and Python.\n\n3.2.1 Set up your Python Virtual Environment\nI suggest that a Python virtual environment for the classnotes be set up in the current directory for reproducibility. A Python virtual environment is simply a directory with a particular file structure, which contains a specific Python interpreter and software libraries and binaries needed to support a project. It allows us to isolate our Python development projects from our system installed Python and other Python environments.\nTo create a Python virtual environment for our classnotes:\npython3 -m venv .ids-s25-venv\nHere .ids-s25-venv is the name of the virtual environment to be created. Choose an informative name. This only needs to be set up once.\nTo activate this virtual environment:\n. .ids-s25-venv/bin/activate\nAfter activating the virtual environment, you will see (.ids-s25-venv) at the beginning of your shell prompt. Then, the Python interpreter and packages needed will be the local versions in this virtual environment without interfering your system-wide installation or other virtual environments.\nTo install the Python packages that are needed to compile the classnotes, we have a requirements.txt file that specifies the packages and their versions. They can be installed easily with:\npip install -r requirements.txt\nIf you are interested in learning how to create the requirements.txt file, just put your question into a Google search.\nTo exit the virtual environment, simply type deactivate in your command line. This will return you to your system’s global Python environment.\n\n\n3.2.2 Clone the Repository\nClone the repository to your own computer. In a terminal (command line), go to an appropriate directory (folder), and clone the repo. For example, if you use ssh for authentication:\ngit clone git@github.com:statds/ids-s25.git\n\n\n3.2.3 Render the Classnotes\nAssuming quarto has been set up, we render the classnotes in the cloned repository\ncd ids-s25\nquarto render\nIf there are error messages, search and find solutions to clear them. Otherwise, the html version of the notes will be available under _book/index.html, which is default location of the output.\n\n\n3.2.4 Login Requirements\nFor some illustrations, you need to interact with certain sites that require account information. For example, for Google map services, you need to save your API key in a file named api_key.txt in the root folder of the source. Another example is to access the US Census API, where you would need to register an account and get your Census API Key.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reproducible Data Science</span>"
    ]
  },
  {
    "objectID": "python.html",
    "href": "python.html",
    "title": "4  Python Refreshment",
    "section": "",
    "text": "4.1 Know Your Computer",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "python.html#know-your-computer",
    "href": "python.html#know-your-computer",
    "title": "4  Python Refreshment",
    "section": "",
    "text": "4.1.1 Operating System\nYour computer has an operating system (OS), which is responsible for managing the software packages on your computer. Each operating system has its own package management system. For example:\n\nLinux: Linux distributions have a variety of package managers depending on the distribution. For instance, Ubuntu uses APT (Advanced Package Tool), Fedora uses DNF (Dandified Yum), and Arch Linux uses Pacman. These package managers are integral to the Linux experience, allowing users to install, update, and manage software packages easily from repositories.\nmacOS: macOS uses Homebrew as its primary package manager. Homebrew simplifies the installation of software and tools that aren’t included in the standard macOS installation, using simple commands in the terminal.\nWindows: Windows users often rely on the Microsoft Store for apps and software. For more developer-focused package management, tools like Chocolatey and Windows Package Manager (Winget) are used. Additionally, recent versions of Windows have introduced the Windows Subsystem for Linux (WSL). WSL allows Windows users to run a Linux environment directly on Windows, unifying Windows and Linux applications and tools. This is particularly useful for developers and data scientists who need to run Linux-specific software or scripts. It saves a lot of trouble Windows users used to have before its time.\n\nUnderstanding the package management system of your operating system is crucial for effectively managing and installing software, especially for data science tools and applications.\n\n\n4.1.2 File System\nA file system is a fundamental aspect of a computer’s operating system, responsible for managing how data is stored and retrieved on a storage device, such as a hard drive, SSD, or USB flash drive. Essentially, it provides a way for the OS and users to organize and keep track of files. Different operating systems typically use different file systems. For instance, NTFS and FAT32 are common in Windows, APFS and HFS+ in macOS, and Ext4 in many Linux distributions. Each file system has its own set of rules for controlling the allocation of space on the drive and the naming, storage, and access of files, which impacts performance, security, and compatibility. Understanding file systems is crucial for tasks such as data recovery, disk partitioning, and managing file permissions, making it an important concept for anyone working with computers, especially in data science and IT fields.\nNavigating through folders in the command line, especially in Unix-like environments such as Linux or macOS, and Windows Subsystem for Linux (WSL), is an essential skill for effective file management. The command cd (change directory) is central to this process. To move into a specific directory, you use cd followed by the directory name, like cd Documents. To go up one level in the directory hierarchy, you use cd ... To return to the home directory, simply typing cd or cd ~ will suffice. The ls command lists all files and folders in the current directory, providing a clear view of your options for navigation. Mastering these commands, along with others like pwd (print working directory), which displays your current directory, equips you with the basics of moving around the file system in the command line, an indispensable skill for a wide range of computing tasks in Unix-like systems.\nYou have programmed in Python. Regardless of your skill level, let us do some refreshing.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "python.html#the-python-world",
    "href": "python.html#the-python-world",
    "title": "4  Python Refreshment",
    "section": "4.2 The Python World",
    "text": "4.2 The Python World\n\nFunction: a block of organized, reusable code to complete certain task.\nModule: a file containing a collection of functions, variables, and statements.\nPackage: a structured directory containing collections of modules and an __init.py__ file by which the directory is interpreted as a package.\nLibrary: a collection of related functionality of codes. It is a reusable chunk of code that we can use by importing it in our program, we can just use it by importing that library and calling the method of that library with period(.).\n\nSee, for example, how to build a Python libratry.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "python.html#standard-library",
    "href": "python.html#standard-library",
    "title": "4  Python Refreshment",
    "section": "4.3 Standard Library",
    "text": "4.3 Standard Library\nPython’s has an extensive standard library that offers a wide range of facilities as indicated by the long table of contents listed below. See documentation online.\n\nThe library contains built-in modules (written in C) that provide access to system functionality such as file I/O that would otherwise be inaccessible to Python programmers, as well as modules written in Python that provide standardized solutions for many problems that occur in everyday programming. Some of these modules are explicitly designed to encourage and enhance the portability of Python programs by abstracting away platform-specifics into platform-neutral APIs.\n\nQuestion: How to get the constant \\(e\\) to an arbitary precision?\nThe constant is only represented by a given double precision.\n\nimport math\nprint(\"%0.20f\" % math.e)\nprint(\"%0.80f\" % math.e)\n\n2.71828182845904509080\n2.71828182845904509079559829842764884233474731445312500000000000000000000000000000\n\n\nNow use package decimal to export with an arbitary precision.\n\nimport decimal  # for what?\n\n## set the required number digits to 150\ndecimal.getcontext().prec = 150\ndecimal.Decimal(1).exp().to_eng_string()\ndecimal.Decimal(1).exp().to_eng_string()[2:]\n\n'71828182845904523536028747135266249775724709369995957496696762772407663035354759457138217852516642742746639193200305992181741359662904357290033429526'",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "python.html#important-libraries",
    "href": "python.html#important-libraries",
    "title": "4  Python Refreshment",
    "section": "4.4 Important Libraries",
    "text": "4.4 Important Libraries\n\nNumPy\npandas\nmatplotlib\nIPython/Jupyter\nSciPy\nscikit-learn\nstatsmodels\n\nQuestion: how to draw a random sample from a normal distribution and evaluate the density and distributions at these points?\n\nfrom scipy.stats import norm\n\nmu, sigma = 2, 4\nmean, var, skew, kurt = norm.stats(mu, sigma, moments='mvsk')\nprint(mean, var, skew, kurt)\nx = norm.rvs(loc = mu, scale = sigma, size = 10)\nx\n\n2.0 16.0 0.0 0.0\n\n\narray([-3.28934389,  2.9952612 , -1.32370512, -2.66628623,  6.23720469,\n        3.36843925,  1.39829177,  4.80799741,  3.40809235,  1.02414416])\n\n\nThe pdf and cdf can be evaluated:\n\nnorm.pdf(x, loc = mu, scale = sigma)\n\narray([0.04160566, 0.0966956 , 0.07061928, 0.05050528, 0.05690953,\n       0.09406658, 0.09861351, 0.07795415, 0.09374349, 0.09681125])",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "python.html#writing-a-function",
    "href": "python.html#writing-a-function",
    "title": "4  Python Refreshment",
    "section": "4.5 Writing a Function",
    "text": "4.5 Writing a Function\nConsider the Fibonacci Sequence \\(1, 1, 2, 3, 5, 8, 13, 21, 34, ...\\). The next number is found by adding up the two numbers before it. We are going to use 3 ways to solve the problems.\nThe first is a recursive solution.\n\ndef fib_rs(n):\n    if (n==1 or n==2):\n        return 1\n    else:\n        return fib_rs(n - 1) + fib_rs(n - 2)\n\n%timeit fib_rs(10)\n\n9.25 μs ± 228 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n\n\nThe second uses dynamic programming memoization.\n\ndef fib_dm_helper(n, mem):\n    if mem[n] is not None:\n        return mem[n]\n    elif (n == 1 or n == 2):\n        result = 1\n    else:\n        result = fib_dm_helper(n - 1, mem) + fib_dm_helper(n - 2, mem)\n    mem[n] = result\n    return result\n\ndef fib_dm(n):\n    mem = [None] * (n + 1)\n    return fib_dm_helper(n, mem)\n\n%timeit fib_dm(10)\n\n1.96 μs ± 28.7 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n\n\nThe third is still dynamic programming but bottom-up.\n\ndef fib_dbu(n):\n    mem = [None] * (n + 1)\n    mem[1] = 1;\n    mem[2] = 1;\n    for i in range(3, n + 1):\n        mem[i] = mem[i - 1] + mem[i - 2]\n    return mem[n]\n\n\n%timeit fib_dbu(500)\n\n68.3 μs ± 1.91 μs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n\n\nApparently, the three solutions have very different performance for larger n.\n\n4.5.1 Monty Hall\nHere is a function that performs the Monty Hall experiments.\n\nimport numpy as np\n\ndef montyhall(ndoors, ntrials):\n    doors = np.arange(1, ndoors + 1) / 10\n    prize = np.random.choice(doors, size=ntrials)\n    player = np.random.choice(doors, size=ntrials)\n    host = np.array([np.random.choice([d for d in doors\n                                       if d not in [player[x], prize[x]]])\n                     for x in range(ntrials)])\n    player2 = np.array([np.random.choice([d for d in doors\n                                          if d not in [player[x], host[x]]])\n                        for x in range(ntrials)])\n    return {'noswitch': np.sum(prize == player), 'switch': np.sum(prize == player2)}\n\nTest it out:\n\nmontyhall(3, 1000)\nmontyhall(4, 1000)\n\n{'noswitch': np.int64(234), 'switch': np.int64(331)}\n\n\nThe true value for the two strategies with \\(n\\) doors are, respectively, \\(1 / n\\) and \\(\\frac{n - 1}{n (n - 2)}\\).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "python.html#variables-versus-objects",
    "href": "python.html#variables-versus-objects",
    "title": "4  Python Refreshment",
    "section": "4.6 Variables versus Objects",
    "text": "4.6 Variables versus Objects\nIn Python, variables and the objects they point to actually live in two different places in the computer memory. Think of variables as pointers to the objects they’re associated with, rather than being those objects. This matters when multiple variables point to the same object.\n\nx = [1, 2, 3]  # create a list; x points to the list\ny = x          # y also points to the same list in the memory\ny.append(4)    # append to y\nx              # x changed!\n\n[1, 2, 3, 4]\n\n\nNow check their addresses\n\nprint(id(x))   # address of x\nprint(id(y))   # address of y\n\n4521052672\n4521052672\n\n\nNonetheless, some data types in Python are “immutable”, meaning that their values cannot be changed in place. One such example is strings.\n\nx = \"abc\"\ny = x\ny = \"xyz\"\nx\n\n'abc'\n\n\nNow check their addresses\n\nprint(id(x))   # address of x\nprint(id(y))   # address of y\n\n4362603040\n4446245184\n\n\nQuestion: What’s mutable and what’s immutable?\nAnything that is a collection of other objects is mutable, except tuples.\nNot all manipulations of mutable objects change the object rather than create a new object. Sometimes when you do something to a mutable object, you get back a new object. Manipulations that change an existing object, rather than create a new one, are referred to as “in-place mutations” or just “mutations.” So:\n\nAll manipulations of immutable types create new objects.\nSome manipulations of mutable types create new objects.\n\nDifferent variables may all be pointing at the same object is preserved through function calls (a behavior known as “pass by object-reference”). So if you pass a list to a function, and that function manipulates that list using an in-place mutation, that change will affect any variable that was pointing to that same object outside the function.\n\nx = [1, 2, 3]\ny = x\n\ndef append_42(input_list):\n    input_list.append(42)\n    return input_list\n\nappend_42(x)\n\n[1, 2, 3, 42]\n\n\nNote that both x and y have been appended by \\(42\\).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "python.html#number-representation",
    "href": "python.html#number-representation",
    "title": "4  Python Refreshment",
    "section": "4.7 Number Representation",
    "text": "4.7 Number Representation\nNumers in a computer’s memory are represented by binary styles (on and off of bits).\n\n4.7.1 Integers\nIf not careful, It is easy to be bitten by overflow with integers when using Numpy and Pandas in Python.\n\nimport numpy as np\n\nx = np.array(2 ** 63 - 1 , dtype = 'int')\nx\n# This should be the largest number numpy can display, with\n# the default int8 type (64 bits)\n\narray(9223372036854775807)\n\n\nNote: on Windows and other platforms, dtype = 'int' may have to be changed to dtype = np.int64 for the code to execute. Source: Stackoverflow\nWhat if we increment it by 1?\n\ny = np.array(x + 1, dtype = 'int')\ny\n# Because of the overflow, it becomes negative!\n\narray(-9223372036854775808)\n\n\nFor vanilla Python, the overflow errors are checked and more digits are allocated when needed, at the cost of being slow.\n\n2 ** 63 * 1000\n\n9223372036854775808000\n\n\nThis number is 1000 times larger than the prior number, but still displayed perfectly without any overflows\n\n\n4.7.2 Floating Number\nStandard double-precision floating point number uses 64 bits. Among them, 1 is for sign, 11 is for exponent, and 52 are fraction significand, See https://en.wikipedia.org/wiki/Double-precision_floating-point_format. The bottom line is that, of course, not every real number is exactly representable.\nIf you have played the Game 24, here is a tricky one:\n\n8 / (3 - 8 / 3) == 24\n\nFalse\n\n\nSurprise?\nThere are more.\n\n0.1 + 0.1 + 0.1 == 0.3\n\nFalse\n\n\n\n0.3 - 0.2 == 0.1\n\nFalse\n\n\nWhat is really going on?\n\nimport decimal\ndecimal.Decimal(0.1)\n\nDecimal('0.1000000000000000055511151231257827021181583404541015625')\n\n\n\ndecimal.Decimal(8 / (3 - 8 / 3))\n\nDecimal('23.999999999999989341858963598497211933135986328125')\n\n\nBecause the mantissa bits are limited, it can not represent a floating point that’s both very big and very precise. Most computers can represent all integers up to \\(2^{53}\\), after that it starts skipping numbers.\n\n2.1 ** 53 + 1 == 2.1 ** 53\n\n# Find a number larger than 2 to the 53rd\n\nTrue\n\n\n\nx = 2.1 ** 53\nfor i in range(1000000):\n    x = x + 1\nx == 2.1 ** 53\n\nTrue\n\n\nWe add 1 to x by 1000000 times, but it still equal to its initial value, 2.1 ** 53. This is because this number is too big that computer can’t handle it with precision like add 1.\nMachine epsilon is the smallest positive floating-point number x such that 1 + x != 1.\n\nprint(np.finfo(float).eps)\nprint(np.finfo(np.float32).eps)\n\n2.220446049250313e-16\n1.1920929e-07",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "python.html#sec-python-venv",
    "href": "python.html#sec-python-venv",
    "title": "4  Python Refreshment",
    "section": "4.8 Virtual Environment",
    "text": "4.8 Virtual Environment\nVirtual environments in Python are essential tools for managing dependencies and ensuring consistency across projects. They allow you to create isolated environments for each project, with its own set of installed packages, separate from the global Python installation. This isolation prevents conflicts between project dependencies and versions, making your projects more reliable and easier to manage. It’s particularly useful when working on multiple projects with differing requirements, or when collaborating with others who may have different setups.\nTo set up a virtual environment, you first need to ensure that Python is installed on your system. Most modern Python installations come with the venv module, which is used to create virtual environments. Here’s how to set one up:\n\nOpen your command line interface.\nNavigate to your project directory.\nRun python3 -m venv myenv, where myenv is the name of the virtual environment to be created. Choose an informative name.\n\nThis command creates a new directory named myenv (or your chosen name) in your project directory, containing the virtual environment.\nTo start using this environment, you need to activate it. The activation command varies depending on your operating system:\n\nOn Windows, run myenv\\Scripts\\activate.\nOn Linux or MacOS, use source myenv/bin/activate or . myenv/bin/activate.\n\nOnce activated, your command line will typically show the name of the virtual environment, and you can then install and use packages within this isolated environment without affecting your global Python setup.\nTo exit the virtual environment, simply type deactivate in your command line. This will return you to your system’s global Python environment.\nAs an example, let’s install a package, like numpy, in this newly created virtual environment:\n\nEnsure your virtual environment is activated.\nRun pip install numpy.\n\nThis command installs the requests library in your virtual environment. You can verify the installation by running pip list, which should show requests along with its version.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "exercises.html",
    "href": "exercises.html",
    "title": "5  Exercises",
    "section": "",
    "text": "Quarto and Git setup Quarto and Git are two important tools for data science. Get familiar with them through the following tasks. Please use the templates/hw.qmd template to document, for each step, what you did, the obstacles you encountered, and how you overcame them. Think of this as a user manual for students who are new to this. Use the command line interface.\n\nSet up SSH authentication between your computer and your GitHub account.\nInstall Quarto onto your computer following the instructions of Get Started.\nPick a tool of your choice (e.g., VS Code, Jupyter Notebook, Emacs, etc.), follow the instructions to reproduce the example of line plot on polar axis.\nRender the homework into a pdf file and put the file into a release in your GitHub repo.\n\nWorking on Homework Problems All the requirements on homework styles have reasons. Reviewing these questions help you to understand them.\n\nWhat are the differences between binary and source files?\nWhy do we not want to track binary files in a repo?\nWhy do I require pdf output via release?\nWhy do I not want your files added via ‘upload’?\nWhy do I require line width under 80?\nWhy is it not a good idea to have spaces in file/folder names?\n\nContributing to the Class Notes To contribute to the classnotes, you need to have a working copy of the sources on your computer. Document the following steps in a qmd file in the form of a step-by-step manual, as if you are explaining them to someone who wants to contribute too. Make at least 10 commits for this task, each with an informative message.\n\nCreate a fork of the notes repo into your own GitHub account.\nClone it to an appropriate folder on your computer.\nRender the classnotes on your computer; document the obstacles and solutions.\nMake a new branch (and name it appropriately) to experiment with your changes.\nCheckout your branch and add your wishes to the wish list; commit with an informative message; and push the changes to your GitHub account.\nMake a pull request to class notes repo from your fork at GitHub. Make sure you have clear messages to document the changes.\n\nMonty Hall Consider a generalized Monty Hall experiment. Suppose that the game start with \\(n\\) doors; after you pick one, the host opens \\(m \\le n - 2\\) doors, that show no award. Include sufficient text around the code chunks to explain them.\n\nWrite a function to simulate the experiment once. The function takes two arguments ndoors and nempty, which represent the number of doors and the number of empty doors showed by the host, respectively, It returns the result of two strategies, switch and no-switch, from playing this game.\nPlay this game with 3 doors and 1 empty a few times.\nPlay this game with 10 doors and 8 empty a few times.\nWrite a function to play this game ntrial times and return the proportion of wins for both strategies.\nApply your function to play this game 1000 times, with 3 doors and 10 doors, and summarize your results.\nWrite a function to demonstrate the Monty Hall problem through simulation. The function takes two arguments ndoors and ntrials, representing the number of doors in the experiment and the number of trials in a simulation, respectively. The function should return the proportion of wins for both the switch and no-switch strategy.\nApply your function with 3 doors (1 empty) and 10 doors (8 empty), both with 1000 trials. Summarize your results.\n\nApproximating \\(\\pi\\) Write a function to do a Monte Carlo approximation of \\(\\pi\\). The function takes a Monte Carlo sample size n as input, and returns a point estimate of \\(\\pi\\) and a 95% confidence interval. Apply your function with sample size 1000, 2000, 4000, and 8000. Repeat the experiment 1000 times for each sample size and check the empirical probability that the confidence intervals cover the true value of \\(\\pi\\). Comment on the results.\nGoogle Billboard Ad Find the first 10-digit prime number occurring in consecutive digits of \\(e\\). This was a Google recruiting ad.\nGame 24 The math game 24 is one of the addictive games among number lovers. With four randomly selected cards form a deck of poker cards, use all four values and elementary arithmetic operations (\\(+-\\times /\\)) to come up with 24. Let \\(\\square\\) be one of the four numbers. Let \\(\\bigcirc\\) represent one of the four operators. For example, \\[\\begin{equation*}\n(\\square \\bigcirc \\square) \\bigcirc (\\square \\bigcirc \\square)\n\\end{equation*}\\] is one way to group the the operations.\n\nList all the possible ways to group the four numbers.\nHow many possible ways are there to check for a solution?\nWrite a function to solve the problem in a brutal force way. The inputs of the function are four numbers. The function returns a list of solutions. Some of the solutions will be equivalent, but let us not worry about that for now.\n\nNYC Crash Data Cleaning The NYC motor vehicle collisions data with documentation is available from NYC Open Data. The raw data needs some cleaning.\n\nUse the filter from the website to download the crash data of the week of June 30, 2024 in CSV format; save it under a directory data with an informative name (e.g., nyccrashes_2024w0630_by20240916.csv); read the data into a Panda data frame with careful handling of the date time variables.\nClean up the variable names. Use lower cases and replace spaces with underscores.\nGet the basic summaries of each variables: missing percentage; descriptive statistics for continuous variables; frequency tables for discrete variables.\nAre their invalid longitude and latitude in the data? If so, replace them with NA.\nAre there zip_code values that are not legit NYC zip codes? If so, replace them with NA.\nAre there missing in zip_code and borough? Do they always co-occur?\nAre there cases where zip_code and borough are missing but the geo codes are not missing? If so, fill in zip_code and borough using the geo codes.\nIs it redundant to keep both location and the longitude/latitude at the NYC Open Data server?\nCheck the frequency of crash_time by hour. Is there a matter of bad luck at exactly midnight? How would you interpret this?\nAre the number of persons killed/injured the summation of the numbers of pedestrians, cyclist, and motorists killed/injured? If so, is it redundant to keep these two columns at the NYC Open Data server?\nPrint the whole frequency table of contributing_factor_vehicle_1. Convert lower cases to uppercases and check the frequencies again.\nProvided an opportunity to meet the data provider, what suggestions would you make based on your data exploration experience?\n\nNYC Crash Data Exploration Except for the first question, use the cleaned crash data in feather format.\n\nConstruct a contigency table for missing in geocode (latitude and longitude) by borough. Is the missing pattern the same across boroughs? Formulate a hypothesis and test it.\nConstruct a hour variable with integer values from 0 to 23. Plot the histogram of the number of crashes by hour. Plot it by borough.\nOverlay the locations of the crashes on a map of NYC. The map could be a static map or Google map.\nCreate a new variable severe which is one if the number of persons injured or deaths is 1 or more; and zero otherwise. Construct a cross table for severe versus borough. Is the severity of the crashes the same across boroughs? Test the null hypothesis that the two variables are not associated with an appropriate test.\nMerge the crash data with the zip code database.\nFit a logistic model with severe as the outcome variable and covariates that are available in the data or can be engineered from the data. For example, zip code level covariates can be obtained by merging with the zip code database; crash hour; number of vehicles involved.\n\nNYC Crash severity modeling Using the cleaned NYC crash data, merged with zipcode level information, predict severe of a crash.\n\nSet random seed to 1234. Randomly select 20% of the crashes as testing data and leave the rest 80% as training data.\nFit a logistic model on the training data and validate the performance on the testing data. Explain the confusion matrix result from the testing data. Compute the F1 score.\nFit a logistic model on the training data with \\(L_1\\) regularization. Select the tuning parameter with 5-fold cross-validation in F1 score\nApply the regularized logistic regression to predict the severity of the crashes in the testing data. Compare the performance of the two logistic models in terms of accuracy, precision, recall, F1-score, and AUC.\n\nMidterm project: Noise complaints in NYC The NYC Open Data of 311 Service Requests contains all requests from 2010 to present. We consider a subset of it with requests to NYPD on noise complaints that are created between 00:00:00 06/30/2024 and 24:00:00 07/06/2024. The subset is available in CSV format as data/nypd311w063024noise_by100724.csv. Read the data dictionary online to understand the meaning of the variables.\n\nData cleaning.\n\nImport the data, rename the columns with our preferred styles.\nSummarize the missing information. Are there variables that are close to completely missing?\nAre there redundant information in the data? Try storing the data using the Arrow format and comment on the efficiency gain.\nAre there invalid NYC zipcode or borough? Justify and clean them if yes.\nAre there date errors? Examples are earlier closed_date than created_date; closed_date and created_date matching to the second; dates exactly at midnight or noon to the second; action_update_date after closed_date.\nSummarize your suggestions to the data curator in several bullet points.\n\nData exploration.\n\nIf we suspect that response time may depend on the time of day when a complaint is made, we can compare the response times for complaints submitted during nighttime and daytime. To do this, we can visualize the comparison by complaint type, borough, and weekday (vs weekend/holiday).\nPerform a formal hypothesis test to confirm the observations from your visualization. Formally state your hypotheses and summarize your conclusions in plain English.\nCreate a binary variable over2h to indicate that a service request took two hours or longer to close.\nDoes over2h depend on the complaint type, borough, or weekday (vs weekend/holiday)? State your hypotheses and summarize your conclusions in plain English.\n\nData analysis.\n\nThe addresses of NYC police precincts are stored in data/nypd_precincts.csv. Use geocoding tools to find their geocode (longitude and latitude) from the addresses.\nCreate a variable dist2pp which represent the distance from each request incidence to the nearest police precinct.\nCreate zip code level variables by merging with data from package uszipcode.\nRandomly select 20% of the complaints as testing data with seeds 1234. Build a logistic model to predict over2h for the noise complaints with the training data, using all the variables you can engineer from the available data. If you have tuning parameters, justify how they were selected.\nAssess the performance of your model in terms of commonly used metrics. Summarize your results to a New Yorker who is not data science savvy.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Exercises</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Agonafir, C., Lakhankar, T., Khanbilvardi, R., Krakauer, N., Radell, D.,\n& Devineni, N. (2022). A machine learning approach to evaluate the\nspatial variability of New York\nCity’s 311 street flooding complaints. Computers,\nEnvironment and Urban Systems, 97, 101854.\n\n\nAgonafir, C., Pabon, A. R., Lakhankar, T., Khanbilvardi, R., &\nDevineni, N. (2022). Understanding New York\nCity street flooding through 311 complaints. Journal of\nHydrology, 605, 127300.\n\n\n(ASA), A. S. A. (2018). Ethical guidelines for statistical\npractice.\n\n\nComputing Machinery (ACM), A. for. (2018). Code of ethics and\nprofessional conduct.\n\n\nCongress, U. S. (1990). Americans with disabilities act of 1990\n(ADA).\n\n\nHealth, U. S. D. of, & Services, H. (1996). Health insurance\nportability and accountability act of 1996 (HIPAA).\n\n\nProtection of Human Subjects of Biomedical, N. C. for the, &\nResearch, B. (1979). The belmont report: Ethical principles and\nguidelines for the protection of human subjects of research.\n\n\nTeam, F. D. S. D. (2019). Federal data strategy 2020 action\nplan.\n\n\nVanderPlas, J. (2016). Python data science handbook:\nEssential tools for working with data. O’Reilly Media,\nInc.",
    "crumbs": [
      "References"
    ]
  }
]