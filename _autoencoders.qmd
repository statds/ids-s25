### Autoencoders

This section was written by Kyle Reed, a senior at the University of 
Connecticut double majoring in Applied Data Analysis and Geographic 
Information Science.

This section will explore:

- What an Autoencoder is

- How an Autoencoder works

- Potential applications of Autoencoders

#### Introduction

Autoencoders are a specialized type of neural network used in unsupervised 
learning. They are trained to encode input data into a compressed 
representation and then decode it back to something as close as possible to the 
original. This process forces the model to learn the most important features or 
patterns in the data.

Unlike traditional supervised learning models, autoencoders do not require 
labeled data. Instead, the model learns from the data itself by minimizing the 
difference between the original input and the reconstructed output.

#### How it works

An autoencoder consists of two primary components:

- Encoder: Deconstructs the input data into a lower-dimensional 
representation.

- Decoder: Reconstructs the original input from the compressed encoding.

##### Encoder

The encoder compresses data into smaller lower-dimensional groupings. It learns 
the data and identifies the most essential features of the data while 
discarding redundant information.

##### Decoder

The decoder reconstructs the data set from the compressed analyzed data 
generated by the encoder. The decoder attempts to reproduce the data as 
closely as possible by reversing the compression process.


#### Application

Autoencoders can be used for:

- Data Compression
  Reduce dataset size for storage and transmission while retaining key 
  information.

- Anomaly Detection 
  Identify unusual patterns that differ from the learned norm based on 
  reconstruction error.

- Image/Audio Refining  
  Remove noise, fill missing pixels or sound samples, colorize images, and more.

- Data Refining/Denoising  
  Improve dataset quality by correcting errors and filling missing values.

#### Example usage

For the example, I wish show how autoencoders can be used to compress data and 
refine data/images.
The mnist data set will be used which is a collection of various numbers 
that are drawn out on a small pixel image.

##### Load and Prepare Data

```{python}
#| echo: true

#Import necessary packages
from tensorflow.keras.datasets import mnist
from tensorflow.keras import layers, models
import numpy as np
import matplotlib.pyplot as plt

#Load data from dataset
(X_train, _), (X_test, _) = mnist.load_data()

#Ensure proper format and divide by 255 to normalize data
X_train = X_train.astype("float32") / 255.
X_test = X_test.astype("float32") / 255.

#Reshape the image to make it one-dimensional
X_train = X_train.reshape((len(X_train), -1))
X_test = X_test.reshape((len(X_test), -1))
```

Here I used Tensorflow Keras package which is one of the most common and easy 
to use frameworks people use for building and training autoencoders.

Data is broken into train and test data, and converted to floating-point 
format. Also, the data is divided by 255 as that is the maximum value for the
grey scale.

Reshaping images as seen in the last two lines allows to neural network to
compress the data easier when it is one-dimensional.

##### Build The Encoder

```{python}
#| echo: true

#Set the input size to the number of pixels for each image
input_dim = X_train.shape[1]

#Create and define elements of the autoencoder
autoencoder = models.Sequential([
    layers.Input(shape=(input_dim,)),
    layers.Dense(256, activation='relu'),  #Initial encoding
    layers.Dense(128, activation='relu'),   #Compressed version (Bottleneck)
    layers.Dense(256, activation='relu'),  #Reconstruction
    layers.Dense(input_dim, activation='sigmoid')  #Final reconstructed version
])
```


For each of the activation lines, the number represents the number of neurons 
for each layer, so '256' means that this layer transforms the input into a 
256-dimensional representation

When choosing the number of neurons, you want to pick a number that fits your 
data well. More neurons are needed for larger complex models, but they are not 
necessary for smaller, less complex models as this could cause overfitting.

##### Train the encoder

```{python}
#| echo: true

#Prepare the autoencoder with the optomizer 
autoencoder.compile(optimizer='adam', loss='binary_crossentropy')

#Train the autoencoder
autoencoder.fit(X_train, X_train,
                epochs=40,
                batch_size=256,
                shuffle=True,
                validation_data=(X_test, X_test))
```

For the model preparation I used “Adam,” which stands for Adaptive Moment 
Estimation. This is a very popular optimizer for autoencoders that works well 
with noisy or large data sets. binary_crossentropy calculates the difference
between the original and reconstructed image.

For training: 'epochs' = number of full passes through the data,'batch_size' = 
number of samples per training step, 'shuffle' = True, randomly shuffles the
data each epoch to help construct the data.

##### Reconstruct The Data

```{python}
#| echo: true

#Reconstruct the images
reconstruct_images = autoencoder.predict(X_test)

#Set the parameters for the plot with original and reconstructed images
n = 10
plt.figure(figsize=(16, 4))
for i in range(n):
    #Top plot for original images
    ax = plt.subplot(2, n, i + 1)
    plt.imshow(X_test[i].reshape(28, 28), cmap="gray")
    plt.title("Original")
    plt.axis("off")
    
    #Bottom plot for reconstructed images
    ax = plt.subplot(2, n, i + 1 + n)
    plt.imshow(reconstruct_images[i].reshape(28, 28), cmap="gray")
    plt.title("Reconstructed")
    plt.axis("off")
plt.show()
```

As we can see above the reconstructed images are slightly more pixelated and 
not as distict as the original. In order to improve on this the number of 
epochs or creating more dimensions when building the model.

#### Conclusion

- Autoencoders compress and reconstruct data, enabling pattern recognition 
without labeled data.

- Useful in tasks like anomaly detection, data cleaning, and feature extraction.

- Training involves minimizing reconstruction error, using loss functions such 
as MSE.

#### Further Readings

- Doersch, C. (2016). Tutorial on variational autoencoders. arXiv:1606.05908.

- Michelucci, U. (2022). Applied Deep Learning with TensorFlow 2, Apress.
