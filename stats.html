<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.39">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>7&nbsp; Statistical Tests and Models – Introduction to Data Science</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./machinelearning.html" rel="next">
<link href="./visualization.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-e26003cea8cd680ca0c55a263523d882.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-626149efe8f5d16e1d391ba177679bf0.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./stats.html"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Statistical Tests and Models</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Introduction to Data Science</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preliminaries</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./git.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Project Management</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./quarto.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Reproducible Data Science</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./python.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Python Refreshment</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./manipulation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Data Manipulation</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./visualization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Data Visualization</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./stats.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Statistical Tests and Models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./machinelearning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Machine Learning: Overview</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./supervised.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Supervised Learning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./unsupervised.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Unsupervised Learning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./advanced.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Advanced Topics</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./exercises.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Exercises</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#an-overview-of-statistical-testing-in-python" id="toc-an-overview-of-statistical-testing-in-python" class="nav-link active" data-scroll-target="#an-overview-of-statistical-testing-in-python"><span class="header-section-number">7.1</span> An Overview of Statistical Testing in Python</a>
  <ul class="collapse">
  <li><a href="#introduction" id="toc-introduction" class="nav-link" data-scroll-target="#introduction"><span class="header-section-number">7.1.1</span> Introduction</a></li>
  <li><a href="#whats-statistical-testing" id="toc-whats-statistical-testing" class="nav-link" data-scroll-target="#whats-statistical-testing"><span class="header-section-number">7.1.2</span> What’s Statistical Testing?</a></li>
  <li><a href="#null-and-alternate-hypotheses" id="toc-null-and-alternate-hypotheses" class="nav-link" data-scroll-target="#null-and-alternate-hypotheses"><span class="header-section-number">7.1.3</span> Null and Alternate Hypotheses</a></li>
  <li><a href="#assumptions" id="toc-assumptions" class="nav-link" data-scroll-target="#assumptions"><span class="header-section-number">7.1.4</span> Assumptions</a></li>
  <li><a href="#parametric-vs.-non-parametric" id="toc-parametric-vs.-non-parametric" class="nav-link" data-scroll-target="#parametric-vs.-non-parametric"><span class="header-section-number">7.1.5</span> Parametric vs.&nbsp;Non-Parametric</a></li>
  <li><a href="#types-of-statistical-testing" id="toc-types-of-statistical-testing" class="nav-link" data-scroll-target="#types-of-statistical-testing"><span class="header-section-number">7.1.6</span> Types of Statistical Testing</a></li>
  <li><a href="#statistical-vs.-practical-significance" id="toc-statistical-vs.-practical-significance" class="nav-link" data-scroll-target="#statistical-vs.-practical-significance"><span class="header-section-number">7.1.7</span> Statistical vs.&nbsp;Practical Significance</a></li>
  <li><a href="#best-practices" id="toc-best-practices" class="nav-link" data-scroll-target="#best-practices"><span class="header-section-number">7.1.8</span> Best Practices</a></li>
  <li><a href="#further-reading" id="toc-further-reading" class="nav-link" data-scroll-target="#further-reading"><span class="header-section-number">7.1.9</span> Further Reading</a></li>
  </ul></li>
  <li><a href="#tests-for-exploratory-data-analysis" id="toc-tests-for-exploratory-data-analysis" class="nav-link" data-scroll-target="#tests-for-exploratory-data-analysis"><span class="header-section-number">7.2</span> Tests for Exploratory Data Analysis</a></li>
  <li><a href="#statistical-modeling" id="toc-statistical-modeling" class="nav-link" data-scroll-target="#statistical-modeling"><span class="header-section-number">7.3</span> Statistical Modeling</a>
  <ul class="collapse">
  <li><a href="#installation-of-statsmodels" id="toc-installation-of-statsmodels" class="nav-link" data-scroll-target="#installation-of-statsmodels"><span class="header-section-number">7.3.1</span> Installation of <code>statsmodels</code></a></li>
  <li><a href="#linear-model" id="toc-linear-model" class="nav-link" data-scroll-target="#linear-model"><span class="header-section-number">7.3.2</span> Linear Model</a></li>
  <li><a href="#generalized-linear-regression" id="toc-generalized-linear-regression" class="nav-link" data-scroll-target="#generalized-linear-regression"><span class="header-section-number">7.3.3</span> Generalized Linear Regression</a></li>
  </ul></li>
  <li><a href="#logistic-regression" id="toc-logistic-regression" class="nav-link" data-scroll-target="#logistic-regression"><span class="header-section-number">7.4</span> Logistic Regression</a>
  <ul class="collapse">
  <li><a href="#interpreting-coefficients" id="toc-interpreting-coefficients" class="nav-link" data-scroll-target="#interpreting-coefficients"><span class="header-section-number">7.4.1</span> Interpreting Coefficients</a></li>
  <li><a href="#probabilistic-interpretation" id="toc-probabilistic-interpretation" class="nav-link" data-scroll-target="#probabilistic-interpretation"><span class="header-section-number">7.4.2</span> Probabilistic Interpretation</a></li>
  <li><a href="#evaluating-statistical-significance" id="toc-evaluating-statistical-significance" class="nav-link" data-scroll-target="#evaluating-statistical-significance"><span class="header-section-number">7.4.3</span> Evaluating Statistical Significance</a></li>
  <li><a href="#confusion-matrix" id="toc-confusion-matrix" class="nav-link" data-scroll-target="#confusion-matrix"><span class="header-section-number">7.4.4</span> Confusion Matrix</a></li>
  <li><a href="#accuracy" id="toc-accuracy" class="nav-link" data-scroll-target="#accuracy"><span class="header-section-number">7.4.5</span> Accuracy</a></li>
  <li><a href="#precision" id="toc-precision" class="nav-link" data-scroll-target="#precision"><span class="header-section-number">7.4.6</span> Precision</a></li>
  <li><a href="#recall" id="toc-recall" class="nav-link" data-scroll-target="#recall"><span class="header-section-number">7.4.7</span> Recall</a></li>
  <li><a href="#f-beta-score" id="toc-f-beta-score" class="nav-link" data-scroll-target="#f-beta-score"><span class="header-section-number">7.4.8</span> F-beta Score</a></li>
  <li><a href="#receiver-operating-characteristic-roc-curve" id="toc-receiver-operating-characteristic-roc-curve" class="nav-link" data-scroll-target="#receiver-operating-characteristic-roc-curve"><span class="header-section-number">7.4.9</span> Receiver Operating Characteristic (ROC) Curve</a></li>
  <li><a href="#demonstration" id="toc-demonstration" class="nav-link" data-scroll-target="#demonstration"><span class="header-section-number">7.4.10</span> Demonstration</a></li>
  </ul></li>
  <li><a href="#lasso-logistic-models" id="toc-lasso-logistic-models" class="nav-link" data-scroll-target="#lasso-logistic-models"><span class="header-section-number">7.5</span> LASSO Logistic Models</a>
  <ul class="collapse">
  <li><a href="#theoretical-formulation-of-the-problem" id="toc-theoretical-formulation-of-the-problem" class="nav-link" data-scroll-target="#theoretical-formulation-of-the-problem"><span class="header-section-number">7.5.1</span> Theoretical Formulation of the Problem</a></li>
  <li><a href="#solution-path" id="toc-solution-path" class="nav-link" data-scroll-target="#solution-path"><span class="header-section-number">7.5.2</span> Solution Path</a></li>
  <li><a href="#selection-the-tuning-parameter" id="toc-selection-the-tuning-parameter" class="nav-link" data-scroll-target="#selection-the-tuning-parameter"><span class="header-section-number">7.5.3</span> Selection the Tuning Parameter</a></li>
  <li><a href="#preparing-for-logistic-regression-fitting" id="toc-preparing-for-logistic-regression-fitting" class="nav-link" data-scroll-target="#preparing-for-logistic-regression-fitting"><span class="header-section-number">7.5.4</span> Preparing for Logistic Regression Fitting</a></li>
  </ul></li>
  <li><a href="#count-data-modeling" id="toc-count-data-modeling" class="nav-link" data-scroll-target="#count-data-modeling"><span class="header-section-number">7.6</span> Count Data Modeling</a>
  <ul class="collapse">
  <li><a href="#poisson-regression" id="toc-poisson-regression" class="nav-link" data-scroll-target="#poisson-regression"><span class="header-section-number">7.6.1</span> Poisson Regression</a></li>
  <li><a href="#negative-binomial-regression" id="toc-negative-binomial-regression" class="nav-link" data-scroll-target="#negative-binomial-regression"><span class="header-section-number">7.6.2</span> Negative Binomial Regression</a></li>
  <li><a href="#model-diagnosis" id="toc-model-diagnosis" class="nav-link" data-scroll-target="#model-diagnosis"><span class="header-section-number">7.6.3</span> Model Diagnosis</a></li>
  </ul></li>
  <li><a href="#an-example-with-nyc-street-flood" id="toc-an-example-with-nyc-street-flood" class="nav-link" data-scroll-target="#an-example-with-nyc-street-flood"><span class="header-section-number">7.7</span> An Example with NYC Street Flood</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Statistical Tests and Models</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="an-overview-of-statistical-testing-in-python" class="level2" data-number="7.1">
<h2 data-number="7.1" class="anchored" data-anchor-id="an-overview-of-statistical-testing-in-python"><span class="header-section-number">7.1</span> An Overview of Statistical Testing in Python</h2>
<section id="introduction" class="level3" data-number="7.1.1">
<h3 data-number="7.1.1" class="anchored" data-anchor-id="introduction"><span class="header-section-number">7.1.1</span> Introduction</h3>
<p>This section was written by John Ackerman, a sophomore at the University of Connecticut double majoring in mathematics and statistical data science.</p>
<p>By the end of this section, you’ll understand how to approach problems of the following forms using Python:</p>
<ul>
<li>Does the average height of male and female university students differ significantly?<br>
</li>
<li>Is there a relationship between the type of movie a person sees and whether or not they buy snacks at the theater?<br>
</li>
<li>Can we predict whether a patient has a disease based on their temperature and gender?<br>
</li>
<li>How can a company estimate customer satisfaction with a small sample of feedback?</li>
</ul>
</section>
<section id="whats-statistical-testing" class="level3" data-number="7.1.2">
<h3 data-number="7.1.2" class="anchored" data-anchor-id="whats-statistical-testing"><span class="header-section-number">7.1.2</span> What’s Statistical Testing?</h3>
<p>Statistical testing is a method of making probabilistic decisions about a population parameter based on sample data. It involves formulating hypotheses and using data to assess the evidence against a null hypothesis.</p>
</section>
<section id="null-and-alternate-hypotheses" class="level3" data-number="7.1.3">
<h3 data-number="7.1.3" class="anchored" data-anchor-id="null-and-alternate-hypotheses"><span class="header-section-number">7.1.3</span> Null and Alternate Hypotheses</h3>
<p>The null hypothesis (<span class="math inline">\(H_o\)</span>) is the default assumption that there is no effect, no difference, or no relationship in the data. The alternative hypothesis (<span class="math inline">\(H_1\)</span> or <span class="math inline">\(H_a\)</span>) is what we seek to provide evidence for-it suggests that there is an effect, a difference, or a relationship. Hypothesis testing evaluates whether the observed data provides enough evidence to reject the null hypothesis in favor of the alternative.</p>
<p>The following table contains the null and alternative hypotheses for some common tests:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 9%">
<col style="width: 34%">
<col style="width: 55%">
</colgroup>
<thead>
<tr class="header">
<th>Test</th>
<th>Null Hypothesis (H₀)</th>
<th>Alternative Hypothesis (H₁ or Hₐ)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>t-test (one-sample or two-sample)</td>
<td>The population mean(s) are equal.</td>
<td>The population mean(s) are different.</td>
</tr>
<tr class="even">
<td>Chi-square test for independence</td>
<td>Two categorical variables are independent.</td>
<td>Two categorical variables are dependent.</td>
</tr>
<tr class="odd">
<td>ANOVA (Analysis of Variance)</td>
<td>All group means are equal.</td>
<td>At least one group mean is different.</td>
</tr>
<tr class="even">
<td>Linear Regression (F-test)</td>
<td>The independent variable(s) have no effect on the dependent variable.</td>
<td>At least one independent variable significantly affects the dependent variable.</td>
</tr>
<tr class="odd">
<td>Mann-Whitney U test</td>
<td>The distributions of the two groups are the same.</td>
<td>The distributions of the two groups are different.</td>
</tr>
</tbody>
</table>
</section>
<section id="assumptions" class="level3" data-number="7.1.4">
<h3 data-number="7.1.4" class="anchored" data-anchor-id="assumptions"><span class="header-section-number">7.1.4</span> Assumptions</h3>
<p>Statistical tests are based on certain assumptions about the data, such as the distribution, variance, or independence of observations. Violating assumptions increases your risk of Type I errors (falsely rejecting a true null hypothesis) and Type II errors (failing to reject a false null hypothesis).</p>
<table class="caption-top table">
<colgroup>
<col style="width: 26%">
<col style="width: 34%">
<col style="width: 39%">
</colgroup>
<thead>
<tr class="header">
<th>Assumption</th>
<th>Method to Test</th>
<th>Package:Function</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Normality: (data is normally distributed)</td>
<td>Shapiro-Wilk test or Q-Q plot</td>
<td><code>scipy.stats.shapiro</code> (Shapiro-Wilk Test), <code>statsmodels.api.qqplot</code> (Q-Q Plot)</td>
</tr>
<tr class="even">
<td>Independence: (observations are independent)</td>
<td>Durbin-Watson test (for time series)</td>
<td><code>statsmodels.stats.stattools.durbin_watson</code> (Durbin-Watson Test)</td>
</tr>
<tr class="odd">
<td>Homoscedasticity: (equal variance across groups)</td>
<td>Levene’s test</td>
<td><code>scipy.stats.levene</code> (Levene’s Test)</td>
</tr>
<tr class="even">
<td>Heteroscedasticity: (unequal variance)</td>
<td>Breusch-Pagan test</td>
<td><code>statsmodels.stats.diagnostic.het_breuschpagan</code> (Breusch-Pagan Test)</td>
</tr>
<tr class="odd">
<td>No Outliers: (no extreme values)</td>
<td>IQR method or Z-score</td>
<td><code>scipy.stats.iqr</code> (Interquartile Range), <code>scipy.stats.zscore</code> (Z-score)</td>
</tr>
<tr class="even">
<td>No Multicollinearity: (independent predictors in regression)</td>
<td>Variance Inflation Factor (VIF)</td>
<td><code>statsmodels.stats.outliers_influence.variance_inflation_factor</code> (VIF)</td>
</tr>
</tbody>
</table>
</section>
<section id="parametric-vs.-non-parametric" class="level3" data-number="7.1.5">
<h3 data-number="7.1.5" class="anchored" data-anchor-id="parametric-vs.-non-parametric"><span class="header-section-number">7.1.5</span> Parametric vs.&nbsp;Non-Parametric</h3>
<p>Parametric tests assume that the data follows a specific distribution (typically normal), and rely on parameters such as the mean and standard deviation. Non-parametric tests don’t assume any distribution and as such are used when parametric assumptions are violated. Both are important because parametric tests offer greater precision if assumptions hold, while non-parametric tests provide a robust alternative when they don’t.</p>
<p>Let’s run a series of simulations to compare the accuracy of the independent t-test (parametric) vs the Wilcoxon test (non-parametric) with right-skewed samples with a known difference between groups:</p>
<div id="9fc177b5" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy <span class="im">import</span> stats</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Set random seed</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">124</span>)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulation parameters</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>n_simulations <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>sample_size <span class="op">=</span> <span class="dv">30</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Arrays to store p-values</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>t_test_pvalues <span class="op">=</span> []</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>wilcoxon_pvalues <span class="op">=</span> []</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Run simulations</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(n_simulations):</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Generate skewed data with a true difference</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>    group1 <span class="op">=</span> np.random.lognormal(mean<span class="op">=</span><span class="dv">0</span>, sigma<span class="op">=</span><span class="fl">0.8</span>, size<span class="op">=</span>sample_size)</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>    group2 <span class="op">=</span> np.random.lognormal(mean<span class="op">=</span><span class="fl">0.3</span>, sigma<span class="op">=</span><span class="fl">0.8</span>, size<span class="op">=</span>sample_size)  </span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Run both tests</span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>    _, t_pvalue <span class="op">=</span> stats.ttest_ind(group1, group2)</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>    _, w_pvalue <span class="op">=</span> stats.mannwhitneyu(group1, group2)</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Storing p-values</span></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>    t_test_pvalues.append(t_pvalue)</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>    wilcoxon_pvalues.append(w_pvalue)</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Find prop of results w/ (p &lt; 0.05)</span></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>t_test_significant <span class="op">=</span> (</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>  np.<span class="bu">sum</span>(np.array(t_test_pvalues) <span class="op">&lt;</span> <span class="fl">0.05</span>) <span class="op">/</span> n_simulations</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>wilcoxon_significant <span class="op">=</span> (</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>  np.<span class="bu">sum</span>(np.array(wilcoxon_pvalues) <span class="op">&lt;</span> <span class="fl">0.05</span>) <span class="op">/</span> n_simulations</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot histograms</span></span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">5</span>))</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>plt.hist(t_test_pvalues, bins<span class="op">=</span><span class="dv">20</span>, color<span class="op">=</span><span class="st">'blue'</span>)</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>plt.axvline(x<span class="op">=</span><span class="fl">0.05</span>, color<span class="op">=</span><span class="st">'red'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>)</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>plt.ylim(<span class="va">None</span>, <span class="dv">310</span>)</span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="ss">f't-test</span><span class="ch">\n</span><span class="ss">Detection Rate: </span><span class="sc">{</span>t_test_significant<span class="sc">:.2f}</span><span class="ss">'</span>)</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'p-value'</span>)</span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Frequency'</span>)</span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>plt.hist(wilcoxon_pvalues, bins<span class="op">=</span><span class="dv">20</span>, color<span class="op">=</span><span class="st">'green'</span>)</span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>plt.axvline(x<span class="op">=</span><span class="fl">0.05</span>, color<span class="op">=</span><span class="st">'red'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>)</span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a>plt.ylim(<span class="va">None</span>, <span class="dv">310</span>)</span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="ss">f'Wilcoxon Test</span><span class="ch">\n</span><span class="ss">Detection Rate: </span><span class="sc">{</span>wilcoxon_significant<span class="sc">:.2f}</span><span class="ss">'</span>)</span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'p-value'</span>)</span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(</span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a>  <span class="ss">f"Proportion of significant t-test results: "</span></span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a>  <span class="ss">f"</span><span class="sc">{</span>t_test_significant<span class="sc">:.2f}</span><span class="ss">"</span></span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(</span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a>  <span class="ss">f"Proportion of significant Wilcoxon results: "</span></span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a>  <span class="ss">f"</span><span class="sc">{</span>wilcoxon_significant<span class="sc">:.2f}</span><span class="ss">"</span></span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(</span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a>  <span class="ss">f"The non-parametric Wilcoxon test correctly identifies the difference "</span></span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a>  <span class="ss">f"</span><span class="sc">{</span>wilcoxon_significant<span class="op">/</span>t_test_significant<span class="sc">:.2f}</span><span class="ss">x more often"</span></span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="stats_files/figure-html/cell-2-output-1.png" width="758" height="471" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Proportion of significant t-test results: 0.23
Proportion of significant Wilcoxon results: 0.30
The non-parametric Wilcoxon test correctly identifies the difference 1.28x more often</code></pre>
</div>
</div>
<p>The Wilcoxon test correctly detected the difference more often because it didn’t assume the samples followed a normal distribution.</p>
</section>
<section id="types-of-statistical-testing" class="level3" data-number="7.1.6">
<h3 data-number="7.1.6" class="anchored" data-anchor-id="types-of-statistical-testing"><span class="header-section-number">7.1.6</span> Types of Statistical Testing</h3>
<section id="is-there-a-significant-difference-ab-testing" class="level4" data-number="7.1.6.1">
<h4 data-number="7.1.6.1" class="anchored" data-anchor-id="is-there-a-significant-difference-ab-testing"><span class="header-section-number">7.1.6.1</span> Is There a Significant Difference? (A/B Testing)</h4>
<p>These tests help determine whether the observed differences between groups are significant or just due to chance.</p>
<p>Ex: Does our new drug significantly lower blood pressure compared to placebo?</p>
<section id="overview-of-relevant-tests" class="level5" data-number="7.1.6.1.1">
<h5 data-number="7.1.6.1.1" class="anchored" data-anchor-id="overview-of-relevant-tests"><span class="header-section-number">7.1.6.1.1</span> Overview of Relevant Tests:</h5>
<table class="caption-top table">
<colgroup>
<col style="width: 11%">
<col style="width: 19%">
<col style="width: 33%">
<col style="width: 35%">
</colgroup>
<thead>
<tr class="header">
<th>Test</th>
<th>Use Case</th>
<th>Key Assumptions</th>
<th>Package:Function</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>t-test</td>
<td>Compare two means</td>
<td>Normality, equal variance (for independent test)</td>
<td><code>scipy.stats:ttest_ind</code></td>
</tr>
<tr class="even">
<td>Mann-Whitney U</td>
<td>Compare two groups (non-parametric)</td>
<td>Continuous data, different distributions</td>
<td><code>scipy.stats:mannwhitneyu</code></td>
</tr>
<tr class="odd">
<td>ANOVA</td>
<td>Compare 3+ groups</td>
<td>Normality, equal variance</td>
<td><code>scipy.stats:f_oneway</code></td>
</tr>
<tr class="even">
<td>Kruskal-Wallis</td>
<td>Compare 3+ groups (non-parametric)</td>
<td>Same shape distributions</td>
<td><code>scipy.stats:kruskal</code></td>
</tr>
</tbody>
</table>
</section>
<section id="t-test-mann-whitney-u-python-implementation" class="level5" data-number="7.1.6.1.2">
<h5 data-number="7.1.6.1.2" class="anchored" data-anchor-id="t-test-mann-whitney-u-python-implementation"><span class="header-section-number">7.1.6.1.2</span> t-test &amp; Mann-Whitney U Python Implementation:</h5>
<div id="a58cc6fd" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> ttest_ind, mannwhitneyu</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>group_a <span class="op">=</span> [<span class="dv">5</span>, <span class="dv">7</span>, <span class="dv">8</span>, <span class="dv">6</span>, <span class="dv">9</span>]</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>group_b <span class="op">=</span> [<span class="dv">6</span>, <span class="dv">8</span>, <span class="dv">7</span>, <span class="dv">10</span>, <span class="dv">12</span>]</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="co"># If data is normally distributed:</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>t_stat, p_value <span class="op">=</span> ttest_ind(group_a, group_b)</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"T-test: T=</span><span class="sc">{</span>t_stat<span class="sc">:.3f}</span><span class="ss">, p=</span><span class="sc">{</span>p_value<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="co"># If data is non-normal:</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>u_stat, p_value <span class="op">=</span> mannwhitneyu(group_a, group_b)</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Mann-Whitney U: U=</span><span class="sc">{</span>u_stat<span class="sc">:.3f}</span><span class="ss">, p=</span><span class="sc">{</span>p_value<span class="sc">:.3f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>T-test: T=-1.242, p=0.249
Mann-Whitney U: U=7.500, p=0.343</code></pre>
</div>
</div>
</section>
<section id="key-points" class="level5" data-number="7.1.6.1.3">
<h5 data-number="7.1.6.1.3" class="anchored" data-anchor-id="key-points"><span class="header-section-number">7.1.6.1.3</span> Key Points:</h5>
<p>When choosing statistical tests, follow these guidelines:</p>
<ul>
<li>Use a t-test if the data is normal; otherwise, use the Mann-Whitney U test.<br>
</li>
<li>For three or more groups, use ANOVA (or Kruskal-Wallis if non-parametric).<br>
</li>
<li>For paired and one-sample t-tests, use SciPy’s <code>ttest_rel</code> and <code>ttest_1samp</code>.</li>
</ul>
</section>
</section>
<section id="are-two-variables-related" class="level4" data-number="7.1.6.2">
<h4 data-number="7.1.6.2" class="anchored" data-anchor-id="are-two-variables-related"><span class="header-section-number">7.1.6.2</span> Are Two Variables Related?</h4>
<p>These tests assess whether two variables are correlated or if their relationship is just due to random variation.</p>
<p>Ex: Is there a correlation between hours studied and test performance?</p>
<section id="overview-of-relevant-tests-1" class="level5" data-number="7.1.6.2.1">
<h5 data-number="7.1.6.2.1" class="anchored" data-anchor-id="overview-of-relevant-tests-1"><span class="header-section-number">7.1.6.2.1</span> Overview of Relevant Tests:</h5>
<table class="caption-top table">
<colgroup>
<col style="width: 11%">
<col style="width: 19%">
<col style="width: 33%">
<col style="width: 35%">
</colgroup>
<thead>
<tr class="header">
<th>Test</th>
<th>Use Case</th>
<th>Key Assumptions</th>
<th>Package:Function</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Pearson correlation</td>
<td>Linear relationship between variables</td>
<td>Both variables are continuous and normally distributed</td>
<td><code>scipy.stats:pearsonr</code></td>
</tr>
<tr class="even">
<td>Spearman correlation</td>
<td>Monotonic relationship between variables</td>
<td>No normality assumption</td>
<td><code>scipy.stats:spearmanr</code></td>
</tr>
<tr class="odd">
<td>Chi-square test</td>
<td>Relationship between two categorical variables</td>
<td>Expected frequency &gt;5 in each category</td>
<td><code>scipy.stats:chi2_contingency</code></td>
</tr>
</tbody>
</table>
</section>
<section id="pearson-spearman-python-implementation" class="level5" data-number="7.1.6.2.2">
<h5 data-number="7.1.6.2.2" class="anchored" data-anchor-id="pearson-spearman-python-implementation"><span class="header-section-number">7.1.6.2.2</span> Pearson &amp; Spearman Python Implementation</h5>
<div id="ce86517a" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> pearsonr, spearmanr</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> [<span class="dv">10</span>, <span class="dv">20</span>, <span class="dv">30</span>, <span class="dv">40</span>, <span class="dv">50</span>]</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> [<span class="dv">15</span>, <span class="dv">25</span>, <span class="dv">35</span>, <span class="dv">45</span>, <span class="dv">60</span>]</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Pearson correlation</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>corr, p_value <span class="op">=</span> pearsonr(x, y)</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Pearson correlation: </span><span class="sc">{</span>corr<span class="sc">:.3f}</span><span class="ss">, p=</span><span class="sc">{</span>p_value<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Spearman correlation (if data is non-normal)</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>corr, p_value <span class="op">=</span> spearmanr(x, y)</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Spearman correlation: </span><span class="sc">{</span>corr<span class="sc">:.3f}</span><span class="ss">, p=</span><span class="sc">{</span>p_value<span class="sc">:.3f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Pearson correlation: 0.996, p=0.000
Spearman correlation: 1.000, p=0.000</code></pre>
</div>
</div>
</section>
<section id="key-points-1" class="level5" data-number="7.1.6.2.3">
<h5 data-number="7.1.6.2.3" class="anchored" data-anchor-id="key-points-1"><span class="header-section-number">7.1.6.2.3</span> Key Points</h5>
<p>For correlation analysis:</p>
<ul>
<li>Use Pearson’s correlation for linear relationships and Spearman’s correlation for monotonic ones.<br>
</li>
<li>Use the chi-square test when dealing with categorical data.<br>
</li>
<li>Remember that correlation <span class="math inline">\(\neq\)</span> causation.</li>
</ul>
</section>
</section>
<section id="can-we-predict-one-variable-from-another" class="level4" data-number="7.1.6.3">
<h4 data-number="7.1.6.3" class="anchored" data-anchor-id="can-we-predict-one-variable-from-another"><span class="header-section-number">7.1.6.3</span> Can We Predict One Variable from Another?</h4>
<p>These methods reveal how well one variable can predict another and quantify the strength of that relationship.</p>
<p>Ex: Can we predict a car’s fuel efficiency with its weight and horsepower?</p>
<section id="overview-of-relevant-tests-2" class="level5" data-number="7.1.6.3.1">
<h5 data-number="7.1.6.3.1" class="anchored" data-anchor-id="overview-of-relevant-tests-2"><span class="header-section-number">7.1.6.3.1</span> Overview of Relevant Tests:</h5>
<table class="caption-top table">
<colgroup>
<col style="width: 11%">
<col style="width: 19%">
<col style="width: 33%">
<col style="width: 35%">
</colgroup>
<thead>
<tr class="header">
<th>Test</th>
<th>Use Case</th>
<th>Key Assumptions</th>
<th>Package:Function</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Linear Regression</td>
<td>Predict a continuous outcome</td>
<td>Linearity, normal residuals</td>
<td><code>sklearn.linear_model:LinearRegression</code></td>
</tr>
<tr class="even">
<td>Logistic Regression</td>
<td>Predict a binary outcome</td>
<td>Linear relationship between predictors and log-odds</td>
<td><code>sklearn.linear_model:LogisticRegression</code></td>
</tr>
<tr class="odd">
<td>Chi-square goodness-of-fit</td>
<td>Compare observed vs.&nbsp;expected frequencies</td>
<td>Categories should have expected counts &gt;5</td>
<td><code>scipy.stats:chisquare</code></td>
</tr>
</tbody>
</table>
</section>
<section id="linear-regression-python-implementation" class="level5" data-number="7.1.6.3.2">
<h5 data-number="7.1.6.3.2" class="anchored" data-anchor-id="linear-regression-python-implementation"><span class="header-section-number">7.1.6.3.2</span> Linear Regression Python Implementation:</h5>
<div id="57548a7a" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LinearRegression</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Sample data</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>]).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.array([<span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">5</span>, <span class="dv">4</span>, <span class="dv">5</span>])</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit a simple linear regression model</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LinearRegression()</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>model.fit(X, y)</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>predictions <span class="op">=</span> model.predict(X)</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Predicted values: </span><span class="sc">{</span>predictions<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Predicted values: [2.8 3.4 4.  4.6 5.2]</code></pre>
</div>
</div>
</section>
<section id="key-points-2" class="level5" data-number="7.1.6.3.3">
<h5 data-number="7.1.6.3.3" class="anchored" data-anchor-id="key-points-2"><span class="header-section-number">7.1.6.3.3</span> Key Points:</h5>
<p>For regression modeling:</p>
<ul>
<li>Use linear regression for continuous outcomes and logistic regression for binary outcomes.<br>
</li>
<li>Consider feature scaling and transformations when using regression models.</li>
</ul>
</section>
</section>
<section id="is-this-result-significant-or-noise-alternative-inference" class="level4" data-number="7.1.6.4">
<h4 data-number="7.1.6.4" class="anchored" data-anchor-id="is-this-result-significant-or-noise-alternative-inference"><span class="header-section-number">7.1.6.4</span> Is This Result Significant or Noise? (Alternative Inference)</h4>
<p>These methods help distinguish meaningful patterns from random noise while providing estimates of the existence and magnitude of effects. They offer robust alternatives to traditional approaches by using simulation and resampling techniques, making them valuable when standard assumptions are violated.</p>
<p>Ex: Amazon is considering a new design for their checkout page and wants to know if the new design increases conversion rate and by how much.</p>
<section id="overview-of-relevant-tests-3" class="level5" data-number="7.1.6.4.1">
<h5 data-number="7.1.6.4.1" class="anchored" data-anchor-id="overview-of-relevant-tests-3"><span class="header-section-number">7.1.6.4.1</span> Overview of Relevant Tests</h5>
<table class="caption-top table">
<colgroup>
<col style="width: 17%">
<col style="width: 21%">
<col style="width: 21%">
<col style="width: 39%">
</colgroup>
<thead>
<tr class="header">
<th>Method</th>
<th>Use Case</th>
<th>Key Idea</th>
<th>Package:Function</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Bootstrap CI</td>
<td>Estimate confidence intervals from data</td>
<td>Resample data many times to see possible variations</td>
<td><code>numpy.random:choice</code> (custom implementation)</td>
</tr>
<tr class="even">
<td>Permutation Test</td>
<td>Test if two groups are meaningfully different</td>
<td>Shuffle labels and see how extreme the observed difference is</td>
<td><code>scipy.stats:permutation_test</code></td>
</tr>
<tr class="odd">
<td>Bayesian Inference</td>
<td>Quantify belief in different hypotheses</td>
<td>Treat parameters as probabilities and update based on data</td>
<td><code>pymc3</code> / <code>scipy.stats:beta</code> (for simple cases)</td>
</tr>
</tbody>
</table>
</section>
<section id="python-implementation-of-all-methods" class="level5" data-number="7.1.6.4.2">
<h5 data-number="7.1.6.4.2" class="anchored" data-anchor-id="python-implementation-of-all-methods"><span class="header-section-number">7.1.6.4.2</span> Python Implementation of all methods:</h5>
<div id="bbfd97fa" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> permutation_test, beta</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulated data: time spent on old vs. new layout</span></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>old_layout <span class="op">=</span> np.array([<span class="dv">40</span>, <span class="dv">50</span>, <span class="dv">45</span>, <span class="dv">55</span>, <span class="dv">42</span>, <span class="dv">48</span>])</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>new_layout <span class="op">=</span> np.array([<span class="dv">52</span>, <span class="dv">60</span>, <span class="dv">58</span>, <span class="dv">65</span>, <span class="dv">55</span>, <span class="dv">57</span>])</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Bootstrap CI</span></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>bootstrap_samples <span class="op">=</span> np.random.choice(</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>  new_layout <span class="op">-</span> old_layout,</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>  size<span class="op">=</span>(<span class="dv">10000</span>, <span class="bu">len</span>(old_layout)),</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>  replace<span class="op">=</span><span class="va">True</span></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>ci_lower, ci_upper <span class="op">=</span> np.percentile(</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>  bootstrap_samples.mean(axis<span class="op">=</span><span class="dv">1</span>), [<span class="fl">2.5</span>, <span class="fl">97.5</span>]</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Bootstrap 95% CI: [</span><span class="sc">{</span>ci_lower<span class="sc">:.3f}</span><span class="ss">, </span><span class="sc">{</span>ci_upper<span class="sc">:.3f}</span><span class="ss">]"</span>)</span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Permutation test</span></span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a>perm_result <span class="op">=</span> permutation_test(</span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>  (old_layout, new_layout),</span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>  statistic<span class="op">=</span><span class="kw">lambda</span> x, y: np.mean(x) <span class="op">-</span> np.mean(y),</span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a>  n_resamples<span class="op">=</span><span class="dv">10000</span></span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Permutation test p-value: </span><span class="sc">{</span>perm_result<span class="sc">.</span>pvalue<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Bayesian inference estimates probability that new layout increases time spent</span></span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a>alpha, beta_params <span class="op">=</span> <span class="dv">1</span> <span class="op">+</span> new_layout.<span class="bu">sum</span>(), <span class="dv">1</span> <span class="op">+</span> old_layout.<span class="bu">sum</span>()</span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a>bayes_prob <span class="op">=</span> <span class="dv">1</span> <span class="op">-</span> beta.cdf(<span class="dv">0</span>, alpha, beta_params)</span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Bayesian probability that new layout is better: </span><span class="sc">{</span>bayes_prob<span class="sc">:.3f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Bootstrap 95% CI: [10.000, 12.500]
Permutation test p-value: 0.006
Bayesian probability that new layout is better: 1.000</code></pre>
</div>
</div>
</section>
<section id="key-points-3" class="level5" data-number="7.1.6.4.3">
<h5 data-number="7.1.6.4.3" class="anchored" data-anchor-id="key-points-3"><span class="header-section-number">7.1.6.4.3</span> Key Points:</h5>
<p>Principle ideas for the three methods:</p>
<ul>
<li>Focus on effect sizes and uncertainty rather than just p-values.<br>
</li>
<li>Effect sizes often provide more actionable insights.<br>
</li>
<li>Bootstrap confidence intervals provide robust interval estimates without normality assumptions.<br>
</li>
<li>Permutation tests allow inference without relying on parametric models.<br>
</li>
<li>Bayesian methods offer probability-based conclusions rather than binary p-values.</li>
</ul>
</section>
</section>
</section>
<section id="statistical-vs.-practical-significance" class="level3" data-number="7.1.7">
<h3 data-number="7.1.7" class="anchored" data-anchor-id="statistical-vs.-practical-significance"><span class="header-section-number">7.1.7</span> Statistical vs.&nbsp;Practical Significance</h3>
<section id="multiple-testing" class="level4" data-number="7.1.7.1">
<h4 data-number="7.1.7.1" class="anchored" data-anchor-id="multiple-testing"><span class="header-section-number">7.1.7.1</span> Multiple Testing</h4>
<p>Multiple testing refers to performing several statistical tests on the same data or research question simultaneously, which increases the risk of false positives (Type I errors). For example, suppose the null hypothesis is true, and we use a significance level 0.05. With a single test, there’s a 5% chance of a Type I error. However, if we conduct 10 tests, the probability of encountering at least one Type I error rises to approximately 40%.</p>
<p>A common corrective measure is the Bonferroni correction, which adjusts the significance level as follows:</p>
<p><span class="math display">\[
\alpha_{\text{adjusted}} = \frac{\alpha}{m},
\]</span></p>
<p>where <span class="math inline">\(\alpha\)</span> is the significance level and <span class="math inline">\(m\)</span> is the number of analyses.</p>
</section>
<section id="effect-size" class="level4" data-number="7.1.7.2">
<h4 data-number="7.1.7.2" class="anchored" data-anchor-id="effect-size"><span class="header-section-number">7.1.7.2</span> Effect Size</h4>
<p>P-values only inform us whether an effect exists. Consider that large sample sizes can make small effects significant, and small samples might overlook large effects. Effect size quantifies the magnitude of a difference or relationship, providing context beyond statistical significance. One such measure for mean difference is Cohen’s d, which is defined as:</p>
<p><span class="math display">\[
d = \frac{\bar{X}_1 - \bar{X}_2}{s_p},
\]</span></p>
<p>where <span class="math inline">\(\bar{X}_1\)</span> and <span class="math inline">\(\bar{X}_2\)</span> are the sample means, and <span class="math inline">\(s_p\)</span> is the pooled standard deviation. The pooled standard deviation is calculated as:</p>
<p><span class="math display">\[
s_p = \sqrt{\frac{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}{n_1 + n_2 - 2}},
\]</span></p>
<p>where <span class="math inline">\(s_1\)</span> and <span class="math inline">\(s_2\)</span> are the standard deviations, and <span class="math inline">\(n_1\)</span> and <span class="math inline">\(n_2\)</span> are the sample sizes.</p>
<p>Other measures include:</p>
<ul>
<li>Correlation Coefficient: Measures how two continuous variables relate.<br>
</li>
<li>Eta-Squared: Used for ANOVA; quantifies the proportion of variance explained by a categorical factor.<br>
</li>
<li>Phi and Cramer’s V: Used for categorical data in chi-square tests.</li>
</ul>
</section>
</section>
<section id="best-practices" class="level3" data-number="7.1.8">
<h3 data-number="7.1.8" class="anchored" data-anchor-id="best-practices"><span class="header-section-number">7.1.8</span> Best Practices</h3>
<section id="assumption-violations" class="level4" data-number="7.1.8.1">
<h4 data-number="7.1.8.1" class="anchored" data-anchor-id="assumption-violations"><span class="header-section-number">7.1.8.1</span> Assumption Violations</h4>
<ul>
<li>Check assumptions before choosing a test rather than after obtaining results.<br>
</li>
<li>Use non-parametric alternatives when assumptions are violated and connot be easily corrected.<br>
</li>
<li>Try transforming the data to better meet assumptions.</li>
</ul>
</section>
<section id="interpretation-guidelines" class="level4" data-number="7.1.8.2">
<h4 data-number="7.1.8.2" class="anchored" data-anchor-id="interpretation-guidelines"><span class="header-section-number">7.1.8.2</span> Interpretation Guidelines</h4>
<ul>
<li>Use p-values alongside effect sizes and confidence intervals for a more complete analysis.</li>
<li>Be catious of multiple comparisons and adjust significance levels when necessary.</li>
</ul>
</section>
<section id="documentation" class="level4" data-number="7.1.8.3">
<h4 data-number="7.1.8.3" class="anchored" data-anchor-id="documentation"><span class="header-section-number">7.1.8.3</span> Documentation</h4>
<ul>
<li>Ensure code and data preprocessing steps are well-documented for reproducibility.</li>
<li>Report all relevant statistics.<br>
</li>
<li>Include visualizations when possible.</li>
</ul>
</section>
</section>
<section id="further-reading" class="level3" data-number="7.1.9">
<h3 data-number="7.1.9" class="anchored" data-anchor-id="further-reading"><span class="header-section-number">7.1.9</span> Further Reading</h3>
<ul>
<li><a href="https://docs.scipy.org/doc/scipy/reference/stats.html">scipy.stats</a></li>
<li><a href="https://www.statsmodels.org/stable/stats.html">statsmodels.stats</a></li>
<li><a href="https://scikit-learn.org/stable/model_selection.html">sklearn.userguide</a></li>
</ul>
</section>
</section>
<section id="tests-for-exploratory-data-analysis" class="level2" data-number="7.2">
<h2 data-number="7.2" class="anchored" data-anchor-id="tests-for-exploratory-data-analysis"><span class="header-section-number">7.2</span> Tests for Exploratory Data Analysis</h2>
<p>A collection of functions are available from <code>scipy.stats</code>.</p>
<ul>
<li>Comparing the locations of two samples
<ul>
<li><code>ttest_ind</code>: t-test for two independent samples</li>
<li><code>ttest_rel</code>: t-test for paired samples</li>
<li><code>ranksums</code>: Wilcoxon rank-sum test for two independent samples</li>
<li><code>wilcoxon</code>: Wilcoxon signed-rank test for paired samples</li>
</ul></li>
<li>Comparing the locations of multiple samples
<ul>
<li><code>f_oneway</code>: one-way ANOVA</li>
<li><code>kruskal</code>: Kruskal-Wallis H-test</li>
</ul></li>
<li>Tests for associations in contigency tables
<ul>
<li><code>chi2_contingency</code>: Chi-square test of independence of variables</li>
<li><code>fisher_exact</code>: Fisher exact test on a 2x2 contingency table</li>
</ul></li>
<li>Goodness of fit
<ul>
<li><code>goodness_of_fit</code>: distribution could contain unspecified parameters</li>
<li><code>anderson</code>: Anderson-Darling test</li>
<li><code>kstest</code>: Kolmogorov-Smirnov test</li>
<li><code>chisquare</code>: one-way chi-square test</li>
<li><code>normaltest</code>: test for normality</li>
</ul></li>
</ul>
<p>Since R has a richer collections of statistical functions, we can call R function from Python with <code>rpy2</code>. See, for example, a <a href="https://rviews.rstudio.com/2022/05/25/calling-r-from-python-with-rpy2/">blog on this subject</a>.</p>
<p>For example, <code>fisher_exact</code> can only handle 2x2 contingency tables. For contingency tables larger than 2x2, we can call <code>fisher.test()</code> from R through <code>rpy2</code>. See <a href="https://stackoverflow.com/questions/25368284/fishers-exact-test-for-bigger-than-2-by-2-contingency-table">this StackOverflow post</a>. Note that the <code>.</code> in function names and arguments are replaced with <code>_</code>.</p>
<div id="67b179b9" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> rpy2.robjects.numpy2ri</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> rpy2.robjects.packages <span class="im">import</span> importr</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>rpy2.robjects.numpy2ri.activate()</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>stats <span class="op">=</span> importr(<span class="st">'stats'</span>)</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>w0630 <span class="op">=</span> pd.read_feather(<span class="st">"data/nyccrashes_cleaned.feather"</span>)</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>w0630[<span class="st">"injury"</span>] <span class="op">=</span> np.where(w0630[<span class="st">"number_of_persons_injured"</span>] <span class="op">&gt;</span> <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>)</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> pd.crosstab(w0630[<span class="st">"injury"</span>], w0630[<span class="st">"borough"</span>])</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(m)</span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>res <span class="op">=</span> stats.fisher_test(m.to_numpy(), simulate_p_value <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(res)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Loading custom .Rprofileborough  BRONX  BROOKLYN  MANHATTAN  QUEENS  STATEN ISLAND
injury                                                    
0          149       345        164     249             65
1          129       266        127     227             28

    Fisher's Exact Test for Count Data with simulated p-value (based on
    2000 replicates)

data:  structure(c(149L, 129L, 345L, 266L, 164L, 127L, 249L, 227L, 65L, 28L), dim = c(2L, 5L))
p-value = 0.02949
alternative hypothesis: two.sided

</code></pre>
</div>
</div>
</section>
<section id="statistical-modeling" class="level2" data-number="7.3">
<h2 data-number="7.3" class="anchored" data-anchor-id="statistical-modeling"><span class="header-section-number">7.3</span> Statistical Modeling</h2>
<p>Statistical modeling is a cornerstone of data science, offering tools to understand complex relationships within data and to make predictions. Python, with its rich ecosystem for data analysis, features the <code>statsmodels</code> package— a comprehensive library designed for statistical modeling, tests, and data exploration. <code>statsmodels</code> stands out for its focus on classical statistical models and compatibility with the Python scientific stack (<code>numpy</code>, <code>scipy</code>, <code>pandas</code>).</p>
<section id="installation-of-statsmodels" class="level3" data-number="7.3.1">
<h3 data-number="7.3.1" class="anchored" data-anchor-id="installation-of-statsmodels"><span class="header-section-number">7.3.1</span> Installation of <code>statsmodels</code></h3>
<p>To start with statistical modeling, ensure <code>statsmodels</code> is installed:</p>
<p>Using pip:</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install statsmodels</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="linear-model" class="level3" data-number="7.3.2">
<h3 data-number="7.3.2" class="anchored" data-anchor-id="linear-model"><span class="header-section-number">7.3.2</span> Linear Model</h3>
<p>Let’s simulate some data for illustrations.</p>
<div id="1bafc06c" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>nobs <span class="op">=</span> <span class="dv">200</span></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>ncov <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">123</span>)</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.random.random((nobs, ncov)) <span class="co"># Uniform over [0, 1)</span></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>beta <span class="op">=</span> np.repeat(<span class="dv">1</span>, ncov)</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> <span class="dv">2</span> <span class="op">+</span> np.dot(x, beta) <span class="op">+</span> np.random.normal(size <span class="op">=</span> nobs)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Check the shape of <code>y</code>:</p>
<div id="29316d3a" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>y.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="8">
<pre><code>(200,)</code></pre>
</div>
</div>
<p>Check the shape of <code>x</code>:</p>
<div id="c6308269" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>x.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="9">
<pre><code>(200, 5)</code></pre>
</div>
</div>
<p>That is, the true linear regression model is <span class="math display">\[
y = 2 + x_1 + x_2 + x_3 + x_4 + x_5 + \epsilon.
\]</span></p>
<p>A regression model for the observed data can be fitted as</p>
<div id="bbdeb9b5" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> statsmodels.api <span class="im">as</span> sma</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>xmat <span class="op">=</span> sma.add_constant(x)</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>mymod <span class="op">=</span> sma.OLS(y, xmat)</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>myfit <span class="op">=</span> mymod.fit()</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>myfit.summary()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="10">
<table class="simpletable caption-top table table-sm table-striped small" data-quarto-postprocess="true">
<caption>OLS Regression Results</caption>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">Dep. Variable:</td>
<td>y</td>
<td data-quarto-table-cell-role="th">R-squared:</td>
<td>0.309</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">Model:</td>
<td>OLS</td>
<td data-quarto-table-cell-role="th">Adj. R-squared:</td>
<td>0.292</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">Method:</td>
<td>Least Squares</td>
<td data-quarto-table-cell-role="th">F-statistic:</td>
<td>17.38</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">Date:</td>
<td>Wed, 16 Apr 2025</td>
<td data-quarto-table-cell-role="th">Prob (F-statistic):</td>
<td>3.31e-14</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">Time:</td>
<td>09:26:43</td>
<td data-quarto-table-cell-role="th">Log-Likelihood:</td>
<td>-272.91</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">No. Observations:</td>
<td>200</td>
<td data-quarto-table-cell-role="th">AIC:</td>
<td>557.8</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">Df Residuals:</td>
<td>194</td>
<td data-quarto-table-cell-role="th">BIC:</td>
<td>577.6</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">Df Model:</td>
<td>5</td>
<td data-quarto-table-cell-role="th"></td>
<td></td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">Covariance Type:</td>
<td>nonrobust</td>
<td data-quarto-table-cell-role="th"></td>
<td></td>
</tr>
</tbody>
</table>


<table class="simpletable caption-top table table-sm table-striped small" data-quarto-postprocess="true">
<tbody>
<tr class="odd">
<td></td>
<td data-quarto-table-cell-role="th">coef</td>
<td data-quarto-table-cell-role="th">std err</td>
<td data-quarto-table-cell-role="th">t</td>
<td data-quarto-table-cell-role="th">P&gt;|t|</td>
<td data-quarto-table-cell-role="th">[0.025</td>
<td data-quarto-table-cell-role="th">0.975]</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">const</td>
<td>1.8754</td>
<td>0.282</td>
<td>6.656</td>
<td>0.000</td>
<td>1.320</td>
<td>2.431</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">x1</td>
<td>1.1703</td>
<td>0.248</td>
<td>4.723</td>
<td>0.000</td>
<td>0.682</td>
<td>1.659</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">x2</td>
<td>0.8988</td>
<td>0.235</td>
<td>3.825</td>
<td>0.000</td>
<td>0.435</td>
<td>1.362</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">x3</td>
<td>0.9784</td>
<td>0.238</td>
<td>4.114</td>
<td>0.000</td>
<td>0.509</td>
<td>1.448</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">x4</td>
<td>1.3418</td>
<td>0.250</td>
<td>5.367</td>
<td>0.000</td>
<td>0.849</td>
<td>1.835</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">x5</td>
<td>0.6027</td>
<td>0.239</td>
<td>2.519</td>
<td>0.013</td>
<td>0.131</td>
<td>1.075</td>
</tr>
</tbody>
</table>


<table class="simpletable caption-top table table-sm table-striped small" data-quarto-postprocess="true">
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">Omnibus:</td>
<td>0.810</td>
<td data-quarto-table-cell-role="th">Durbin-Watson:</td>
<td>1.978</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">Prob(Omnibus):</td>
<td>0.667</td>
<td data-quarto-table-cell-role="th">Jarque-Bera (JB):</td>
<td>0.903</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">Skew:</td>
<td>-0.144</td>
<td data-quarto-table-cell-role="th">Prob(JB):</td>
<td>0.637</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">Kurtosis:</td>
<td>2.839</td>
<td data-quarto-table-cell-role="th">Cond. No.</td>
<td>8.31</td>
</tr>
</tbody>
</table>
<br><br>Notes:<br>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
</div>
</div>
<p>Questions to review:</p>
<ul>
<li>How are the regression coefficients interpreted? Intercept?</li>
<li>Why does it make sense to center the covariates?</li>
</ul>
<p>Now we form a data frame with the variables</p>
<div id="e4fff972" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> np.concatenate((y.reshape((nobs, <span class="dv">1</span>)), x), axis <span class="op">=</span> <span class="dv">1</span>)</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame(data <span class="op">=</span> df,</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>                  columns <span class="op">=</span> [<span class="st">"y"</span>] <span class="op">+</span> [<span class="st">"x"</span> <span class="op">+</span> <span class="bu">str</span>(i) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>,</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>                  ncov <span class="op">+</span> <span class="dv">1</span>)])</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>df.info()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 200 entries, 0 to 199
Data columns (total 6 columns):
 #   Column  Non-Null Count  Dtype  
---  ------  --------------  -----  
 0   y       200 non-null    float64
 1   x1      200 non-null    float64
 2   x2      200 non-null    float64
 3   x3      200 non-null    float64
 4   x4      200 non-null    float64
 5   x5      200 non-null    float64
dtypes: float64(6)
memory usage: 9.5 KB</code></pre>
</div>
</div>
<p>Let’s use a formula to specify the regression model as in R, and fit a robust linear model (<code>rlm</code>) instead of OLS. Note that the model specification and the function interface is similar to R.</p>
<div id="6534f4e5" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> statsmodels.formula.api <span class="im">as</span> smf</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>mymod <span class="op">=</span> smf.rlm(formula <span class="op">=</span> <span class="st">"y ~ x1 + x2 + x3 + x4 + x5"</span>, data <span class="op">=</span> df)</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>myfit <span class="op">=</span> mymod.fit()</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>myfit.summary()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="12">
<table class="simpletable caption-top table table-sm table-striped small" data-quarto-postprocess="true">
<caption>Robust linear Model Regression Results</caption>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">Dep. Variable:</td>
<td>y</td>
<td data-quarto-table-cell-role="th">No. Observations:</td>
<td>200</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">Model:</td>
<td>RLM</td>
<td data-quarto-table-cell-role="th">Df Residuals:</td>
<td>194</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">Method:</td>
<td>IRLS</td>
<td data-quarto-table-cell-role="th">Df Model:</td>
<td>5</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">Norm:</td>
<td>HuberT</td>
<td data-quarto-table-cell-role="th"></td>
<td></td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">Scale Est.:</td>
<td>mad</td>
<td data-quarto-table-cell-role="th"></td>
<td></td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">Cov Type:</td>
<td>H1</td>
<td data-quarto-table-cell-role="th"></td>
<td></td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">Date:</td>
<td>Wed, 16 Apr 2025</td>
<td data-quarto-table-cell-role="th"></td>
<td></td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">Time:</td>
<td>09:26:43</td>
<td data-quarto-table-cell-role="th"></td>
<td></td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">No. Iterations:</td>
<td>16</td>
<td data-quarto-table-cell-role="th"></td>
<td></td>
</tr>
</tbody>
</table>


<table class="simpletable caption-top table table-sm table-striped small" data-quarto-postprocess="true">
<tbody>
<tr class="odd">
<td></td>
<td data-quarto-table-cell-role="th">coef</td>
<td data-quarto-table-cell-role="th">std err</td>
<td data-quarto-table-cell-role="th">z</td>
<td data-quarto-table-cell-role="th">P&gt;|z|</td>
<td data-quarto-table-cell-role="th">[0.025</td>
<td data-quarto-table-cell-role="th">0.975]</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">Intercept</td>
<td>1.8353</td>
<td>0.294</td>
<td>6.246</td>
<td>0.000</td>
<td>1.259</td>
<td>2.411</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">x1</td>
<td>1.1254</td>
<td>0.258</td>
<td>4.355</td>
<td>0.000</td>
<td>0.619</td>
<td>1.632</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">x2</td>
<td>0.9664</td>
<td>0.245</td>
<td>3.944</td>
<td>0.000</td>
<td>0.486</td>
<td>1.447</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">x3</td>
<td>0.9995</td>
<td>0.248</td>
<td>4.029</td>
<td>0.000</td>
<td>0.513</td>
<td>1.486</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">x4</td>
<td>1.3275</td>
<td>0.261</td>
<td>5.091</td>
<td>0.000</td>
<td>0.816</td>
<td>1.839</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">x5</td>
<td>0.6768</td>
<td>0.250</td>
<td>2.712</td>
<td>0.007</td>
<td>0.188</td>
<td>1.166</td>
</tr>
</tbody>
</table>
<br><br>If the model instance has been used for another fit with different fit parameters, then the fit options might not be the correct ones anymore .
</div>
</div>
<p>For model diagnostics, one can check residual plots.</p>
<div id="ba3b1b20" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>myOlsFit <span class="op">=</span> smf.ols(formula <span class="op">=</span> <span class="st">"y ~ x1 + x2 + x3 + x4 + x5"</span>, data <span class="op">=</span> df).fit()</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> plt.figure(figsize <span class="op">=</span> (<span class="dv">6</span>, <span class="dv">6</span>))</span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a><span class="co">## residual versus x1; can do the same for other covariates</span></span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> sma.graphics.plot_regress_exog(myOlsFit, <span class="st">'x1'</span>, fig<span class="op">=</span>fig)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="stats_files/figure-html/cell-14-output-1.png" width="566" height="570" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>See more on <a href="https://www.statsmodels.org/stable/stats.html#residual-diagnostics-and-specification-tests">residual diagnostics and specification tests</a>.</p>
</section>
<section id="generalized-linear-regression" class="level3" data-number="7.3.3">
<h3 data-number="7.3.3" class="anchored" data-anchor-id="generalized-linear-regression"><span class="header-section-number">7.3.3</span> Generalized Linear Regression</h3>
<p>A linear regression model cannot be applied to presence/absence or count data. Generalized Linear Models (GLM) extend the classical linear regression to accommodate such response variables, that follow distributions other than the normal distribution. GLMs consist of three main components:</p>
<ul>
<li>Random Component: This specifies the distribution of the response variable <span class="math inline">\(Y\)</span>. It is assumed to be from the exponential family of distributions, such as Binomial for binary data and Poisson for count data.</li>
<li>Systematic Component: This consists of the linear predictor, a linear combination of unknown parameters and explanatory variables. It is denoted as <span class="math inline">\(\eta = X\beta\)</span>, where <span class="math inline">\(X\)</span> represents the explanatory variables, and <span class="math inline">\(\beta\)</span> represents the coefficients.</li>
<li>Link Function: The link function, <span class="math inline">\(g\)</span>, provides the relationship between the linear predictor and the mean of the distribution function. For a GLM, the mean of <span class="math inline">\(Y\)</span> is related to the linear predictor through the link function as <span class="math inline">\(\mu = g^{-1}(\eta)\)</span>.</li>
</ul>
<p>GLMs adapt to various data types through the selection of appropriate link functions and probability distributions. Here, we outline four special cases of GLM: normal regression, logistic regression, Poisson regression, and gamma regression.</p>
<ul>
<li>Normal Regression (Linear Regression). In normal regression, the response variable has a normal distribution. The identity link function is typically used, making this case equivalent to classical linear regression.
<ul>
<li>Use Case: Modeling continuous data where residuals are normally distributed.</li>
<li>Link Function: Identity, <span class="math inline">\(g(\mu) = \mu\)</span>.</li>
<li>Distribution: Normal.</li>
</ul></li>
<li>Logistic Regression. Logistic regression is used for binary response variables. It employs the logit link function to model the probability that an observation falls into one of two categories.
<ul>
<li>Use Case: Binary outcomes (e.g., success/failure).</li>
<li>Link Function: Logit, <span class="math inline">\(g(\mu) = \log\frac{\mu}{1-\mu}\)</span>.</li>
<li>Distribution: Binomial.</li>
</ul></li>
<li>Poisson Regression. Poisson regression models count data using the Poisson distribution. It’s ideal for modeling the rate at which events occur.
<ul>
<li>Use Case: Count data, such as the number of occurrences of an event.</li>
<li>Link Function: Log, <span class="math inline">\(g(\mu) = \log(\mu)\)</span></li>
<li>Distribution: Poisson.</li>
</ul></li>
<li>Gamma Regression. Gamma regression is suited for modeling positive continuous variables, especially when data are skewed and variance increases with the mean.
<ul>
<li>Use Case: Positive continuous outcomes with non-constant variance.</li>
<li>Link Function: Inverse <span class="math inline">\(g(\mu) = \frac{1}{\mu}\)</span>.</li>
<li>Distribution: Gamma.</li>
</ul></li>
</ul>
<p>Each GLM variant addresses specific types of data and research questions, enabling precise modeling and inference based on the underlying data distribution. Prediction will need the inverse link function which transforms the linear predictor to the expectation of the outcome.</p>
<p>To demonstrate the validation of logistic regression models, we first create a simulated dataset with binary outcomes. This setup involves generating logistic probabilities and then drawing binary outcomes based on these probabilities.</p>
<div id="d6bbdf37" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> statsmodels.api <span class="im">as</span> sm</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Set seed for reproducibility</span></span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a DataFrame with random features named `simdat`</span></span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a>simdat <span class="op">=</span> pd.DataFrame(np.random.randn(<span class="dv">1000</span>, <span class="dv">5</span>), columns<span class="op">=</span>[<span class="st">'x1'</span>, <span class="st">'x2'</span>, <span class="st">'x3'</span>, <span class="st">'x4'</span>, <span class="st">'x5'</span>])</span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculating the linear combination of inputs plus an intercept</span></span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a>eta <span class="op">=</span> simdat.dot([<span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">0</span>, <span class="dv">0</span>]) <span class="op">-</span> <span class="dv">5</span></span>
<span id="cb24-13"><a href="#cb24-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-14"><a href="#cb24-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Applying the logistic function to get probabilities using statsmodels' logit link</span></span>
<span id="cb24-15"><a href="#cb24-15" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> sm.families.links.Logit().inverse(eta)</span>
<span id="cb24-16"><a href="#cb24-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-17"><a href="#cb24-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Generating binary outcomes based on these probabilities and adding them to `simdat`</span></span>
<span id="cb24-18"><a href="#cb24-18" aria-hidden="true" tabindex="-1"></a>simdat[<span class="st">'yb'</span>] <span class="op">=</span> np.random.binomial(<span class="dv">1</span>, p, p.size)</span>
<span id="cb24-19"><a href="#cb24-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-20"><a href="#cb24-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the first few rows of the dataframe</span></span>
<span id="cb24-21"><a href="#cb24-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(simdat.head())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>         x1        x2        x3        x4        x5  yb
0  0.496714 -0.138264  0.647689  1.523030 -0.234153   0
1 -0.234137  1.579213  0.767435 -0.469474  0.542560   0
2 -0.463418 -0.465730  0.241962 -1.913280 -1.724918   0
3 -0.562288 -1.012831  0.314247 -0.908024 -1.412304   0
4  1.465649 -0.225776  0.067528 -1.424748 -0.544383   0</code></pre>
</div>
</div>
<p>Fit a logistic regression for <code>y1b</code> with the formula interface.</p>
<div id="622ecd57" class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> statsmodels.formula.api <span class="im">as</span> smf</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Specify the model formula</span></span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>formula <span class="op">=</span> <span class="st">'yb ~ x1 + x2 + x3 + x4 + x5'</span></span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit the logistic regression model using glm and a formula</span></span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a>fit <span class="op">=</span> smf.glm(formula<span class="op">=</span>formula, data<span class="op">=</span>simdat, family<span class="op">=</span>sm.families.Binomial()).fit()</span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the summary of the model</span></span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(fit.summary())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>                 Generalized Linear Model Regression Results                  
==============================================================================
Dep. Variable:                     yb   No. Observations:                 1000
Model:                            GLM   Df Residuals:                      994
Model Family:                Binomial   Df Model:                            5
Link Function:                  Logit   Scale:                          1.0000
Method:                          IRLS   Log-Likelihood:                -136.44
Date:                Wed, 16 Apr 2025   Deviance:                       272.89
Time:                        09:26:45   Pearson chi2:                 1.09e+03
No. Iterations:                     8   Pseudo R-squ. (CS):             0.2793
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          z      P&gt;|z|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept     -5.4564      0.453    -12.049      0.000      -6.344      -4.569
x1             2.1544      0.244      8.822      0.000       1.676       2.633
x2             2.0781      0.225      9.234      0.000       1.637       2.519
x3             1.9260      0.237      8.125      0.000       1.461       2.391
x4            -0.1085      0.166     -0.652      0.514      -0.434       0.217
x5             0.2672      0.158      1.695      0.090      -0.042       0.576
==============================================================================</code></pre>
</div>
</div>
</section>
</section>
<section id="logistic-regression" class="level2" data-number="7.4">
<h2 data-number="7.4" class="anchored" data-anchor-id="logistic-regression"><span class="header-section-number">7.4</span> Logistic Regression</h2>
<p>Once a logistic regression model is fitted, interpreting its results is crucial for understanding how predictor variables influence the probability of the outcome. Logistic regression models the log-odds of the response variable as a linear function of the predictor variables. To ease the intrepretation, consider a logistic model with a single binary predictor (e.g., treatment indicator):</p>
<p><span class="math display">\[
\log\left(\frac{\mu}{1 - \mu}\right) = \beta_0 + \beta_1 X
\]</span></p>
<p>where <span class="math inline">\(\mu = E(Y \mid X)\)</span> represents the probability of the positive class, and <span class="math inline">\(\beta_1\)</span> is the estimated coefficient for the binary predictor <span class="math inline">\(X\)</span>.</p>
<section id="interpreting-coefficients" class="level3" data-number="7.4.1">
<h3 data-number="7.4.1" class="anchored" data-anchor-id="interpreting-coefficients"><span class="header-section-number">7.4.1</span> Interpreting Coefficients</h3>
<p>If <span class="math inline">\(X\)</span> is a binary variable (e.g., 0 for “No” and 1 for “Yes”), <span class="math inline">\(\beta_1\)</span> represents the difference in log-odds between the two groups. Exponentiating <span class="math inline">\(\beta_1\)</span> gives the odds ratio:</p>
<p><span class="math display">\[
\text{Odds Ratio} = \frac{\exp(\beta_0 + \beta_1)}{\exp(\beta_0)} = e^{\beta_1}.
\]</span></p>
<ul>
<li>If <span class="math inline">\(e^{\beta_1} &gt; 1\)</span>, the outcome is more likely when <span class="math inline">\(X = 1\)</span> than when <span class="math inline">\(X = 0\)</span>.</li>
<li>If <span class="math inline">\(e^{\beta_1} &lt; 1\)</span>, the outcome is less likely when <span class="math inline">\(X = 1\)</span>.</li>
<li>If <span class="math inline">\(e^{\beta_1} = 1\)</span>, there is no effect of <span class="math inline">\(X\)</span> on the odds of the outcome.</li>
</ul>
<p>Equivalently, <span class="math inline">\(\beta_1\)</span> is the log odds ratio between the two groups.</p>
<p>When there are multiple predictors, the intrepretation needs to state that all the other predictors are unchanged.</p>
<p>How would you intreprete the coefficient of a continuous predictor?</p>
</section>
<section id="probabilistic-interpretation" class="level3" data-number="7.4.2">
<h3 data-number="7.4.2" class="anchored" data-anchor-id="probabilistic-interpretation"><span class="header-section-number">7.4.2</span> Probabilistic Interpretation</h3>
<p>We can transform the linear predictor into a probability estimate using the inverse logit function:</p>
<p><span class="math display">\[
\Pr(Y=1 | X) = \frac{e^{\beta_0 + \beta_1 X}}{1 + e^{\beta_0 + \beta_1 X}}.
\]</span></p>
<p>This allows for a direct interpretation of how being in one category of <span class="math inline">\(X\)</span> influences the predicted probability of the outcome. By construction, this value is always in <span class="math inline">\((0, 1)\)</span>.</p>
</section>
<section id="evaluating-statistical-significance" class="level3" data-number="7.4.3">
<h3 data-number="7.4.3" class="anchored" data-anchor-id="evaluating-statistical-significance"><span class="header-section-number">7.4.3</span> Evaluating Statistical Significance</h3>
<p>The significance of <span class="math inline">\(\beta_1\)</span> is assessed using standard errors and p-values:</p>
<ul>
<li>A small p-value (e.g., &lt; 0.05) suggests that <span class="math inline">\(X\)</span> has a statistically significant effect on the outcome.</li>
<li>Confidence intervals for <span class="math inline">\(e^{\beta_1}\)</span> help understand the precision of odds ratio estimates.</li>
</ul>
</section>
<section id="confusion-matrix" class="level3" data-number="7.4.4">
<h3 data-number="7.4.4" class="anchored" data-anchor-id="confusion-matrix"><span class="header-section-number">7.4.4</span> Confusion Matrix</h3>
<p>Validating the performance of logistic regression models is crucial to assess their effectiveness and reliability. This section explores key metrics used to evaluate the performance of logistic regression models, starting with the confusion matrix, then moving on to accuracy, precision, recall, F1 score, and the area under the ROC curve (AUC). Using simulated data, we will demonstrate how to calculate and interpret these metrics using Python.</p>
<p>The <a href="https://en.wikipedia.org/wiki/Confusion_matrix">confusion matrix</a> is a fundamental tool used for calculating several other classification metrics. It is a table used to describe the performance of a classification model on a set of data for which the true values are known. The matrix displays the actual values against the predicted values, providing insight into the number of correct and incorrect predictions.</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Actual</th>
<th>Predicted Positive</th>
<th>Predicted Negative</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Actual Positive</td>
<td>True Positive (TP)</td>
<td>False Negative (FN)</td>
</tr>
<tr class="even">
<td>Actual Negative</td>
<td>False Positive (FP)</td>
<td>True Negative (TN)</td>
</tr>
</tbody>
</table>
<p>Four entries in the confusion matrix:</p>
<ul>
<li>True Positive (TP): The cases in which the model correctly predicted the positive class.</li>
<li>False Positive (FP): The cases in which the model incorrectly predicted the positive class (i.e., the model predicted positive, but the actual class was negative).</li>
<li>True Negative (TN): The cases in which the model correctly predicted the negative class.</li>
<li>False Negative (FN): The cases in which the model incorrectly predicted the negative class (i.e., the model predicted negative, but the actual class was positive).</li>
</ul>
<p>Four rates from the confusion matrix with actual (row) margins:</p>
<ul>
<li>True positive rate (TPR): TP / (TP + FN). Also known as sensitivity.</li>
<li>False negative rate (FNR): FN / (TP + FN). Also known as miss rate.</li>
<li>False positive rate (FPR): FP / (FP + TN). Also known as false alarm, fall-out.</li>
<li>True negative rate (TNR): TN / (FP + TN). Also known as specificity.</li>
</ul>
<p>Note that TPR and FPR do not add up to one. Neither do FNR and FPR.</p>
<ul>
<li>Positive predictive value (PPV): TP / (TP + FP). Also known as precision.</li>
<li>False discovery rate (FDR): FP / (TP + FP).</li>
<li>False omission rate (FOR): FN / (FN + TN).</li>
<li>Negative predictive value (NPV): TN / (FN + TN).</li>
</ul>
<p>Note that PPV and NP do not add up to one.</p>
</section>
<section id="accuracy" class="level3" data-number="7.4.5">
<h3 data-number="7.4.5" class="anchored" data-anchor-id="accuracy"><span class="header-section-number">7.4.5</span> Accuracy</h3>
<p>Accuracy measures the overall correctness of the model and is defined as the ratio of correct predictions (both positive and negative) to the total number of cases examined.</p>
<pre><code>  Accuracy = (TP + TN) / (TP + TN + FP + FN)</code></pre>
<ul>
<li>Imbalanced Classes: Accuracy can be misleading if there is a significant imbalance between the classes. For instance, in a dataset where 95% of the samples are of one class, a model that naively predicts the majority class for all instances will still achieve 95% accuracy, which does not reflect true predictive performance.</li>
<li>Misleading Interpretations: High overall accuracy might hide the fact that the model is performing poorly on a smaller, yet important, segment of the data.</li>
</ul>
</section>
<section id="precision" class="level3" data-number="7.4.6">
<h3 data-number="7.4.6" class="anchored" data-anchor-id="precision"><span class="header-section-number">7.4.6</span> Precision</h3>
<p>Precision (or PPV) measures the accuracy of positive predictions. It quantifies the number of correct positive predictions made.</p>
<pre><code>  Precision = TP / (TP + FP)</code></pre>
<ul>
<li>Neglect of False Negatives: Precision focuses solely on the positive class predictions. It does not take into account false negatives (instances where the actual class is positive but predicted as negative). This can be problematic in cases like disease screening where missing a positive case (disease present) could be dangerous.</li>
<li>Not a Standalone Metric: High precision alone does not indicate good model performance, especially if recall is low. This situation could mean the model is too conservative in predicting positives, thus missing out on a significant number of true positive instances.</li>
</ul>
</section>
<section id="recall" class="level3" data-number="7.4.7">
<h3 data-number="7.4.7" class="anchored" data-anchor-id="recall"><span class="header-section-number">7.4.7</span> Recall</h3>
<p>Recall (Sensitivity or TPR) measures the ability of a model to find all relevant cases (all actual positives).</p>
<pre><code>  Recall = TP / (TP + FN)</code></pre>
<ul>
<li>Neglect of False Positives: Recall does not consider false positives (instances where the actual class is negative but predicted as positive). High recall can be achieved at the expense of precision, leading to a large number of false positives which can be costly or undesirable in certain contexts, such as in spam detection.</li>
<li>Trade-off with Precision: Often, increasing recall decreases precision. This trade-off needs to be managed carefully, especially in contexts where both false positives and false negatives carry significant costs or risks.</li>
</ul>
</section>
<section id="f-beta-score" class="level3" data-number="7.4.8">
<h3 data-number="7.4.8" class="anchored" data-anchor-id="f-beta-score"><span class="header-section-number">7.4.8</span> F-beta Score</h3>
<p>The F-beta score is a weighted harmonic mean of precision and recall, taking into account a <span class="math inline">\(\beta\)</span> parameter such that recall is considered <span class="math inline">\(\beta\)</span> times as important as precision: <span class="math display">\[
(1 + \beta^2) \frac{\text{precision} \cdot \text{recall}}
{\beta^2 \text{precision} + \text{recall}}.
\]</span></p>
<p>See <a href="https://stats.stackexchange.com/questions/221997/why-f-beta-score-define-beta-like-that">stackexchange post</a> for the motivation of <span class="math inline">\(\beta^2\)</span> instead of just <span class="math inline">\(\beta\)</span>.</p>
<p>The F-beta score reaches its best value at 1 (perfect precision and recall) and worst at 0.</p>
<p>If reducing false negatives is more important (as might be the case in medical diagnostics where missing a positive diagnosis could be critical), you might choose a beta value greater than 1. If reducing false positives is more important (as in spam detection, where incorrectly classifying an email as spam could be inconvenient), a beta value less than 1 might be appropriate.</p>
<p>The F1 Score is a specific case of the F-beta score where beta is 1, giving equal weight to precision and recall. It is the harmonic mean of Precision and Recall and is a useful measure when you seek a balance between Precision and Recall and there is an uneven class distribution (large number of actual negatives).</p>
</section>
<section id="receiver-operating-characteristic-roc-curve" class="level3" data-number="7.4.9">
<h3 data-number="7.4.9" class="anchored" data-anchor-id="receiver-operating-characteristic-roc-curve"><span class="header-section-number">7.4.9</span> Receiver Operating Characteristic (ROC) Curve</h3>
<p>The Receiver Operating Characteristic (ROC) curve is a plot that illustrates the diagnostic ability of a binary classifier as its discrimination threshold is varied. It shows the trade-off between the TPR and FPR. The ROC plots TPR against FPR as the decision threshold is varied. It can be particularly useful in evaluating the performance of classifiers when the class distribution is imbalanced,</p>
<ul>
<li>Increasing from <span class="math inline">\((0, 0)\)</span> to <span class="math inline">\((1, 1)\)</span>.</li>
<li>Best classification passes <span class="math inline">\((0, 1)\)</span>.</li>
<li>Classification by random guess gives the 45-degree line.</li>
<li>Area between the ROC and the 45-degree line is the Gini coefficient, a measure of inequality.</li>
<li>Area under the curve (AUC) of ROC thus provides an important metric of classification results.</li>
</ul>
<p>The Area Under the ROC Curve (AUC) is a scalar value that summarizes the performance of a classifier. It measures the total area underneath the ROC curve, providing a single metric to compare models. The value of AUC ranges from 0 to 1:</p>
<ul>
<li>AUC = 1: A perfect classifier, which perfectly separates positive and negative classes.</li>
<li>AUC = 0.5: A classifier that performs no better than random chance.</li>
<li>AUC &lt; 0.5: A classifier performing worse than random.</li>
</ul>
<p>The AUC value provides insight into the model’s ability to discriminate between positive and negative classes across all possible threshold values.</p>
</section>
<section id="demonstration" class="level3" data-number="7.4.10">
<h3 data-number="7.4.10" class="anchored" data-anchor-id="demonstration"><span class="header-section-number">7.4.10</span> Demonstration</h3>
<p>Let’s apply these metrics to the <code>simdat</code> dataset to understand their practical implications. We will fit a logistic regression model, make predictions, and then compute accuracy, precision, and recall.</p>
<div id="54af5619" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> (</span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a>    accuracy_score, precision_score, recall_score, confusion_matrix,</span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a>    f1_score, roc_curve, auc</span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_classification</span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-11"><a href="#cb31-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate synthetic data</span></span>
<span id="cb31-12"><a href="#cb31-12" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> make_classification(n_samples<span class="op">=</span><span class="dv">1000</span>, n_features<span class="op">=</span><span class="dv">20</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb31-13"><a href="#cb31-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-14"><a href="#cb31-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the dataset into training and testing sets</span></span>
<span id="cb31-15"><a href="#cb31-15" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.25</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb31-16"><a href="#cb31-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-17"><a href="#cb31-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit the logistic regression model</span></span>
<span id="cb31-18"><a href="#cb31-18" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LogisticRegression()</span>
<span id="cb31-19"><a href="#cb31-19" aria-hidden="true" tabindex="-1"></a>model.fit(X_train, y_train)</span>
<span id="cb31-20"><a href="#cb31-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-21"><a href="#cb31-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Predict labels on the test set</span></span>
<span id="cb31-22"><a href="#cb31-22" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> model.predict(X_test)</span>
<span id="cb31-23"><a href="#cb31-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-24"><a href="#cb31-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Get predicted probabilities for ROC curve and AUC</span></span>
<span id="cb31-25"><a href="#cb31-25" aria-hidden="true" tabindex="-1"></a>y_scores <span class="op">=</span> model.predict_proba(X_test)[:, <span class="dv">1</span>]  <span class="co"># Probability for the positive class</span></span>
<span id="cb31-26"><a href="#cb31-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-27"><a href="#cb31-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute confusion matrix</span></span>
<span id="cb31-28"><a href="#cb31-28" aria-hidden="true" tabindex="-1"></a>cm <span class="op">=</span> confusion_matrix(y_test, y_pred)</span>
<span id="cb31-29"><a href="#cb31-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-30"><a href="#cb31-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate accuracy, precision, and recall</span></span>
<span id="cb31-31"><a href="#cb31-31" aria-hidden="true" tabindex="-1"></a>accuracy <span class="op">=</span> accuracy_score(y_test, y_pred)</span>
<span id="cb31-32"><a href="#cb31-32" aria-hidden="true" tabindex="-1"></a>precision <span class="op">=</span> precision_score(y_test, y_pred)</span>
<span id="cb31-33"><a href="#cb31-33" aria-hidden="true" tabindex="-1"></a>recall <span class="op">=</span> recall_score(y_test, y_pred)</span>
<span id="cb31-34"><a href="#cb31-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-35"><a href="#cb31-35" aria-hidden="true" tabindex="-1"></a><span class="co"># Print confusion matrix and metrics</span></span>
<span id="cb31-36"><a href="#cb31-36" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Confusion Matrix:</span><span class="ch">\n</span><span class="st">"</span>, cm)</span>
<span id="cb31-37"><a href="#cb31-37" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Accuracy: </span><span class="sc">{</span>accuracy<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb31-38"><a href="#cb31-38" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Precision: </span><span class="sc">{</span>precision<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb31-39"><a href="#cb31-39" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Recall: </span><span class="sc">{</span>recall<span class="sc">:.2f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Confusion Matrix:
 [[104  11]
 [ 26 109]]
Accuracy: 0.85
Precision: 0.91
Recall: 0.81</code></pre>
</div>
</div>
<p>By varying threshold, one can plot the whole ROC curve.</p>
<div id="f6617b69" class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute ROC curve and AUC</span></span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>fpr, tpr, thresholds <span class="op">=</span> roc_curve(y_test, y_scores)</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>roc_auc <span class="op">=</span> auc(fpr, tpr)</span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Print AUC</span></span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"AUC: </span><span class="sc">{</span>roc_auc<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot ROC curve</span></span>
<span id="cb33-9"><a href="#cb33-9" aria-hidden="true" tabindex="-1"></a>plt.figure()</span>
<span id="cb33-10"><a href="#cb33-10" aria-hidden="true" tabindex="-1"></a>plt.plot(fpr, tpr, color<span class="op">=</span><span class="st">'darkorange'</span>, lw<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="ss">f'ROC curve (AUC = </span><span class="sc">{</span>roc_auc<span class="sc">:.2f}</span><span class="ss">)'</span>)</span>
<span id="cb33-11"><a href="#cb33-11" aria-hidden="true" tabindex="-1"></a>plt.plot([<span class="dv">0</span>, <span class="dv">1</span>], [<span class="dv">0</span>, <span class="dv">1</span>], color<span class="op">=</span><span class="st">'navy'</span>, lw<span class="op">=</span><span class="dv">2</span>, linestyle<span class="op">=</span><span class="st">'--'</span>)  <span class="co"># Diagonal line (random classifier)</span></span>
<span id="cb33-12"><a href="#cb33-12" aria-hidden="true" tabindex="-1"></a>plt.xlim([<span class="fl">0.0</span>, <span class="fl">1.0</span>])</span>
<span id="cb33-13"><a href="#cb33-13" aria-hidden="true" tabindex="-1"></a>plt.ylim([<span class="fl">0.0</span>, <span class="fl">1.05</span>])</span>
<span id="cb33-14"><a href="#cb33-14" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'False Positive Rate'</span>)</span>
<span id="cb33-15"><a href="#cb33-15" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'True Positive Rate'</span>)</span>
<span id="cb33-16"><a href="#cb33-16" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Receiver Operating Characteristic (ROC) Curve'</span>)</span>
<span id="cb33-17"><a href="#cb33-17" aria-hidden="true" tabindex="-1"></a>plt.legend(loc<span class="op">=</span><span class="st">"lower right"</span>)</span>
<span id="cb33-18"><a href="#cb33-18" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>AUC: 0.92</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="stats_files/figure-html/cell-18-output-2.png" width="599" height="449" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>We could pick the best threshold that optmizes F1-score/</p>
<div id="190292e7" class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute F1 score for each threshold</span></span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>f1_scores <span class="op">=</span> []</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> thresh <span class="kw">in</span> thresholds:</span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a>    y_pred_thresh <span class="op">=</span> (y_scores <span class="op">&gt;=</span> thresh).astype(<span class="bu">int</span>)  <span class="co"># Apply threshold to get binary predictions</span></span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a>    f1 <span class="op">=</span> f1_score(y_test, y_pred_thresh)</span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a>    f1_scores.append(f1)</span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Find the best threshold (the one that maximizes F1 score)</span></span>
<span id="cb35-9"><a href="#cb35-9" aria-hidden="true" tabindex="-1"></a>best_thresh <span class="op">=</span> thresholds[np.argmax(f1_scores)]</span>
<span id="cb35-10"><a href="#cb35-10" aria-hidden="true" tabindex="-1"></a>best_f1 <span class="op">=</span> <span class="bu">max</span>(f1_scores)</span>
<span id="cb35-11"><a href="#cb35-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-12"><a href="#cb35-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the best threshold and corresponding F1 score</span></span>
<span id="cb35-13"><a href="#cb35-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Best threshold: </span><span class="sc">{</span>best_thresh<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb35-14"><a href="#cb35-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Best F1 score: </span><span class="sc">{</span>best_f1<span class="sc">:.2f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Best threshold: 0.3960
Best F1 score: 0.89</code></pre>
</div>
</div>
</section>
</section>
<section id="lasso-logistic-models" class="level2" data-number="7.5">
<h2 data-number="7.5" class="anchored" data-anchor-id="lasso-logistic-models"><span class="header-section-number">7.5</span> LASSO Logistic Models</h2>
<p>The Least Absolute Shrinkage and Selection Operator (LASSO) <span class="citation" data-cites="tibshirani1996regression">(<a href="references.html#ref-tibshirani1996regression" role="doc-biblioref">Tibshirani, 1996</a>)</span>, is a regression method that performs both variable selection and regularization. LASSO imposes an L1 penalty on the regression coefficients, which has the effect of shrinking some coefficients exactly to zero. This results in simpler, more interpretable models, especially in situations where the number of predictors exceeds the number of observations.</p>
<section id="theoretical-formulation-of-the-problem" class="level3" data-number="7.5.1">
<h3 data-number="7.5.1" class="anchored" data-anchor-id="theoretical-formulation-of-the-problem"><span class="header-section-number">7.5.1</span> Theoretical Formulation of the Problem</h3>
<p>The objective function for LASSO logistic regression can be expressed as,</p>
<p><span class="math display">\[
\min_{\beta}
\left\{ -\frac{1}{n} \sum_{i=1}^n \left[ y_i \log(\hat{p}_i) + (1 - y_i) \log(1 - \hat{p}_i) \right] + \lambda \sum_{j=1}^p |\beta_j| \right\}
\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(\hat{p}_i = \frac{1}{1 + e^{-X_i\beta}}\)</span> is the predicted probability for the <span class="math inline">\(i\)</span>-th sample.</li>
<li><span class="math inline">\(y_i\)</span> represents the actual class label (binary: 0 or 1).</li>
<li><span class="math inline">\(X_i\)</span> is the feature vector for the <span class="math inline">\(i\)</span>-th observation.</li>
<li><span class="math inline">\(\beta\)</span> is the vector of model coefficients (including the intercept).</li>
<li><span class="math inline">\(\lambda\)</span> is the regularization parameter that controls the trade-off between model fit and sparsity (higher <span class="math inline">\(\lambda\)</span>) encourages sparsity by shrinking more coefficients to zero).</li>
</ul>
<p>The lasso penalty encourages the sum of the absolute values of the coefficients to be small, effectively shrinking some coefficients to zero. This results in sparser solutions, simplifying the model and reducing variance without substantial increase in bias.</p>
<p>Practical benefits of LASSO:</p>
<ul>
<li>Dimensionality Reduction: LASSO is particularly useful when the number of features <span class="math inline">\(p\)</span> is large, potentially even larger than the number of observations <span class="math inline">\(n\)</span>, as it automatically reduces the number of features.</li>
<li>Preventing Overfitting: The L1 penalty helps prevent overfitting by constraining the model, especially when <span class="math inline">\(p\)</span> is large or there is multicollinearity among features.</li>
<li>Interpretability: By selecting only the most important features, LASSO makes the resulting model more interpretable, which is valuable in fields like bioinformatics, economics, and social sciences.</li>
</ul>
</section>
<section id="solution-path" class="level3" data-number="7.5.2">
<h3 data-number="7.5.2" class="anchored" data-anchor-id="solution-path"><span class="header-section-number">7.5.2</span> Solution Path</h3>
<p>To illustrate the effect of the lasso penalty in logistic regression, we can plot the solution path of the coefficients as a function of the regularization parameter <span class="math inline">\(\lambda\)</span>. This demonstration will use a simulated dataset to show how increasing <span class="math inline">\(\lambda\)</span> leads to more coefficients being set to zero.</p>
<div id="838574ee" class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_classification</span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler</span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 1: Generate a classification dataset</span></span>
<span id="cb37-8"><a href="#cb37-8" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> make_classification(n_samples<span class="op">=</span><span class="dv">100</span>, n_features<span class="op">=</span><span class="dv">20</span>, n_informative<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb37-9"><a href="#cb37-9" aria-hidden="true" tabindex="-1"></a>                               random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb37-10"><a href="#cb37-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-11"><a href="#cb37-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 2: Get a lambda grid given length of lambda and min_ratio of lambda_max</span></span>
<span id="cb37-12"><a href="#cb37-12" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_lambda_l1(xs: np.ndarray, y: np.ndarray, nlambda: <span class="bu">int</span>, min_ratio: <span class="bu">float</span>):</span>
<span id="cb37-13"><a href="#cb37-13" aria-hidden="true" tabindex="-1"></a>    ybar <span class="op">=</span> np.mean(y)</span>
<span id="cb37-14"><a href="#cb37-14" aria-hidden="true" tabindex="-1"></a>    xbar <span class="op">=</span> np.mean(xs, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb37-15"><a href="#cb37-15" aria-hidden="true" tabindex="-1"></a>    xs_centered <span class="op">=</span> xs <span class="op">-</span> xbar</span>
<span id="cb37-16"><a href="#cb37-16" aria-hidden="true" tabindex="-1"></a>    xty <span class="op">=</span> np.dot(xs_centered.T, (y <span class="op">-</span> ybar))</span>
<span id="cb37-17"><a href="#cb37-17" aria-hidden="true" tabindex="-1"></a>    lmax <span class="op">=</span> np.<span class="bu">max</span>(np.<span class="bu">abs</span>(xty))</span>
<span id="cb37-18"><a href="#cb37-18" aria-hidden="true" tabindex="-1"></a>    lambdas <span class="op">=</span> np.logspace(np.log10(lmax), np.log10(min_ratio <span class="op">*</span> lmax),</span>
<span id="cb37-19"><a href="#cb37-19" aria-hidden="true" tabindex="-1"></a>                              num<span class="op">=</span>nlambda)</span>
<span id="cb37-20"><a href="#cb37-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> lambdas</span>
<span id="cb37-21"><a href="#cb37-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-22"><a href="#cb37-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 3: Calculate lambda values</span></span>
<span id="cb37-23"><a href="#cb37-23" aria-hidden="true" tabindex="-1"></a>nlambda <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb37-24"><a href="#cb37-24" aria-hidden="true" tabindex="-1"></a>min_ratio <span class="op">=</span> <span class="fl">0.01</span></span>
<span id="cb37-25"><a href="#cb37-25" aria-hidden="true" tabindex="-1"></a>lambda_values <span class="op">=</span> get_lambda_l1(X, y, nlambda, min_ratio)</span>
<span id="cb37-26"><a href="#cb37-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-27"><a href="#cb37-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 4: Standardize the features</span></span>
<span id="cb37-28"><a href="#cb37-28" aria-hidden="true" tabindex="-1"></a>scaler <span class="op">=</span> StandardScaler()</span>
<span id="cb37-29"><a href="#cb37-29" aria-hidden="true" tabindex="-1"></a>X_scaled <span class="op">=</span> scaler.fit_transform(X)</span>
<span id="cb37-30"><a href="#cb37-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-31"><a href="#cb37-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 5: Initialize arrays to store the coefficients for each lambda value</span></span>
<span id="cb37-32"><a href="#cb37-32" aria-hidden="true" tabindex="-1"></a>coefficients <span class="op">=</span> []</span>
<span id="cb37-33"><a href="#cb37-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-34"><a href="#cb37-34" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 6: Fit logistic regression with L1 regularization (Lasso) for each lambda value</span></span>
<span id="cb37-35"><a href="#cb37-35" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> lam <span class="kw">in</span> lambda_values:</span>
<span id="cb37-36"><a href="#cb37-36" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> LogisticRegression(penalty<span class="op">=</span><span class="st">'l1'</span>, solver<span class="op">=</span><span class="st">'liblinear'</span>, C<span class="op">=</span><span class="dv">1</span><span class="op">/</span>lam, max_iter<span class="op">=</span><span class="dv">1000</span>)</span>
<span id="cb37-37"><a href="#cb37-37" aria-hidden="true" tabindex="-1"></a>    model.fit(X_scaled, y)</span>
<span id="cb37-38"><a href="#cb37-38" aria-hidden="true" tabindex="-1"></a>    coefficients.append(model.coef_.flatten())</span>
<span id="cb37-39"><a href="#cb37-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-40"><a href="#cb37-40" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert coefficients list to a NumPy array for plotting</span></span>
<span id="cb37-41"><a href="#cb37-41" aria-hidden="true" tabindex="-1"></a>coefficients <span class="op">=</span> np.array(coefficients)</span>
<span id="cb37-42"><a href="#cb37-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-43"><a href="#cb37-43" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 7: Plot the solution path for each feature</span></span>
<span id="cb37-44"><a href="#cb37-44" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb37-45"><a href="#cb37-45" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(coefficients.shape[<span class="dv">1</span>]):</span>
<span id="cb37-46"><a href="#cb37-46" aria-hidden="true" tabindex="-1"></a>    plt.plot(lambda_values, coefficients[:, i], label<span class="op">=</span><span class="ss">f'Feature </span><span class="sc">{</span>i <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb37-47"><a href="#cb37-47" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb37-48"><a href="#cb37-48" aria-hidden="true" tabindex="-1"></a>plt.xscale(<span class="st">'log'</span>)</span>
<span id="cb37-49"><a href="#cb37-49" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Lambda values (log scale)'</span>)</span>
<span id="cb37-50"><a href="#cb37-50" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Coefficient value'</span>)</span>
<span id="cb37-51"><a href="#cb37-51" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Solution Path of Logistic Lasso Regression'</span>)</span>
<span id="cb37-52"><a href="#cb37-52" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb37-53"><a href="#cb37-53" aria-hidden="true" tabindex="-1"></a>plt.legend(bbox_to_anchor<span class="op">=</span>(<span class="fl">1.05</span>, <span class="dv">1</span>), loc<span class="op">=</span><span class="st">'upper left'</span>)</span>
<span id="cb37-54"><a href="#cb37-54" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="stats_files/figure-html/cell-20-output-1.png" width="974" height="525" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="selection-the-tuning-parameter" class="level3" data-number="7.5.3">
<h3 data-number="7.5.3" class="anchored" data-anchor-id="selection-the-tuning-parameter"><span class="header-section-number">7.5.3</span> Selection the Tuning Parameter</h3>
<p>In logistic regression with LASSO regularization, selecting the optimal value of the regularization parameter <span class="math inline">\(C\)</span> (the inverse of <span class="math inline">\(\lambda\)</span>) is crucial to balancing the model’s bias and variance. A small <span class="math inline">\(C\)</span> value (large <span class="math inline">\(\lambda\)</span>) increases the regularization effect, shrinking more coefficients to zero and simplifying the model. Conversely, a large <span class="math inline">\(C\)</span> (small <span class="math inline">\(\lambda\)</span>) allows the model to fit the data more closely.</p>
<p>The best way to select the optimal <span class="math inline">\(C\)</span> is through cross-validation. In cross-validation, the dataset is split into several folds, and the model is trained on some folds while evaluated on the remaining fold. This process is repeated for each fold, and the results are averaged to ensure the model generalizes well to unseen data. The <span class="math inline">\(C\)</span> value that results in the best performance is selected.</p>
<p>The performance metric used in cross-validation can vary based on the task. Common metrics include:</p>
<ul>
<li>Log-loss: Measures how well the predicted probabilities match the actual outcomes.</li>
<li>Accuracy: Measures the proportion of correctly classified instances.</li>
<li>F1-Score: Balances precision and recall, especially useful for imbalanced classes.</li>
<li>AUC-ROC: Evaluates how well the model discriminates between the positive and negative classes.</li>
</ul>
<p>In Python, the <code>LogisticRegressionCV</code> class from <code>scikit-learn</code> automates cross-validation for logistic regression. It evaluates the model’s performance for a range of <span class="math inline">\(C\)</span> values and selects the best one.</p>
<div id="f9a9fa90" class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegressionCV</span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_classification</span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score</span>
<span id="cb38-6"><a href="#cb38-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-7"><a href="#cb38-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate synthetic data</span></span>
<span id="cb38-8"><a href="#cb38-8" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> make_classification(n_samples<span class="op">=</span><span class="dv">1000</span>, n_features<span class="op">=</span><span class="dv">20</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb38-9"><a href="#cb38-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-10"><a href="#cb38-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the dataset into training and testing sets</span></span>
<span id="cb38-11"><a href="#cb38-11" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.25</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb38-12"><a href="#cb38-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-13"><a href="#cb38-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize LogisticRegressionCV with L1 penalty for Lasso and cross-validation</span></span>
<span id="cb38-14"><a href="#cb38-14" aria-hidden="true" tabindex="-1"></a>log_reg_cv <span class="op">=</span> LogisticRegressionCV(</span>
<span id="cb38-15"><a href="#cb38-15" aria-hidden="true" tabindex="-1"></a>    Cs<span class="op">=</span>np.logspace(<span class="op">-</span><span class="dv">4</span>, <span class="dv">4</span>, <span class="dv">20</span>),  <span class="co"># Range of C values (inverse of lambda)</span></span>
<span id="cb38-16"><a href="#cb38-16" aria-hidden="true" tabindex="-1"></a>    cv<span class="op">=</span><span class="dv">5</span>,                       <span class="co"># 5-fold cross-validation</span></span>
<span id="cb38-17"><a href="#cb38-17" aria-hidden="true" tabindex="-1"></a>    penalty<span class="op">=</span><span class="st">'l1'</span>,               <span class="co"># Lasso regularization (L1 penalty)</span></span>
<span id="cb38-18"><a href="#cb38-18" aria-hidden="true" tabindex="-1"></a>    solver<span class="op">=</span><span class="st">'liblinear'</span>,         <span class="co"># Solver for L1 regularization</span></span>
<span id="cb38-19"><a href="#cb38-19" aria-hidden="true" tabindex="-1"></a>    scoring<span class="op">=</span><span class="st">'accuracy'</span>,         <span class="co"># Optimize for accuracy</span></span>
<span id="cb38-20"><a href="#cb38-20" aria-hidden="true" tabindex="-1"></a>    max_iter<span class="op">=</span><span class="dv">10000</span>              <span class="co"># Ensure convergence</span></span>
<span id="cb38-21"><a href="#cb38-21" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb38-22"><a href="#cb38-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-23"><a href="#cb38-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Train the model with cross-validation</span></span>
<span id="cb38-24"><a href="#cb38-24" aria-hidden="true" tabindex="-1"></a>log_reg_cv.fit(X_train, y_train)</span>
<span id="cb38-25"><a href="#cb38-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-26"><a href="#cb38-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Best C value (inverse of lambda)</span></span>
<span id="cb38-27"><a href="#cb38-27" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Best C value: </span><span class="sc">{</span>log_reg_cv<span class="sc">.</span>C_[<span class="dv">0</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb38-28"><a href="#cb38-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-29"><a href="#cb38-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluate the model on the test set</span></span>
<span id="cb38-30"><a href="#cb38-30" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> log_reg_cv.predict(X_test)</span>
<span id="cb38-31"><a href="#cb38-31" aria-hidden="true" tabindex="-1"></a>test_accuracy <span class="op">=</span> accuracy_score(y_test, y_pred)</span>
<span id="cb38-32"><a href="#cb38-32" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Test Accuracy: </span><span class="sc">{</span>test_accuracy<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb38-33"><a href="#cb38-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-34"><a href="#cb38-34" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the coefficients of the best model</span></span>
<span id="cb38-35"><a href="#cb38-35" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Model Coefficients:</span><span class="ch">\n</span><span class="st">"</span>, log_reg_cv.coef_)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Best C value: 0.08858667904100823
Test Accuracy: 0.86
Model Coefficients:
 [[ 0.          0.          0.05552448  0.          0.          1.90889734
   0.          0.          0.          0.          0.0096863   0.23541942
   0.          0.         -0.0268928   0.          0.          0.
   0.          0.        ]]</code></pre>
</div>
</div>
</section>
<section id="preparing-for-logistic-regression-fitting" class="level3" data-number="7.5.4">
<h3 data-number="7.5.4" class="anchored" data-anchor-id="preparing-for-logistic-regression-fitting"><span class="header-section-number">7.5.4</span> Preparing for Logistic Regression Fitting</h3>
<p>The <code>LogisticRegression()</code> function in <code>scikit.learn</code> takes the design matrix of the regression as input, which needs to be prepared with care from the covariates or features that we have.</p>
<section id="continuous-variables" class="level4" data-number="7.5.4.1">
<h4 data-number="7.5.4.1" class="anchored" data-anchor-id="continuous-variables"><span class="header-section-number">7.5.4.1</span> Continuous Variables</h4>
<p>For continuous variables, it is often desirable to standardized them so that they have mean zero and standard deviation one. There are multiple advantages of doing so. It improves numerical stability in algorithms like logistic regression that rely on gradient descent, ensuring faster convergence and preventing features with large scales from dominating the optimization process. Standardization also enhances the interpretability of model coefficients by allowing for direct comparison of the effects of different features, as coefficients then represent the change in outcome for a one standard deviation increase in each variable. Additionally, it ensures that regularization techniques like Lasso and Ridge treat all features equally, allowing the model to select the most relevant ones without being biased by feature magnitude.</p>
<p>Moreover, standardization is essential for distance-based models such as k-Nearest Neighbors (k-NN) and Support Vector Machines (SVMs), where differences in feature scale can distort the calculations. It also prevents models from being sensitive to arbitrary changes in the units of measurement, improving robustness and consistency. Finally, standardization facilitates better visualizations and diagnostics by putting all variables on a comparable scale, making patterns and residuals easier to interpret. Overall, it is a simple yet powerful preprocessing step that leads to better model performance and interpretability.</p>
<p>We have already seen this with <code>StandardScaler</code>.</p>
</section>
<section id="categorical-variables" class="level4" data-number="7.5.4.2">
<h4 data-number="7.5.4.2" class="anchored" data-anchor-id="categorical-variables"><span class="header-section-number">7.5.4.2</span> Categorical Variables</h4>
<p>Categorical variables can be classified into two types: nominal and ordinal. Nominal variables represent categories with no inherent order or ranking between them. Examples include variables like “gender” (male, female) or “color” (red, blue, green), where the categories are simply labels and one category does not carry more significance than another. Ordinal variables, on the other hand, represent categories with a meaningful order or ranking. For example, education levels such as “high school,” “bachelor,” “master,” and “PhD” have a clear hierarchy, where each level is ranked higher than the previous one. However, the differences between the ranks are not necessarily uniform or quantifiable, making ordinal variables distinct from numerical variables. Understanding the distinction between nominal and ordinal variables is important when deciding how to encode and interpret them in statistical models.</p>
<p>Categorical variables needs to be coded into numerical values before further processing. In Python, nominal and ordinal variables are typically encoded differently to account for their unique properties. Nominal variables, which have no inherent order, are often encoded using One-Hot Encoding, where each category is transformed into a binary column (0 or 1). For example, the OneHotEncoder from scikit-learn can be used to convert a “color” variable with categories like “red,” “blue,” and “green” into separate columns color_red, color_blue, and color_green, with only one column being 1 for each observation. On the other hand, ordinal variables, which have a meaningful order, are best encoded using Ordinal Encoding. This method assigns an integer to each category based on their rank. For example, an “education” variable with categories “high school,” “bachelor,” “master,” and “PhD” can be encoded as 0, 1, 2, and 3, respectively. The OrdinalEncoder from scikit-learn can be used to implement this encoding, which ensures that the model respects the order of the categories during analysis.</p>
</section>
<section id="an-example" class="level4" data-number="7.5.4.3">
<h4 data-number="7.5.4.3" class="anchored" data-anchor-id="an-example"><span class="header-section-number">7.5.4.3</span> An Example</h4>
<p>Here is a demo with <code>pipeline</code> using a simulated dataset.</p>
<p>First we generate data with sample size 1000 from a logistic model with both categorical and numerical covariates.</p>
<div id="600dc4eb" class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> OneHotEncoder, StandardScaler</span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.pipeline <span class="im">import</span> Pipeline</span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.compose <span class="im">import</span> ColumnTransformer</span>
<span id="cb40-7"><a href="#cb40-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb40-8"><a href="#cb40-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.special <span class="im">import</span> expit  <span class="co"># Sigmoid function</span></span>
<span id="cb40-9"><a href="#cb40-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-10"><a href="#cb40-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate a dataset with the specified size</span></span>
<span id="cb40-11"><a href="#cb40-11" aria-hidden="true" tabindex="-1"></a>dataset_size <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb40-12"><a href="#cb40-12" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">20241014</span>)</span>
<span id="cb40-13"><a href="#cb40-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-14"><a href="#cb40-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulate categorical and numerical features</span></span>
<span id="cb40-15"><a href="#cb40-15" aria-hidden="true" tabindex="-1"></a>gender <span class="op">=</span> np.random.choice(</span>
<span id="cb40-16"><a href="#cb40-16" aria-hidden="true" tabindex="-1"></a>    [<span class="st">'male'</span>, <span class="st">'female'</span>], size<span class="op">=</span>dataset_size)  <span class="co"># Nominal variable</span></span>
<span id="cb40-17"><a href="#cb40-17" aria-hidden="true" tabindex="-1"></a>education <span class="op">=</span> np.random.choice(</span>
<span id="cb40-18"><a href="#cb40-18" aria-hidden="true" tabindex="-1"></a>    [<span class="st">'high_school'</span>, <span class="st">'bachelor'</span>, <span class="st">'master'</span>, <span class="st">'phd'</span>], size<span class="op">=</span>dataset_size)  <span class="co"># Ordinal variable</span></span>
<span id="cb40-19"><a href="#cb40-19" aria-hidden="true" tabindex="-1"></a>age <span class="op">=</span> np.random.randint(<span class="dv">18</span>, <span class="dv">65</span>, size<span class="op">=</span>dataset_size)</span>
<span id="cb40-20"><a href="#cb40-20" aria-hidden="true" tabindex="-1"></a>income <span class="op">=</span> np.random.randint(<span class="dv">30000</span>, <span class="dv">120000</span>, size<span class="op">=</span>dataset_size)</span>
<span id="cb40-21"><a href="#cb40-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-22"><a href="#cb40-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a logistic relationship between the features and the outcome</span></span>
<span id="cb40-23"><a href="#cb40-23" aria-hidden="true" tabindex="-1"></a>gender_num <span class="op">=</span> np.where(gender <span class="op">==</span> <span class="st">'male'</span>, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb40-24"><a href="#cb40-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-25"><a href="#cb40-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the linear predictor with regression coefficients</span></span>
<span id="cb40-26"><a href="#cb40-26" aria-hidden="true" tabindex="-1"></a>linear_combination <span class="op">=</span> (</span>
<span id="cb40-27"><a href="#cb40-27" aria-hidden="true" tabindex="-1"></a>    <span class="fl">0.3</span> <span class="op">*</span> gender_num <span class="op">-</span> <span class="fl">0.02</span> <span class="op">*</span> age <span class="op">+</span> <span class="fl">0.00002</span> <span class="op">*</span> income</span>
<span id="cb40-28"><a href="#cb40-28" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb40-29"><a href="#cb40-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-30"><a href="#cb40-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Apply sigmoid function to get probabilities</span></span>
<span id="cb40-31"><a href="#cb40-31" aria-hidden="true" tabindex="-1"></a>probabilities <span class="op">=</span> expit(linear_combination)</span>
<span id="cb40-32"><a href="#cb40-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-33"><a href="#cb40-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate binary outcome based on the probabilities</span></span>
<span id="cb40-34"><a href="#cb40-34" aria-hidden="true" tabindex="-1"></a>outcome <span class="op">=</span> np.random.binomial(<span class="dv">1</span>, probabilities)</span>
<span id="cb40-35"><a href="#cb40-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-36"><a href="#cb40-36" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a DataFrame</span></span>
<span id="cb40-37"><a href="#cb40-37" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> pd.DataFrame({</span>
<span id="cb40-38"><a href="#cb40-38" aria-hidden="true" tabindex="-1"></a>    <span class="st">'gender'</span>: gender,</span>
<span id="cb40-39"><a href="#cb40-39" aria-hidden="true" tabindex="-1"></a>    <span class="st">'education'</span>: education,</span>
<span id="cb40-40"><a href="#cb40-40" aria-hidden="true" tabindex="-1"></a>    <span class="st">'age'</span>: age,</span>
<span id="cb40-41"><a href="#cb40-41" aria-hidden="true" tabindex="-1"></a>    <span class="st">'income'</span>: income,</span>
<span id="cb40-42"><a href="#cb40-42" aria-hidden="true" tabindex="-1"></a>    <span class="st">'outcome'</span>: outcome</span>
<span id="cb40-43"><a href="#cb40-43" aria-hidden="true" tabindex="-1"></a>})</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Next we split the data into features and target and define transformers for each types of feature columns.</p>
<div id="277cfeab" class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the dataset into features (X) and target (y)</span></span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> data[[<span class="st">'gender'</span>, <span class="st">'education'</span>, <span class="st">'age'</span>, <span class="st">'income'</span>]]</span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> data[<span class="st">'outcome'</span>]</span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Define categorical and numerical columns</span></span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a>categorical_cols <span class="op">=</span> [<span class="st">'gender'</span>, <span class="st">'education'</span>]  </span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a>numerical_cols <span class="op">=</span> [<span class="st">'age'</span>, <span class="st">'income'</span>]</span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-9"><a href="#cb41-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Define transformations for categorical variable</span></span>
<span id="cb41-10"><a href="#cb41-10" aria-hidden="true" tabindex="-1"></a>categorical_transformer <span class="op">=</span> OneHotEncoder(</span>
<span id="cb41-11"><a href="#cb41-11" aria-hidden="true" tabindex="-1"></a>    categories<span class="op">=</span>[[<span class="st">'male'</span>, <span class="st">'female'</span>], [<span class="st">'high_school'</span>, <span class="st">'bachelor'</span>, <span class="st">'master'</span>, <span class="st">'phd'</span>]],</span>
<span id="cb41-12"><a href="#cb41-12" aria-hidden="true" tabindex="-1"></a>    drop<span class="op">=</span><span class="st">'first'</span>)</span>
<span id="cb41-13"><a href="#cb41-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-14"><a href="#cb41-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Define transformations for continuous variables</span></span>
<span id="cb41-15"><a href="#cb41-15" aria-hidden="true" tabindex="-1"></a>numerical_transformer <span class="op">=</span> StandardScaler()</span>
<span id="cb41-16"><a href="#cb41-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-17"><a href="#cb41-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Use ColumnTransformer to transform the columns</span></span>
<span id="cb41-18"><a href="#cb41-18" aria-hidden="true" tabindex="-1"></a>preprocessor <span class="op">=</span> ColumnTransformer(</span>
<span id="cb41-19"><a href="#cb41-19" aria-hidden="true" tabindex="-1"></a>    transformers<span class="op">=</span>[</span>
<span id="cb41-20"><a href="#cb41-20" aria-hidden="true" tabindex="-1"></a>        (<span class="st">'cat'</span>, categorical_transformer, categorical_cols),</span>
<span id="cb41-21"><a href="#cb41-21" aria-hidden="true" tabindex="-1"></a>        (<span class="st">'num'</span>, numerical_transformer, numerical_cols)</span>
<span id="cb41-22"><a href="#cb41-22" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb41-23"><a href="#cb41-23" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Define a pipeline, which preprocess the data and then fits a logistic model.</p>
<div id="c27cc802" class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a>pipeline <span class="op">=</span> Pipeline(steps<span class="op">=</span>[</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>    (<span class="st">'preprocessor'</span>, preprocessor),</span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a>    (<span class="st">'classifier'</span>, LogisticRegression(penalty<span class="op">=</span><span class="st">'l1'</span>, solver<span class="op">=</span><span class="st">'liblinear'</span>,</span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a>    max_iter<span class="op">=</span><span class="dv">1000</span>))</span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb42-6"><a href="#cb42-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-7"><a href="#cb42-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the data into training and testing sets</span></span>
<span id="cb42-8"><a href="#cb42-8" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(</span>
<span id="cb42-9"><a href="#cb42-9" aria-hidden="true" tabindex="-1"></a>    X, y, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">2024</span>)</span>
<span id="cb42-10"><a href="#cb42-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-11"><a href="#cb42-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit the pipeline to the training data</span></span>
<span id="cb42-12"><a href="#cb42-12" aria-hidden="true" tabindex="-1"></a>pipeline.fit(X_train, y_train)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="23">
<style>#sk-container-id-1 {
  /* Definition of color scheme common for light and dark mode */
  --sklearn-color-text: #000;
  --sklearn-color-text-muted: #666;
  --sklearn-color-line: gray;
  /* Definition of color scheme for unfitted estimators */
  --sklearn-color-unfitted-level-0: #fff5e6;
  --sklearn-color-unfitted-level-1: #f6e4d2;
  --sklearn-color-unfitted-level-2: #ffe0b3;
  --sklearn-color-unfitted-level-3: chocolate;
  /* Definition of color scheme for fitted estimators */
  --sklearn-color-fitted-level-0: #f0f8ff;
  --sklearn-color-fitted-level-1: #d4ebff;
  --sklearn-color-fitted-level-2: #b3dbfd;
  --sklearn-color-fitted-level-3: cornflowerblue;

  /* Specific color for light theme */
  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));
  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-icon: #696969;

  @media (prefers-color-scheme: dark) {
    /* Redefinition of color scheme for dark theme */
    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));
    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-icon: #878787;
  }
}

#sk-container-id-1 {
  color: var(--sklearn-color-text);
}

#sk-container-id-1 pre {
  padding: 0;
}

#sk-container-id-1 input.sk-hidden--visually {
  border: 0;
  clip: rect(1px 1px 1px 1px);
  clip: rect(1px, 1px, 1px, 1px);
  height: 1px;
  margin: -1px;
  overflow: hidden;
  padding: 0;
  position: absolute;
  width: 1px;
}

#sk-container-id-1 div.sk-dashed-wrapped {
  border: 1px dashed var(--sklearn-color-line);
  margin: 0 0.4em 0.5em 0.4em;
  box-sizing: border-box;
  padding-bottom: 0.4em;
  background-color: var(--sklearn-color-background);
}

#sk-container-id-1 div.sk-container {
  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`
     but bootstrap.min.css set `[hidden] { display: none !important; }`
     so we also need the `!important` here to be able to override the
     default hidden behavior on the sphinx rendered scikit-learn.org.
     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */
  display: inline-block !important;
  position: relative;
}

#sk-container-id-1 div.sk-text-repr-fallback {
  display: none;
}

div.sk-parallel-item,
div.sk-serial,
div.sk-item {
  /* draw centered vertical line to link estimators */
  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));
  background-size: 2px 100%;
  background-repeat: no-repeat;
  background-position: center center;
}

/* Parallel-specific style estimator block */

#sk-container-id-1 div.sk-parallel-item::after {
  content: "";
  width: 100%;
  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);
  flex-grow: 1;
}

#sk-container-id-1 div.sk-parallel {
  display: flex;
  align-items: stretch;
  justify-content: center;
  background-color: var(--sklearn-color-background);
  position: relative;
}

#sk-container-id-1 div.sk-parallel-item {
  display: flex;
  flex-direction: column;
}

#sk-container-id-1 div.sk-parallel-item:first-child::after {
  align-self: flex-end;
  width: 50%;
}

#sk-container-id-1 div.sk-parallel-item:last-child::after {
  align-self: flex-start;
  width: 50%;
}

#sk-container-id-1 div.sk-parallel-item:only-child::after {
  width: 0;
}

/* Serial-specific style estimator block */

#sk-container-id-1 div.sk-serial {
  display: flex;
  flex-direction: column;
  align-items: center;
  background-color: var(--sklearn-color-background);
  padding-right: 1em;
  padding-left: 1em;
}


/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is
clickable and can be expanded/collapsed.
- Pipeline and ColumnTransformer use this feature and define the default style
- Estimators will overwrite some part of the style using the `sk-estimator` class
*/

/* Pipeline and ColumnTransformer style (default) */

#sk-container-id-1 div.sk-toggleable {
  /* Default theme specific background. It is overwritten whether we have a
  specific estimator or a Pipeline/ColumnTransformer */
  background-color: var(--sklearn-color-background);
}

/* Toggleable label */
#sk-container-id-1 label.sk-toggleable__label {
  cursor: pointer;
  display: flex;
  width: 100%;
  margin-bottom: 0;
  padding: 0.5em;
  box-sizing: border-box;
  text-align: center;
  align-items: start;
  justify-content: space-between;
  gap: 0.5em;
}

#sk-container-id-1 label.sk-toggleable__label .caption {
  font-size: 0.6rem;
  font-weight: lighter;
  color: var(--sklearn-color-text-muted);
}

#sk-container-id-1 label.sk-toggleable__label-arrow:before {
  /* Arrow on the left of the label */
  content: "▸";
  float: left;
  margin-right: 0.25em;
  color: var(--sklearn-color-icon);
}

#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {
  color: var(--sklearn-color-text);
}

/* Toggleable content - dropdown */

#sk-container-id-1 div.sk-toggleable__content {
  max-height: 0;
  max-width: 0;
  overflow: hidden;
  text-align: left;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-1 div.sk-toggleable__content.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-1 div.sk-toggleable__content pre {
  margin: 0.2em;
  border-radius: 0.25em;
  color: var(--sklearn-color-text);
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-1 div.sk-toggleable__content.fitted pre {
  /* unfitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {
  /* Expand drop-down */
  max-height: 200px;
  max-width: 100%;
  overflow: auto;
}

#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {
  content: "▾";
}

/* Pipeline/ColumnTransformer-specific style */

#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator-specific style */

/* Colorize estimator box */
#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

#sk-container-id-1 div.sk-label label.sk-toggleable__label,
#sk-container-id-1 div.sk-label label {
  /* The background is the default theme color */
  color: var(--sklearn-color-text-on-default-background);
}

/* On hover, darken the color of the background */
#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

/* Label box, darken color on hover, fitted */
#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator label */

#sk-container-id-1 div.sk-label label {
  font-family: monospace;
  font-weight: bold;
  display: inline-block;
  line-height: 1.2em;
}

#sk-container-id-1 div.sk-label-container {
  text-align: center;
}

/* Estimator-specific */
#sk-container-id-1 div.sk-estimator {
  font-family: monospace;
  border: 1px dotted var(--sklearn-color-border-box);
  border-radius: 0.25em;
  box-sizing: border-box;
  margin-bottom: 0.5em;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-1 div.sk-estimator.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

/* on hover */
#sk-container-id-1 div.sk-estimator:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-1 div.sk-estimator.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Specification for estimator info (e.g. "i" and "?") */

/* Common style for "i" and "?" */

.sk-estimator-doc-link,
a:link.sk-estimator-doc-link,
a:visited.sk-estimator-doc-link {
  float: right;
  font-size: smaller;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1em;
  height: 1em;
  width: 1em;
  text-decoration: none !important;
  margin-left: 0.5em;
  text-align: center;
  /* unfitted */
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
  color: var(--sklearn-color-unfitted-level-1);
}

.sk-estimator-doc-link.fitted,
a:link.sk-estimator-doc-link.fitted,
a:visited.sk-estimator-doc-link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
div.sk-estimator:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover,
div.sk-label-container:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover,
div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

/* Span, style for the box shown on hovering the info icon */
.sk-estimator-doc-link span {
  display: none;
  z-index: 9999;
  position: relative;
  font-weight: normal;
  right: .2ex;
  padding: .5ex;
  margin: .5ex;
  width: min-content;
  min-width: 20ex;
  max-width: 50ex;
  color: var(--sklearn-color-text);
  box-shadow: 2pt 2pt 4pt #999;
  /* unfitted */
  background: var(--sklearn-color-unfitted-level-0);
  border: .5pt solid var(--sklearn-color-unfitted-level-3);
}

.sk-estimator-doc-link.fitted span {
  /* fitted */
  background: var(--sklearn-color-fitted-level-0);
  border: var(--sklearn-color-fitted-level-3);
}

.sk-estimator-doc-link:hover span {
  display: block;
}

/* "?"-specific style due to the `<a>` HTML tag */

#sk-container-id-1 a.estimator_doc_link {
  float: right;
  font-size: 1rem;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1rem;
  height: 1rem;
  width: 1rem;
  text-decoration: none;
  /* unfitted */
  color: var(--sklearn-color-unfitted-level-1);
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
}

#sk-container-id-1 a.estimator_doc_link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
#sk-container-id-1 a.estimator_doc_link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

#sk-container-id-1 a.estimator_doc_link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
}
</style><div id="sk-container-id-1" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>Pipeline(steps=[('preprocessor',
                 ColumnTransformer(transformers=[('cat',
                                                  OneHotEncoder(categories=[['male',
                                                                             'female'],
                                                                            ['high_school',
                                                                             'bachelor',
                                                                             'master',
                                                                             'phd']],
                                                                drop='first'),
                                                  ['gender', 'education']),
                                                 ('num', StandardScaler(),
                                                  ['age', 'income'])])),
                ('classifier',
                 LogisticRegression(max_iter=1000, penalty='l1',
                                    solver='liblinear'))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden=""><div class="sk-item sk-dashed-wrapped"><div class="sk-label-container"><div class="sk-label fitted sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-1" type="checkbox"><label for="sk-estimator-id-1" class="sk-toggleable__label fitted sk-toggleable__label-arrow"><div><div>Pipeline</div></div><div><a class="sk-estimator-doc-link fitted" rel="noreferrer" target="_blank" href="https://scikit-learn.org/1.6/modules/generated/sklearn.pipeline.Pipeline.html">?<span>Documentation for Pipeline</span></a><span class="sk-estimator-doc-link fitted">i<span>Fitted</span></span></div></label><div class="sk-toggleable__content fitted"><pre>Pipeline(steps=[('preprocessor',
                 ColumnTransformer(transformers=[('cat',
                                                  OneHotEncoder(categories=[['male',
                                                                             'female'],
                                                                            ['high_school',
                                                                             'bachelor',
                                                                             'master',
                                                                             'phd']],
                                                                drop='first'),
                                                  ['gender', 'education']),
                                                 ('num', StandardScaler(),
                                                  ['age', 'income'])])),
                ('classifier',
                 LogisticRegression(max_iter=1000, penalty='l1',
                                    solver='liblinear'))])</pre></div> </div></div><div class="sk-serial"><div class="sk-item sk-dashed-wrapped"><div class="sk-label-container"><div class="sk-label fitted sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-2" type="checkbox"><label for="sk-estimator-id-2" class="sk-toggleable__label fitted sk-toggleable__label-arrow"><div><div>preprocessor: ColumnTransformer</div></div><div><a class="sk-estimator-doc-link fitted" rel="noreferrer" target="_blank" href="https://scikit-learn.org/1.6/modules/generated/sklearn.compose.ColumnTransformer.html">?<span>Documentation for preprocessor: ColumnTransformer</span></a></div></label><div class="sk-toggleable__content fitted"><pre>ColumnTransformer(transformers=[('cat',
                                 OneHotEncoder(categories=[['male', 'female'],
                                                           ['high_school',
                                                            'bachelor',
                                                            'master', 'phd']],
                                               drop='first'),
                                 ['gender', 'education']),
                                ('num', StandardScaler(), ['age', 'income'])])</pre></div> </div></div><div class="sk-parallel"><div class="sk-parallel-item"><div class="sk-item"><div class="sk-label-container"><div class="sk-label fitted sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-3" type="checkbox"><label for="sk-estimator-id-3" class="sk-toggleable__label fitted sk-toggleable__label-arrow"><div><div>cat</div></div></label><div class="sk-toggleable__content fitted"><pre>['gender', 'education']</pre></div> </div></div><div class="sk-serial"><div class="sk-item"><div class="sk-estimator fitted sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-4" type="checkbox"><label for="sk-estimator-id-4" class="sk-toggleable__label fitted sk-toggleable__label-arrow"><div><div>OneHotEncoder</div></div><div><a class="sk-estimator-doc-link fitted" rel="noreferrer" target="_blank" href="https://scikit-learn.org/1.6/modules/generated/sklearn.preprocessing.OneHotEncoder.html">?<span>Documentation for OneHotEncoder</span></a></div></label><div class="sk-toggleable__content fitted"><pre>OneHotEncoder(categories=[['male', 'female'],
                          ['high_school', 'bachelor', 'master', 'phd']],
              drop='first')</pre></div> </div></div></div></div></div><div class="sk-parallel-item"><div class="sk-item"><div class="sk-label-container"><div class="sk-label fitted sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-5" type="checkbox"><label for="sk-estimator-id-5" class="sk-toggleable__label fitted sk-toggleable__label-arrow"><div><div>num</div></div></label><div class="sk-toggleable__content fitted"><pre>['age', 'income']</pre></div> </div></div><div class="sk-serial"><div class="sk-item"><div class="sk-estimator fitted sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-6" type="checkbox"><label for="sk-estimator-id-6" class="sk-toggleable__label fitted sk-toggleable__label-arrow"><div><div>StandardScaler</div></div><div><a class="sk-estimator-doc-link fitted" rel="noreferrer" target="_blank" href="https://scikit-learn.org/1.6/modules/generated/sklearn.preprocessing.StandardScaler.html">?<span>Documentation for StandardScaler</span></a></div></label><div class="sk-toggleable__content fitted"><pre>StandardScaler()</pre></div> </div></div></div></div></div></div></div><div class="sk-item"><div class="sk-estimator fitted sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-7" type="checkbox"><label for="sk-estimator-id-7" class="sk-toggleable__label fitted sk-toggleable__label-arrow"><div><div>LogisticRegression</div></div><div><a class="sk-estimator-doc-link fitted" rel="noreferrer" target="_blank" href="https://scikit-learn.org/1.6/modules/generated/sklearn.linear_model.LogisticRegression.html">?<span>Documentation for LogisticRegression</span></a></div></label><div class="sk-toggleable__content fitted"><pre>LogisticRegression(max_iter=1000, penalty='l1', solver='liblinear')</pre></div> </div></div></div></div></div></div>
</div>
</div>
<p>Check the coefficients of the fitted logistic regression model.</p>
<div id="13d2b747" class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> pipeline.named_steps[<span class="st">'classifier'</span>]</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>intercept <span class="op">=</span> model.intercept_</span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a>coefficients <span class="op">=</span> model.coef_</span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Check the preprocessor's encoding</span></span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a>encoded_columns <span class="op">=</span> pipeline.named_steps[<span class="st">'preprocessor'</span>]<span class="op">\</span></span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a>.transformers_[<span class="dv">0</span>][<span class="dv">1</span>].get_feature_names_out(categorical_cols)</span>
<span id="cb43-8"><a href="#cb43-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-9"><a href="#cb43-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Show intercept, coefficients, and encoded feature names</span></span>
<span id="cb43-10"><a href="#cb43-10" aria-hidden="true" tabindex="-1"></a>intercept, coefficients, <span class="bu">list</span>(encoded_columns)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="24">
<pre><code>(array([0.66748582]),
 array([[ 0.30568894,  0.10069842,  0.12087311,  0.22576774, -0.24749201,
          0.55828424]]),
 ['gender_female', 'education_bachelor', 'education_master', 'education_phd'])</code></pre>
</div>
</div>
<p>Note that the encoded columns has one for gender and three for education, with <code>male</code> and <code>high_school</code> as reference levels, respectively. The reference level was determined when calling <code>oneHotEncoder()</code> with <code>drop = 'first'</code>. If <code>categories</code> were not specified, the first level in alphabetical order would be dropped. With the default <code>drop = 'none'</code>, the estimated coefficients will have two columns that are not estimable and were set to zero. Obviously, if no level were dropped in forming the model matrix, the columns of the one hot encoding for each categorical variable would be perfectly linearly dependent because they would sum to one.</p>
<p>The regression coefficients returned by the logistic regression model in this case should be interpreted on the standardized scale of the numerical covariates (e.g., <code>age</code> and <code>income</code>). This is because we applied standardization to the numerical features using StandardScaler in the pipeline before fitting the model. For example, the coefficient for age would reflect the change in the log-odds of the outcome for a 1 standard deviation increase in age, rather than a 1-unit increase in years. The coefficients for the one-hot encoded categorical variables (gender and education) are on the original scale because one-hot encoding does not change the scale of the variables. For instance, the coefficient for <code>gender_female</code> tells us how much the log-odds of the outcome changes when the observation is male versus the reference category (<code>male</code>).</p>
</section>
</section>
</section>
<section id="count-data-modeling" class="level2" data-number="7.6">
<h2 data-number="7.6" class="anchored" data-anchor-id="count-data-modeling"><span class="header-section-number">7.6</span> Count Data Modeling</h2>
<p>Count data consists of non-negative integers representing event occurrences over a fixed unit of time or space. These data often exhibit skewness, overdispersion, and a prevalence of zeros, requiring specialized statistical models. Count data is common in applied fields such as urban planning and environmental studies. This section introduces statistical models for count data, focusing on the Poisson and Negative Binomial (NB) distributions. Their probability mass functions (pmfs) are linked to Generalized Linear Model (GLM) parameters.</p>
<section id="poisson-regression" class="level3" data-number="7.6.1">
<h3 data-number="7.6.1" class="anchored" data-anchor-id="poisson-regression"><span class="header-section-number">7.6.1</span> Poisson Regression</h3>
<p>The Poisson model is a member of the GLM. It assumes that the count variable <span class="math inline">\(Y\)</span> follows a Poisson distribution:</p>
<p><span class="math display">\[
\Pr(Y = y) = \frac{\lambda^y e^{-\lambda}}{y!}, \quad y = 0,1,2,\dots
\]</span> where <span class="math inline">\(\lambda\)</span> is the expected count, linked to predictor variables through a log link function:</p>
<p><span class="math display">\[
\log \lambda = X^{\top} \beta.
\]</span></p>
<p>Here, $X represents the vector of covariates, and <span class="math inline">\(\beta\)</span> denotes the regression coefficients. The model assumes equidispersion (i.e., <span class="math inline">\(E[Y] = \text{Var}(Y)\)</span>), which is often violated in practice.</p>
<ul>
<li>The coefficient <span class="math inline">\(\beta_j\)</span> represents the log change in the expected count per unit increase in <span class="math inline">\(X_j\)</span>.</li>
<li>Exponentiating <span class="math inline">\(\beta_j\)</span> provides the multiplicative effect on the mean count.</li>
<li>If <span class="math inline">\(\beta_j &gt; 0\)</span>, increasing <span class="math inline">\(X_j\)</span> leads to higher counts, while <span class="math inline">\(\beta_j &lt; 0\)</span> suggests a negative association</li>
</ul>
</section>
<section id="negative-binomial-regression" class="level3" data-number="7.6.2">
<h3 data-number="7.6.2" class="anchored" data-anchor-id="negative-binomial-regression"><span class="header-section-number">7.6.2</span> Negative Binomial Regression</h3>
<p>When overdispersion (variance exceeding the mean) is present, the Negative Binomial (NB) regression provides a more flexible alternative. The NB model introduces an overdispersion parameter <span class="math inline">\(\theta\)</span>, modifying the variance structure:</p>
<p><span class="math display">\[
\Pr(Y = y) = \frac{\Gamma(y + \theta^{-1})}{y! \Gamma(\theta^{-1})}
\left( \frac{\theta \lambda}{1 + \theta \lambda} \right)^y
\left( \frac{1}{1 + \theta \lambda} \right)^{\theta^{-1}},
\]</span></p>
<p>where <span class="math inline">\(\theta\)</span> controls the degree of dispersion. The mean remains <span class="math inline">\(\lambda\)</span>, but the variance expands to:</p>
<p><span class="math display">\[
\text{Var}(Y) = \lambda + \frac{\lambda^2}{\theta}.
\]</span></p>
<p>The log link function remains to be commonly used.</p>
<ul>
<li>Coefficients in NB regression are interpreted similarly to Poisson regression.</li>
<li>The dispersion parameter <span class="math inline">\(\theta\)</span> quantifies the degree of overdispersion; larger values suggest the Poisson model may still be appropriate.</li>
</ul>
</section>
<section id="model-diagnosis" class="level3" data-number="7.6.3">
<h3 data-number="7.6.3" class="anchored" data-anchor-id="model-diagnosis"><span class="header-section-number">7.6.3</span> Model Diagnosis</h3>
<p>Assessing model fit is crucial in count data modeling. Common diagnostic methods include:</p>
<ul>
<li>Overdispersion Check: If the variance significantly exceeds the mean, NB regression is preferred.</li>
<li>Goodness-of-Fit: Comparing Akaike Information Criterion (AIC) values for Poisson and NB models; lower AIC suggests a better fit.</li>
<li>Residual Analysis: Examining Pearson and deviance residuals for systematic patterns.</li>
<li>Zero-Inflation Check: If excess zeros exist, zero-inflated models may be required.</li>
</ul>
</section>
</section>
<section id="an-example-with-nyc-street-flood" class="level2" data-number="7.7">
<h2 data-number="7.7" class="anchored" data-anchor-id="an-example-with-nyc-street-flood"><span class="header-section-number">7.7</span> An Example with NYC Street Flood</h2>
<p>We use the NYC street flood data from the mid-term project. This analysis focuses on two sewer-related complaints in 2024: Street Flooding (SF) and Catch Basin (CB). SF complaints serve as a practical indicator of street flooding, while CB complaints provide insights into a key infrastructural factor—when catch basins fail to drain rainwater properly due to blockages or structural issues, water accumulates on the streets</p>
<p>Let’s reformat the data to create count times series of SF and CB complaints by zip code.</p>
<div id="acc97222" class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Reload the dataset, ensuring 'Incident Zip' is read as a string from the start</span></span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_csv(<span class="st">"data/nycflood2024.csv"</span>,</span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a>                 dtype<span class="op">=</span>{<span class="st">"Incident Zip"</span>: <span class="bu">str</span>}, parse_dates<span class="op">=</span>[<span class="st">"Created Date"</span>])</span>
<span id="cb45-5"><a href="#cb45-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-6"><a href="#cb45-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Filter for incidents in 2024, but also include 2023-12-31 for lag calculation</span></span>
<span id="cb45-7"><a href="#cb45-7" aria-hidden="true" tabindex="-1"></a>df_2024 <span class="op">=</span> df[(df[<span class="st">"Created Date"</span>] <span class="op">&gt;=</span> <span class="st">"2023-12-31"</span>) <span class="op">&amp;</span></span>
<span id="cb45-8"><a href="#cb45-8" aria-hidden="true" tabindex="-1"></a>                           (df[<span class="st">"Created Date"</span>] <span class="op">&lt;=</span> <span class="st">"2024-12-31"</span>)].copy()</span>
<span id="cb45-9"><a href="#cb45-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-10"><a href="#cb45-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract date and ensure proper formatting</span></span>
<span id="cb45-11"><a href="#cb45-11" aria-hidden="true" tabindex="-1"></a>df_2024[<span class="st">"Date"</span>] <span class="op">=</span> df_2024[<span class="st">"Created Date"</span>].dt.date</span>
<span id="cb45-12"><a href="#cb45-12" aria-hidden="true" tabindex="-1"></a>df_2024[<span class="st">"Zipcode"</span>] <span class="op">=</span> df_2024[<span class="st">"Incident Zip"</span>].astype(<span class="bu">str</span>)</span>
<span id="cb45-13"><a href="#cb45-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-14"><a href="#cb45-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Identify complaint types</span></span>
<span id="cb45-15"><a href="#cb45-15" aria-hidden="true" tabindex="-1"></a>df_2024[<span class="st">"SFcount"</span>] <span class="op">=</span> df_2024[<span class="st">"Descriptor"</span>].<span class="op">\</span></span>
<span id="cb45-16"><a href="#cb45-16" aria-hidden="true" tabindex="-1"></a><span class="bu">str</span>.contains(<span class="st">"Street Flooding"</span>, na<span class="op">=</span><span class="va">False</span>).astype(<span class="bu">int</span>)</span>
<span id="cb45-17"><a href="#cb45-17" aria-hidden="true" tabindex="-1"></a>df_2024[<span class="st">"CBcount"</span>] <span class="op">=</span> df_2024[<span class="st">"Descriptor"</span>].<span class="op">\</span></span>
<span id="cb45-18"><a href="#cb45-18" aria-hidden="true" tabindex="-1"></a><span class="bu">str</span>.contains(<span class="st">"Catch Basin Clogged"</span>, na<span class="op">=</span><span class="va">False</span>).astype(<span class="bu">int</span>)</span>
<span id="cb45-19"><a href="#cb45-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-20"><a href="#cb45-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Aggregate counts by zip code and date</span></span>
<span id="cb45-21"><a href="#cb45-21" aria-hidden="true" tabindex="-1"></a>df_grouped <span class="op">=</span> df_2024.groupby([<span class="st">"Zipcode"</span>, <span class="st">"Date"</span>])[[<span class="st">"SFcount"</span>, <span class="st">"CBcount"</span>]].<span class="bu">sum</span>().reset_index()</span>
<span id="cb45-22"><a href="#cb45-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-23"><a href="#cb45-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate a full range of dates including 2023-12-31 for lag calculation</span></span>
<span id="cb45-24"><a href="#cb45-24" aria-hidden="true" tabindex="-1"></a>all_dates <span class="op">=</span> pd.date_range(start<span class="op">=</span><span class="st">"2023-12-31"</span>, end<span class="op">=</span><span class="st">"2024-12-31"</span>)</span>
<span id="cb45-25"><a href="#cb45-25" aria-hidden="true" tabindex="-1"></a>all_zipcodes <span class="op">=</span> df_grouped[<span class="st">"Zipcode"</span>].unique()</span>
<span id="cb45-26"><a href="#cb45-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-27"><a href="#cb45-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a complete grid of all zip codes and dates</span></span>
<span id="cb45-28"><a href="#cb45-28" aria-hidden="true" tabindex="-1"></a>multi_index <span class="op">=</span> pd.MultiIndex.from_product([all_zipcodes, all_dates], names<span class="op">=</span>[<span class="st">"Zipcode"</span>, <span class="st">"Date"</span>])</span>
<span id="cb45-29"><a href="#cb45-29" aria-hidden="true" tabindex="-1"></a>full_df <span class="op">=</span> pd.DataFrame(index<span class="op">=</span>multi_index).reset_index()</span>
<span id="cb45-30"><a href="#cb45-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-31"><a href="#cb45-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Ensure 'Date' is in datetime format</span></span>
<span id="cb45-32"><a href="#cb45-32" aria-hidden="true" tabindex="-1"></a>full_df[<span class="st">"Date"</span>] <span class="op">=</span> pd.to_datetime(full_df[<span class="st">"Date"</span>])</span>
<span id="cb45-33"><a href="#cb45-33" aria-hidden="true" tabindex="-1"></a>df_grouped[<span class="st">"Date"</span>] <span class="op">=</span> pd.to_datetime(df_grouped[<span class="st">"Date"</span>])</span>
<span id="cb45-34"><a href="#cb45-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-35"><a href="#cb45-35" aria-hidden="true" tabindex="-1"></a><span class="co"># Merge to include all combinations and fill missing values with 0</span></span>
<span id="cb45-36"><a href="#cb45-36" aria-hidden="true" tabindex="-1"></a>df_final <span class="op">=</span> full_df.merge(df_grouped, on<span class="op">=</span>[<span class="st">"Zipcode"</span>, <span class="st">"Date"</span>], how<span class="op">=</span><span class="st">"left"</span>).fillna(<span class="dv">0</span>)</span>
<span id="cb45-37"><a href="#cb45-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-38"><a href="#cb45-38" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert counts to integers</span></span>
<span id="cb45-39"><a href="#cb45-39" aria-hidden="true" tabindex="-1"></a>df_final[<span class="st">"SFcount"</span>] <span class="op">=</span> df_final[<span class="st">"SFcount"</span>].astype(<span class="bu">int</span>)</span>
<span id="cb45-40"><a href="#cb45-40" aria-hidden="true" tabindex="-1"></a>df_final[<span class="st">"CBcount"</span>] <span class="op">=</span> df_final[<span class="st">"CBcount"</span>].astype(<span class="bu">int</span>)</span>
<span id="cb45-41"><a href="#cb45-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-42"><a href="#cb45-42" aria-hidden="true" tabindex="-1"></a><span class="co"># Add lag-1 variable for CBcount</span></span>
<span id="cb45-43"><a href="#cb45-43" aria-hidden="true" tabindex="-1"></a>df_final[<span class="st">"CBcount_Lag1"</span>] <span class="op">=</span> df_final.groupby(<span class="st">"Zipcode"</span>)[<span class="st">"CBcount"</span>].shift(<span class="dv">1</span>)</span>
<span id="cb45-44"><a href="#cb45-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-45"><a href="#cb45-45" aria-hidden="true" tabindex="-1"></a>df_final.head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/var/folders/cq/5ysgnwfn7c3g0h46xyzvpj800000gn/T/ipykernel_595/2628519755.py:3: UserWarning:

Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.
</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="25">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">Zipcode</th>
<th data-quarto-table-cell-role="th">Date</th>
<th data-quarto-table-cell-role="th">SFcount</th>
<th data-quarto-table-cell-role="th">CBcount</th>
<th data-quarto-table-cell-role="th">CBcount_Lag1</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>10001</td>
<td>2023-12-31</td>
<td>0</td>
<td>0</td>
<td>NaN</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>10001</td>
<td>2024-01-01</td>
<td>0</td>
<td>0</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>10001</td>
<td>2024-01-02</td>
<td>0</td>
<td>0</td>
<td>0.0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>10001</td>
<td>2024-01-03</td>
<td>0</td>
<td>0</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>10001</td>
<td>2024-01-04</td>
<td>0</td>
<td>0</td>
<td>0.0</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>Now let’s fit a Poisson model.</p>
<div id="65f6d1d3" class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> statsmodels.api <span class="im">as</span> sm</span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> statsmodels.formula.api <span class="im">as</span> smf</span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Filter out 2023-12-31 since it has missing values for lagged CBcount</span></span>
<span id="cb47-5"><a href="#cb47-5" aria-hidden="true" tabindex="-1"></a>df_model <span class="op">=</span> df_final[df_final[<span class="st">"Date"</span>] <span class="op">&gt;=</span> <span class="st">"2024-01-01"</span>].copy()</span>
<span id="cb47-6"><a href="#cb47-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-7"><a href="#cb47-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit Poisson regression</span></span>
<span id="cb47-8"><a href="#cb47-8" aria-hidden="true" tabindex="-1"></a>poisson_model <span class="op">=</span> smf.glm(<span class="st">"SFcount ~ CBcount_Lag1"</span>, </span>
<span id="cb47-9"><a href="#cb47-9" aria-hidden="true" tabindex="-1"></a>                         data<span class="op">=</span>df_model, </span>
<span id="cb47-10"><a href="#cb47-10" aria-hidden="true" tabindex="-1"></a>                         family<span class="op">=</span>sm.families.Poisson()).fit()</span>
<span id="cb47-11"><a href="#cb47-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-12"><a href="#cb47-12" aria-hidden="true" tabindex="-1"></a>poisson_model.summary()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="26">
<table class="simpletable caption-top table table-sm table-striped small" data-quarto-postprocess="true">
<caption>Generalized Linear Model Regression Results</caption>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">Dep. Variable:</td>
<td>SFcount</td>
<td data-quarto-table-cell-role="th">No. Observations:</td>
<td>67344</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">Model:</td>
<td>GLM</td>
<td data-quarto-table-cell-role="th">Df Residuals:</td>
<td>67342</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">Model Family:</td>
<td>Poisson</td>
<td data-quarto-table-cell-role="th">Df Model:</td>
<td>1</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">Link Function:</td>
<td>Log</td>
<td data-quarto-table-cell-role="th">Scale:</td>
<td>1.0000</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">Method:</td>
<td>IRLS</td>
<td data-quarto-table-cell-role="th">Log-Likelihood:</td>
<td>-13032.</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">Date:</td>
<td>Wed, 16 Apr 2025</td>
<td data-quarto-table-cell-role="th">Deviance:</td>
<td>21120.</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">Time:</td>
<td>09:26:53</td>
<td data-quarto-table-cell-role="th">Pearson chi2:</td>
<td>1.50e+05</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">No. Iterations:</td>
<td>21</td>
<td data-quarto-table-cell-role="th">Pseudo R-squ. (CS):</td>
<td>0.003481</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">Covariance Type:</td>
<td>nonrobust</td>
<td data-quarto-table-cell-role="th"></td>
<td></td>
</tr>
</tbody>
</table>


<table class="simpletable caption-top table table-sm table-striped small" data-quarto-postprocess="true">
<tbody>
<tr class="odd">
<td></td>
<td data-quarto-table-cell-role="th">coef</td>
<td data-quarto-table-cell-role="th">std err</td>
<td data-quarto-table-cell-role="th">z</td>
<td data-quarto-table-cell-role="th">P&gt;|z|</td>
<td data-quarto-table-cell-role="th">[0.025</td>
<td data-quarto-table-cell-role="th">0.975]</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">Intercept</td>
<td>-3.1437</td>
<td>0.018</td>
<td>-170.303</td>
<td>0.000</td>
<td>-3.180</td>
<td>-3.107</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">CBcount_Lag1</td>
<td>0.2232</td>
<td>0.009</td>
<td>26.008</td>
<td>0.000</td>
<td>0.206</td>
<td>0.240</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>Then we fit a NB model.</p>
<div id="e6ccfe74" class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit Negative Binomial regression</span></span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>nb_model <span class="op">=</span> smf.negativebinomial(<span class="st">"SFcount ~ CBcount_Lag1"</span>, </span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a>                    data<span class="op">=</span>df_model).fit()</span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a>nb_model.summary()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Optimization terminated successfully.
         Current function value: 0.193513
         Iterations: 1
         Function evaluations: 7
         Gradient evaluations: 7</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/Users/junyan/work/teaching/ids-s25/ids-s25/.ids-s25/lib/python3.13/site-packages/statsmodels/base/model.py:595: HessianInversionWarning:

Inverting hessian failed, no bse or cov_params available
</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="27">
<table class="simpletable caption-top table table-sm table-striped small" data-quarto-postprocess="true">
<caption>NegativeBinomial Regression Results</caption>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">Dep. Variable:</td>
<td>SFcount</td>
<td data-quarto-table-cell-role="th">No. Observations:</td>
<td>67344</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">Model:</td>
<td>NegativeBinomial</td>
<td data-quarto-table-cell-role="th">Df Residuals:</td>
<td>67342</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">Method:</td>
<td>MLE</td>
<td data-quarto-table-cell-role="th">Df Model:</td>
<td>1</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">Date:</td>
<td>Wed, 16 Apr 2025</td>
<td data-quarto-table-cell-role="th">Pseudo R-squ.:</td>
<td>-0.1198</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">Time:</td>
<td>09:26:54</td>
<td data-quarto-table-cell-role="th">Log-Likelihood:</td>
<td>-13032.</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">converged:</td>
<td>True</td>
<td data-quarto-table-cell-role="th">LL-Null:</td>
<td>-11638.</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">Covariance Type:</td>
<td>nonrobust</td>
<td data-quarto-table-cell-role="th">LLR p-value:</td>
<td>1.000</td>
</tr>
</tbody>
</table>


<table class="simpletable caption-top table table-sm table-striped small" data-quarto-postprocess="true">
<tbody>
<tr class="odd">
<td></td>
<td data-quarto-table-cell-role="th">coef</td>
<td data-quarto-table-cell-role="th">std err</td>
<td data-quarto-table-cell-role="th">z</td>
<td data-quarto-table-cell-role="th">P&gt;|z|</td>
<td data-quarto-table-cell-role="th">[0.025</td>
<td data-quarto-table-cell-role="th">0.975]</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">Intercept</td>
<td>-3.1437</td>
<td>nan</td>
<td>nan</td>
<td>nan</td>
<td>nan</td>
<td>nan</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">CBcount_Lag1</td>
<td>0.2232</td>
<td>nan</td>
<td>nan</td>
<td>nan</td>
<td>nan</td>
<td>nan</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">alpha</td>
<td>2.252e-10</td>
<td>nan</td>
<td>nan</td>
<td>nan</td>
<td>nan</td>
<td>nan</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>Note that <code>smf.glm</code> does not allow the dispersion parameter to be estimated; instead, it is fixed at 1.</p>
<!-- Check for overdisperson. -->
<!-- ```{python} -->
<!-- # Overdispersion check: Compute Pearson Chi-square statistic -->
<!-- poisson_deviance = poisson_model.pearson_chi2 / poisson_model.df_resid -->
<!-- nb_deviance = nb_model.pearson_chi2 / nb_model.df_resid -->
<!-- # Akaike Information Criterion (AIC) comparison -->
<!-- poisson_aic = poisson_model.aic -->
<!-- nb_aic = nb_model.aic -->
<!-- # Residual analysis -->
<!-- df_model["Poisson_Residuals"] = poisson_model.resid_pearson -->
<!-- df_model["NB_Residuals"] = nb_model.resid_pearson -->
<!-- # Summary of results -->
<!-- summary_results = { -->
<!--     "Poisson Coefficients": poisson_model.params.to_dict(), -->
<!--     "Negative Binomial Coefficients": nb_model.params.to_dict(), -->
<!--     "Poisson Deviance": poisson_deviance, -->
<!--     "Negative Binomial Deviance": nb_deviance, -->
<!--     "Poisson AIC": poisson_aic, -->
<!--     "Negative Binomial AIC": nb_aic -->
<!-- } -->
<!-- # Return the summary of findings -->
<!-- summary_results -->
<!-- ``` -->


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" data-line-spacing="2" role="list" style="display: none">
<div id="ref-tibshirani1996regression" class="csl-entry" role="listitem">
Tibshirani, R. (1996). Regression shrinkage and selection via the <span>LASSO</span>. <em>Journal of the Royal Statistical Society: Series B (Methodological)</em>, <em>58</em>(1), 267–288.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./visualization.html" class="pagination-link" aria-label="Data Visualization">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Data Visualization</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./machinelearning.html" class="pagination-link" aria-label="Machine Learning: Overview">
        <span class="nav-page-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Machine Learning: Overview</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>